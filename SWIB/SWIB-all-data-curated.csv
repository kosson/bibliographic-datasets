type,year,title,author,partof,resources,File,tags,abstract,alias,,,,,,,,,
,2009,Design of a linked data-based library portal,André Hagenbruch,"https://www.swib09.de/vortraege/20091125_hagenbruch.pdf,https://www.swib09.de/videos/swib_hagenbruch.wmv",SWIB/SWIB2009/Design of a linked data-based library portal.md,"#linked/data,#linked-data,#web-of-data,#web/of/data,#bochum,#project,#infrastructure,#university-alliance-metropolis-ruhr,#portal,#design,#architecture,#swib/2009","An important trend that can be clearly seen in libraries in recent years is the integration of as many (bibliographical) data sources as possible under a uniform interface. On the one hand, however, these efforts are often about solutions whose sustainability and portability is limited, on the other hand, implicitly existing relationships between bibliographic and administrative information are not taken into account -- which stocks and departments, for example, are actually hidden behind ""computer science""? In the ""Web of Data", on the other hand, a wide variety of resources from a wide variety of domains and local as well as global sources can be related to one another and linked to one another. By adhering to a few basic principles, a high degree of interoperability and reusability of the data can be achieved. With this approach," the Bochum project ""Integrated Library Portal"" wants to create an infrastructure that can be used to react as flexibly and sustainably as possible to the immediate local conditions (two-tier library system"," University Alliance Metropolis Ruhr) and medium-term requirements (integration of heterogeneous primary data). #linked/data #linked-data #web-of-data #web/of/data #bochum #project #infrastructure #university-alliance-metropolis-ruhr #portal #design #architecture #swib/2009""",SWIB,,,,,
,2009,Free Data - The Road to Linked Data,Patrick Danowski,"https://www.swib09.de/vortraege/20091125_danowski.pdf,https://www.swib09.de/videos/swib_danowski.wmv",SWIB/SWIB2009/Free Data - The Road to Linked Data.md,"#cern,#bibliographic/data,#MARCXML,#libris,#swib/2009","The CERN Library is preparing the final steps to publish the complete bibliographic data on the web under a free license. Initially, these should be made available in MARC-XML. At the same time, there are various considerations as to how this data can be presented in the framework as linked data. For this, the experience of LIBRIS should be used. This approach is to be extended to other resources at the same time. There are already a few ideas that should be presented in the lecture. #cern #bibliographic/data #MARCXML #libris #swib/2009",SWIB,,,,,,,,,,
,2009,Integration of linked data into existing library applications,"Joachim Neubert,Timo Borst",https://www.swib09.de/vortraege/20091124_borst.pdf,SWIB/SWIB2009/Integration of linked data into existing library applications.md,"#sparql,#thesaurus,#indexing,#zbw,#swib/2009,#REST,#indexing,#retrieval,#demo","Existing library applications - catalogues, portal systems or open access repositories - are not prepared to actually exploit the opportunities linked data offers. In order to be able to make better use of sources available on the Semantic Web, such as the standard economics thesaurus, the ZBW has therefore started to develop a lightweight application architecture. It is based on web services that query the data from Semantic Web (-SPARQL) endpoints and make it accessible via a simple, use-case-centric REST interface. This means that the data can be used directly on the websites by conventional web applications. A beta implementation of two key use cases - thesaurus support in indexing and in retrieval - is presented and it is demonstrated. #sparql #thesaurus #indexing #zbw #swib/2009",SWIB,,,,,,,,,,
,2009,Introduction to the Semantic Web,Jakob Voss,"https://www.swib09.de/vortraege/20091124_voss.pdf,https://www.swib09.de/videos/swib_voss_1.wmv",SWIB/SWIB2009/Introduction to the Semantic Web.md,"#introduction,#swib/2009,#semantic/web,#linked/data,#limitations","The Semantic Web as a concrete project is at least a decade old; Although it corresponds much more closely to the original vision of the web than today's World Wide Web, concrete implementation only gained momentum with Linked Data. The introduction is intended to give an insight into why and in what form the Semantic Web has become established and where its main focus and limitations lie, without going into technical details. #introduction #swib/2009",SWIB,,,,,,,,,,
,2009,Linked applications using repository software as an example,Felix Ostrowski,"https://www.swib09.de/vortraege/20091125_ostrowski.pdf,https://www.swib09.de/videos/swib_ostrowski.wmv",SWIB/SWIB2009/Linked applications using repository software as an example.md,"#linked/data,#linked-data,#dbpedia,#repository,#ontologies,#oa-network,#dfg,#swib/2009","Linked Data is a cornerstone of the Semantic Web. After a few quiet years, developments in the field of the Semantic Web (also) are picking up speed again because (open) linked data is available in large numbers thanks to projects such as DBpedia. However, data is only part of the business. Operating on this data, creating and processing it, is another cornerstone. There is a need to complement the concept of linked data with that of linked applications. This lecture presents an approach to using ontologies as a basis for linked applications using repository software as an example. Software of this type is suitable as an object of investigation simply because there are far-reaching efforts to network it (OA network, DFG call for proposals ""Establishment and networking of repositories""). #linked/data #linked-data #dbpedia #repository #ontologies #oa-network #dfg #swib/2009",SWIB,,,,,,,,,,
,2009,Linked data in the context of digital library systems,Bernhard Haslhofer,"https://www.swib09.de/vortraege/20091124_haslhofer.pdf,https://www.swib09.de/videos/swib_haslhofer.wmv",SWIB/SWIB2009/Linked data in the context of digital library systems.md,"#dbpedia,#swib/2009,#digital-library,#digital/library,#sweden,#LOD,#wikipedia,#linked/data,#developments,#trends","The ""W3C Linking Open Data Initiative (LOD)"" aims to expose databases in a structured form on the web so that they can also be reused in other application contexts. DBpedia.org was launched in July 2007 and since then has provided Wikipedia databases in several languages in a structured form. Further databases from a wide variety of domains followed within a very short time, including those from the library sector (e.g.: Library of Congress, Swedish Union Catalogue). The central properties of these databases are, firstly, that they can be accessed on the web in a technically simple manner and in a structured form, and secondly, that they contain references (links) to semantically relevant datasets in other databases (e.g.: DBpedia.org). The lecture ""Linked Data in the Context of Digital Library Systems"" first addresses the question ""What is Linked Data?"" received. This is followed by an overview of current developments and trends in the area of linked data that are relevant to libraries. A description of possible advantages and disadvantages for libraries forms the conclusion of the lecture. #dbpedia #swib/2009 #digital-library #sweden #LOD #wikipedia",SWIB,,,,,,,,,,
,2009,Metadata and the CIDOC Conceptual Reference Model (CRM) - An introduction with application examples,Katrin Teichmann,"https://www.swib09.de/vortraege/20091125_teichmann.pdf,https://www.swib09.de/videos/swib_teichmann.wmv",SWIB/SWIB2009/Metadata and the CIDOC Conceptual Reference Model (CRM) - An introduction with application examples.md,"#cidoc-crm,#conceptual-model,#conceptual/model,#cultural/heritage,#entity,#meta-ontology,#meta/ontology,#cidoc,#swib/2009","If we ask ourselves where all the metadata and ontologies for the Semantic Web come from or how we want to deal with coexisting, competing systems in the future, we can fall back on proven models in our search for solutions. The CIDOC Conceptual Reference Model is a formal ontology that has been in development since 1996 and is now the official ISO 21127:2006 standard. Although less well known in the library environment than in the museum sector, CIDOC CRM offers a formalized conceptual model to integrate disparately structured information from the field of cultural heritage, preferably in open systems, and to convey and exchange information between applications. The semantic focus is on data structures, terminology for domains and knowledge about entities (time, place, person, object), their properties and relationships with each other. In the article, the CIDOC CRM is to be introduced in relation to selected museum object and context information using the example of the astronomer Tycho Brahe and attention is drawn to the use of the model as a meta-ontology. #cidoc-crm #conceptual-model #conceptual/model #cultural/heritage #entity #meta-ontology #meta/ontology #cidoc #swib/2009",SWIB,,,,,,,,,,
,2009,Ontologies from the perspective of information specialists. From theory to practice,Elena Semanova,"https://www.swib09.de/vortraege/20091125_semenova.pdf,https://www.swib09.de/videos/swib_semenova.wmv",SWIB/SWIB2009/Ontologies from the perspective of information specialists. From theory to practice.md,"#Ontology,#conceptual-system,#conceptual/system,#swib/2009,#transporting/content,#information/science,#Protégé","Ontologies are the core components of the Semantic Web, which fulfill the actual semantic task of the whole system: transporting content. However, there is still no unanimous opinion in specialist circles as to what an ontology represents in itself, what properties it has and what potential it conceals. There are basically two perspectives on the nature of ontologies - that of librarians and information specialists, which is determined by the primary semantic interest, and that of programmers, who put the technical problems in the foreground. The ""semantic"" perspective is hardly represented in research and is rarely discussed in the professional world. A detailed discussion in the community can create a better understanding of the advantages of ontologies as a conceptual system and lead to wider use in practice. In the article, this question is discussed from the point of view of information science and explained using a practical example. In addition, practical experience with the development of ontologies using Protégé is reported. #Ontology #conceptual-system #conceptual/system #swib/2009",SWIB,,,,,,,,,,
,2009,Practical experiences from the linked data publication of the STW,Joachim Neubert,"https://www.swib09.de/vortraege/20091125_neubert.pdf,https://www.swib09.de/videos/swib_neubert_2.wmv",SWIB/SWIB2009/Practical experiences from the linked data publication of the STW.md,"#zbw,#skos,#rdfa,#rdfxml,#rdf/xml,#sparql,#swib/2009","In the spring of 2009, the ZBW published the Standard Thesaurus for Economics as Linked Data. The aim of this form (like the publication under a non-commercial Creative Commons license) was to support re-use by humans and machines. We chose the SKOS (only recently approved as a W3C standard) as the data model. The data is made available as websites with embedded RDFa, as an RDF/XML download and can be queried online (via SPARQL endpoint). What we learned - how we proceeded, what difficulties we had to overcome, what tools we used - and what could be useful for others is the subject of this article. #zbw #skos #rdfa #rdfxml #rdf/xml #sparql #swib/2009",SWIB,,,,,,,,,,
,2009,The Bibliographic Ontology as a successor to bibliographic data formats,Jakob Voss,"https://www.swib09.de/vortraege/20091125_voss.pdf,https://www.swib09.de/videos/swib_voss_2.wmv",SWIB/SWIB2009/The Bibliographic Ontology as a successor to bibliographic data formats.md,"#zotero,#RDF,#dublin-core,#friend-of-a-friend,#foaf,#ontology,#bbc,#bibo,#bibliographic,#bibliographic/data,#bibliographic/ontology,#swib/2009","The Bibliographic Ontology (bibo) arose from the consideration of how the literature management program Zotero could be brought together with the Semantic Web. After two and a half years of development in an open discussion process with over 100 participants, version 1.3 is now available. of the RDF vocabulary for bibliographic data. Bibo builds on various ontologies such as Dublin Core (DC), Friend of a Friend (FOAF), the Event Ontology and the BBC Program Ontology. After an overview, it will be discussed how the ontology will assert itself as the standard for bibliographic data in the Semantic Web and whether this will make existing bibliographic data formats superfluous. #zotero #RDF #dublin-core #friend-of-a-friend #foaf #ontology #bbc #bibo #bibliographic #bibliographic/data #bibliographic/ontology #swib/2009",SWIB,,,,,,,,,,
,2009,The Library of Congress Subject Headings as Linked Data,Ed Summers,"https://www.swib09.de/vortraege/20091124_summers.pdf,https://www.swib09.de/videos/swib_summers.wmv",SWIB/SWIB2009/The Library of Congress Subject Headings as Linked Data.md,"#controlled/vocabularies,#subject-headings,#usa,#swib/2009,#OAI-ORE,#newspapers,#linked/data","In this presentation Ed Summers will discuss the implementation details behind two linked data web applications at the Library of Congress. The id.loc.gov service ( http://id.loc.gov ) is an exploratory service to make controlled vocabularies like the Library of Congress Subject Headings available using the FOCUS/SEMANTIC.WEB.GRAFURI/SCHEME-METADATE/Simple Knowledge Organisation System - SKOS/SKOS vocabulary. And Chronicling America ( http://chroniclingamerica.loc.gov ) which is an online archive of 1.5 million historic American newspaper pages, which makes repository objects available as Linked Data using the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary . The presentation will include tips on how to leverage existing web technologies to make linked data available, and potential areas for future development. #controlled/vocabularies #subject-headings #usa #swib/2009 #OAI-ORE #newspapers #linked/data",SWIB,,,,,,,,,,
,2009,The Swedish National Bibliography as Linked Data,Anders Söderbäck,"https://www.swib09.de/vortraege/20091124_soederbaeck.pdf,https://www.swib09.de/videos/swib_soederbaeck.wmv",SWIB/SWIB2009/The Swedish National Bibliography as Linked Data.md,"#bibliography,#sweden,#linked-open-data,#bibliographic,#repository,#union-catalog,#catalog,#sweden,#libris,#swib/2009","LIBRIS, the Swedish union catalog used for cataloging by approximately 170 library institutions and home of the Swedish National Bibliography, has since the beginning of 2008 been available as Linked Open Data. The service is a work in progress, and in this presentation Anders will describe the strategic thinking behind the LIBRIS Linked Data project, as well as the thoughts and ideas that have emerged as a consequence of it. Such ideas will include the implications of viewing bibliographic information as open networks of data, rather than as closed (or semi-closed) repositories of records. The presentation will also include a discussion of how and why libraries should use linked data as a way of connecting to the rest of the world wide web. #bibliography #sweden #linked-open-data #bibliographic #repository #union-catalog #sweden #libris #swib/2009",SWIB,,,,,,,,,,
,2009,"The national bibliography as linked data - motivation, business model, planning",Jürgen Kett,"https: //www.swib09.de/vortraege/20091124_kett.pdf,https: //www.swib09.de/videos/swib_kett.wmv","SWIB/SWIB2009/The national bibliography as linked data - motivation, business model, planning.md","#germany,#knowledge/base,#swib/2009,#bibliographic/data,#authority/files,#authority/data,#swib/2009","The German National Library is currently working on setting up a service that aims to publish its knowledge base as linked data. This service would enable direct use of all national bibliographic data, including all authority files, by the Semantic Web Community. In this short lecture, the motivation for this step will be explained and the planned design of the service will be discussed. But it is also about the urgently needed entry into the discussion about the consequences that this new way of data exchange could have for libraries: new cooperation models, extended requirements for cooperative data maintenance and a new regulation of data usage rights. #germany #knowledge/base #swib/2009 #bibliographic/data #authority/files #authority/data #swib/2009",SWIB,,,,,,,,,,
,2009,The national bibliography as linked data – technical aspects of the DNB's linked data service,"Lars G. Svensson,Jürgen Kett","https: //www.swib09.de/vortraege/20091125kett_svensson_2.pdf,https: //www.swib09.de/videos/swib_svensson.wmv",SWIB/SWIB2009/The national bibliography as linked data – technical aspects of the DNB's linked data service.md,"#bibliography,#linked/data,#linked-data,#dnb,#rdf,#prototype,#librarianship,#architecture,#workshop","The aim of the first prototype is to gather practical experience with the operation of such a service together with the network partners. The first prototype is currently in the design phase and there is no shortage of unanswered questions: Even if there are already models for linked data in the librarianship environment, the implementation of such a service for the German-speaking area still involves a good deal of research and development . We would like to take the opportunity and in this lecture present the current considerations on the architecture, persistent identification and representation of data in RDF and discuss them with the participants of the workshop. #bibliography #linked/data #linked-data #dnb #rdf",SWIB,,,,,,,,,,
,2009,Why should cultural institutions deal with the Semantic Web?,Stefan Gradmann,"https://www.swib09.de/vortraege/20091124_gradmann.pdf,https://www.swib09.de/videos/swib_gradmann_1.wmv",SWIB/SWIB2009/Why should cultural institutions deal with the Semantic Web?.md,"#knowledge/architecture,#swib/2009,#lecture","Why is a lecture with the title question ""Why should cultural institutions deal with the Semantic Web?"" necessary at all? The reasons are perhaps less obvious than the conference participants might realize. There are bad reasons for dealing with the 'Semantic Web' ... ""... because nowadays you have to write that in funding applications"" ""... because Web 2.0 will be followed by Web 3.0"" and good (or at least better) reasons ...""... with that them also in the future as parts of 'knowledge architectures'. #knowledge/architecture #swib/2009",SWIB,,,,,,,,,,
,2010,An Approach to Open Data from a Legal Perspective,Stefanie Grunow,"https://swib.org/swib10/vortraege/swib10_grunow.pptx,http://dx.doi.org/10.4016/27092.01",SWIB/SWIB2010/An Approach to Open Data from a Legal Perspective.md,"#open/data,#open-data,#law,#legal","The area of open data was only marginally noticed by the legislature in the last few changes to copyright law, data protection law and related areas of law. A clear strategy for dealing with the topic of open data, also from a legal point of view, does not yet exist. The lecture should first give an overview of the copyright classification of Open Data in the work types of copyright. In particular, the protection of database manufacturers is considered and a distinction is made from the classic types of copyright work. In order to find alternatives for dealing with free data in practice, the possibilities and limits of various licenses and waivers are explained. #open/data #open-data #law #legal",SWIB,,,,,,,,,,
,2010,Collaboration through the Semantic Web: Strategy and Activities of the German National Library,Reinhard Altenhöner,"[object Object],http://dx.doi.org/10.4016/27094.01",SWIB/SWIB2010/Collaboration through the Semantic Web - Strategy and Activities of the German National Library.md,"#dnb,#knowledge/base,#bibliographic/data,#authority/data,#linked/data,#linked-data,#bibliographic/data,#contentus,#theseus,#web-of-data,#web/of/data,#strategies,#swib/2010","In 2010, the German National Library (DNB - OAI interface) began to publish its knowledge base consisting of bibliographic data and authority data as linked data. Above all, authority data already form an important basis for indexing work in memorial institutions such as museums, archives and libraries. The aim of the DNB is now to enable direct use of the entire national bibliographic data and the authority data by the Semantic Web community by publishing the data as a triple and thus addressing new user groups. This also opens up new ways of cooperative data use: By linking to external data sets from other domains, the bibliographic data gains context and thus relevance on the World Wide Web, while external data sets offer the potential. However, these visible steps are only one link in a chain of other activities, the breeding ground for which are technologies and tools from the Semantic Web environment. This applies in particular to the CONTENTUS use case as part of the THESEUS funding initiative, but also to a number of other projects and plans that serve the purpose of semantically enriching and linking data as well as their adequate (stable) provision and use in new search environments. A new, cooperative dimension of information aggregation, integration and provision comes into view. The long-term goal is to establish libraries and other cultural institutions as a reliable backbone of the web of data. The article presents strategic considerations, concrete plans and activities of the DNB. #dnb #knowledge/base #bibliographic/data #authority/data #linked/data #linked-data #bibliographic/data #theseus #web-of-data #web/of/data #strategies #swib/2010",SWIB,,,,,,,,,,
,2010,Exposing the University of Economics' academic bibliography database as linked data,Jitka Hladká,"https://swib.org/swib10/vortraege/swib10_hladka.pptx,http://dx.doi.org/10.4016/27059.01",SWIB/SWIB2010/Exposing the University of Economics' academic bibliography database as linked data.md,"#zotero,#prague,#rdf,#Czech,#bibliometrics,#MARC,#vocabularies,#ontologies,#sfx,#bibliographic/data,#swib/2010","Academic bibliography of the University of Economics in Prague consists of bibliographic records of publications involving journal articles, conference papers, lecture notes, monographs and monograph chapters created by the academic staff of the university. We would like to discuss the experiences gained in the process of this dataset's transformation from its current data format to RDF-based data. During the course of conversion we will specify the entity types in our data and choose a way to model them. For data description we are going to use the most popular and widely implemented vocabularies and domain ontologies. Next, we will discuss the issues associated with interlinking the data with relevant and well-established datasets constituting a part of the web of data (eg, DBLP Computer Science Bibliography). We would like to examine further possibilities of data re-use by making the data suitable for other applications such as citation managers (eg, Zotero) to generate citations automatically. We are also interested in providing users with the information about availability of a resource as full text or as library holding, so we discuss cooperation with link resolvers (SFX, in our case) as well as other possible ways of linking. In this way, aligned with the concept of the semantic web, we maintain that the potential of the data would be maximized, as the information in bibliographic records would become easy to share, more visible due to incoming links, and more ready to be processed by web applications. We argue that academic bibliography data should be openly available, so it increases transparency of publishing activity of the university' s academic staff. Beyond being reusable, the data can be easily linked to so that they must not be duplicated at multiple locations. Likewise, these features together can provide a few benefits for bibliometric evaluation of science. The adoption of linked data publishing model for academic bibliography datasets involves moving to a more flexible data format for bibliographic data. While linked data is seen as a pragmatic implementation of the semantic web, we argue that it is not possible to implement the library data vision with the MARC, current standard for bibliographic data. Obviously, there is a need for more web-compatible and web-friendly data model. With this in mind, we suggest linked data as a part of the pragmatic implementation of the vision for bibliographic data on the Web. #zotero #prague #rdf #Czech #bibliometrics #MARC #vocabularies #ontologies #sfx",SWIB,,,,,,,,,,
,2010,From open data to an ecosystem of networked knowledge,Sören Auer,"https://swib.org/swib10/vortraege/swib10_auer.pdf,http://dx.doi.org/10.4016/27091.01",SWIB/SWIB2010/From open data to an ecosystem of networked knowledge.md,"#bibliographic,#libraries,#swib/2010","The publication and linking of data on the web has found enormous popularity in recent years as part of the Linked Data Initiative. However, in order to develop the data web that is developing in this way into a networked knowledge ecosystem that can be used by everyone, there are still a few challenges to be overcome: In addition to improving the quality and coherence of the information in the data web, the creation of direct added value for end users is particularly necessary. Libraries, with their gigantic stocks of knowledge, can make an important contribution to this if, for example, the high-quality bibliographic information is linked to background knowledge from the data web. #bibliographic #libraries",SWIB,,,,,,,,,,
,2010,Linked Open Projects: The Linked Data Service of the UB Mannheim,"Kai Eckert,Magnus Pfeffer","https://swib.org/swib10/vortraege/swib10_eckert_pfeffer.pdf,http://dx.doi.org/10.4016/27051.01",SWIB/SWIB2010/Linked Open Projects - The Linked Data Service of the UB Mannheim.md,"#germany,#mannheim,#linked/data,#linked-data,#automatic/classification,#classification,#indexing,#automatic/indexing,#swib/2010","The Mannheim University Library presented its Linked Data service this year, making it the first library in Germany to make the title data in its catalog accessible as a prototype Linked Data. The service should primarily be used to make results from research projects at the UB Mannheim available for easy subsequent use. Linked Data offers the ideal infrastructure for this. As part of this presentation, the technical background of the service will be briefly presented and it will be demonstrated how the results of an automatic classification and an automatic content indexing can be published as linked data. #germany #mannheim #linked/data #linked-data #automatic/classification #classification #indexing #automatic/indexing",SWIB,,,,,,,,,,
,2010,Linked RDA data in practice,"Sarah Hartmann,Alexander Haffner","https://swib.org/swib10/vortraege/swib10_hartmann_haffner.pptx,http://dx.doi.org/10.4016/27060.01",SWIB/SWIB2010/Linked RDA data in practice.md,"#rda,#bibliographic/data,#authority/data,#swib/2010,#practice","The new “Resource Description and Access” (RDA) indexing standard allows bibliographic data and authority data to be represented in a Semantic Web-compliant manner. The presentation aims to show the effects RDA has on cataloging in libraries and access to the resources in the Semantic Web. Based on initial experiences from practical implementation, it is explained how bibliographic data can be made more accessible and, above all, reused through RDA and linked data technologies. #rda #bibliographic/data",SWIB,,,,,,,,,,
,2010,Linked data at the National Széchényi Library - road to the publication,Ádám Horváth,"https://swib.org/swib10/vortraege/swib10_horvath.ppt,http://dx.doi.org/10.4016/27095.01",SWIB/SWIB2010/Linked data at the National Széchényi Library - road to the publication.md,"#linked-data,#linked/data,#infrastructure,#hungary,#Széchényi,#swib/2010","The presentation will focus on how we published our linked data and the tasks to be carried out to be able to publish the data. The presentation will begin with a short introduction of the library, the staff and the software and hardware infrastructure from the point of view of our topic. In the second part of the presentation I will explain what we have published and how it was published. The third part will be about the processes of the actual publication, and the tools used for the publication. Finally I would like to speak about some future plans. #linked-data #linked/data #infrastructure #hungary #Széchényi #swib/2010",SWIB,,,,,,,,,,
,2010,Potential of semantic technologies for the library of the future,Klaus Tochtermann,"https://swib.org/swib10/vortraege/swib10_tochtermann.ppt,http://dx.doi.org/10.4016/27097.01",SWIB/SWIB2010/Potential of semantic technologies for the library of the future.md,"#databases,#semantic/technologies,#information/quality,#swib/2010,#lecture,#library/services,#information/quality,#service/quality,#information/pull","Based on the current situation, in which libraries examine semantic technologies primarily in connection with their databases, the lecture shows the form in which semantic technologies - based on this - can shape the library of the future. This concerns on the one hand the support of the research and publication process of researchers and on the other hand the provision of innovative library services. In this context, semantics-based workplaces and the Internet of Services as a partial aspect of the Future Internet are discussed. The lecture will make the following core statements, among others:",SWIB,,,,,,,,,,
,2010,Semantic Web - a topic for library networks,Silke Schomburg,"https://swib.org/swib10/vortraege/swib10_schomburg.PPS,http://dx.doi.org/10.4016/27098.01",SWIB/SWIB2010/Semantic Web - a topic for library networks.md,"#semantic,#semantic/web,#libraries,#swib/2010,#hype,#why,#opportunities,#challenges,#lecture","Is the Semantic Web just developing into another hype or does it have an important strategic meaning for libraries and their associations? How can the Semantic Web change the library landscape and the supply of information? Why should library institutions deal with the idea of the Semantic Web and use the technologies? What opportunities does this technology bring to libraries? What risks and challenges do you have to face? The lecture provides answers to these questions and presents possible scenarios for concrete applications and perspective goals, especially from the point of view of a library network. #semantic #semantic/web #libraries #swib/2010",SWIB,,,,,,,,,,
,2010,Semantic Web activity in hbz,NN,SWIB/SWIB2010/Semantic Web activity in hbz.md,"#hbz,#skos,#frbr,#mab2,#open/data,#hbz,#swib/2010","The hbz has been dealing with the technology of the Semantic Web since mid-2009. This presentation is intended to give a brief overview of the activities. The following is touched upon: identifiers for libraries and other library institutions, the publication of classifications as SKOS data and, last but not least, the conversion of bibliographic data into linked data and the questions that arose as a result: Which vocabularies should be used? What role does FRBR play? What challenges did the MAB2 source format pose? In addition, the network libraries and the hbz had to deal with the legal question of open data. #hbz #skos #frbr #mab2 #open/data #hbz",SWIB,,,,,,,,,,,
,2010,Semantic Web in librarianship training - Which skills must be taught?,"Günther Neher,Dierk Eichel","https://swib.org/swib10/vortraege/swib10_neher.pdf,http://dx.doi.org/10.4016/27048.01",SWIB/SWIB2010/Semantic Web in librarianship training - Which skills must be taught?.md,"#information/science,#information-science,#courses,#librarianship,#infrastructure,#swib/2010","Semantic web and in particular linked data infrastructures open up a multitude of new possibilities for the application of classic library, general information science competencies in the area of information organization and knowledge transfer. In the opinion of the authors, the combination of classic information science skills and resilient, application-oriented knowledge of Semantic Web concepts and infrastructures offers great application potential in many areas, including non-library areas. However, the teaching of this interface competence often proves to be a non-trivial tightrope walk. Based on the experiences of corresponding courses, the lecture attempts, from the lecturer's point of view (Günther Neher) and from the student's point of view (Dierk Eichel). #information/science #information-science #courses #librarianship #infrastructure",SWIB,,,,,,,,,,
,2010,Semantic web applications based on the BAM portal - A prototype,Volker Conradt,"https://swib.org/swib10/vortraege/swib10_conradt.pptx,http://dx.doi.org/10.4016/27053.01",SWIB/SWIB2010/Semantic web applications based on the BAM portal - A prototype.md,"#BAM,#BSZ,#lucene,#rdf,#xslt,#sparql,#portal,#lam,#swib/2010,#XSLT/transformations,#prototype","The BSZ runs BAM, the joint portal for libraries, archives and museums. The portal collects the metadata from these three cultural sectors, indexes them using the Lucene technical platform and makes them accessible to the general public as a central point of evidence of cultural assets. The holdings of the BAM portal currently include around 42.8 million data records, around 1.8 million of which are digital copies, which are brought in by several large libraries or library associations, 16 museums and museum networks and several archives. Based on this data, the BSZ is currently creating a prototype that converts this metadata into RDF triples using XSLT transformations. The triples are integrated and linked in a Semantic Web-compatible database. With a prototypical implementation of a web interface with a search form, SPARQL query logic and the first output of results can be tested. The lecture describes the procedure for creating this prototype and presents the first results, experiences and insights. #BAM #BSZ #lucene #rdf #xslt #sparql #portal #lam",SWIB,,,,,,,,,,
,2010,W3C Working Group for Libraries and other institutions in the cultural heritage sector,Antoine Isaac,"https://swib.org/swib10/vortraege/swib10_isaac.ppt,http://dx.doi.org/10.4016/27092.01",SWIB/SWIB2010/W3C Working Group for Libraries and other institutions in the cultural heritage sector.md,"#europeana,#linked-data,#linked/data,#w3c,#libraries,#report,#mid-term,#swib/2010","In the last couple of years, the linked data approach to data publishing, sharing and interlinking has gained momentum. A growing amount of data is being released openly on the web using RDF. Some libraries have already jumped in, and the community as a whole manifests great interest in the topic. Yet there remains some technical or organization issues to a wide linked data adoption in the library domain, which requires more outreach and technology mapping work. This is one of the main missions of the Library Linked Data incubator group, a one-year effort launched in May 2010 by the W3C ""to help increase global interoperability of library data on the Web, by bringing together people involved in Semantic Web activities— focusing on Linked Data—in the library community and beyond, building on existing initiatives, and identifying collaboration tracks for the future."" I will present the objectives of this group, as well as a mid-term report on the work being done. #europeana #linked-data #linked/data #w3c #libraries #report #mid-term #swib/2010",SWIB,,,,,,,,,,
,2010,Yes we can! Libraries and the Semantic Web,Karen Coyle,"https://swib.org/swib10/vortraege/swib10_coyle.ppt,http://dx.doi.org/10.4016/27061.01",SWIB/SWIB2010/Yes we can! Libraries and the Semantic Web.md,"#karen-coyle,#libraries,#FRBR,#rda,#controlled/vocabularies,#data/models,#swib/2010","From the book catalog to the card catalog to the online catalogue, libraries have adapted their practices to take advantage of new technologies, and have used these advancements to better serve library users. The next step in this evolution is already on the horizon: it is the web of data known commonly as the Semantic Web. Reimagining library catalogs as a web of data has great potential for library users and all information seekers. Like the web of documents that is the current World Wide Web, the Semantic Web makes connections between resources. The Semantic Web, however, makes those connections meaningful. The transformation to Semantic Web style of metadata will allow libraries to serve users directly on the web, and will make possible an enhanced discovery environment that links documents and data in a rich web of relationships. The transformation of current library data to ""linked data,"" with entities and relationships, has already begun. After a brief introduction to the concepts that underly the Semantic Web, Coyle will show how the library community is developing the structures necessary for the transformation. This development begins with a domain model (FRBR and other FR's), a standardized definition of data elements (DC, RDA, and others), the definition of relationships (FRBR and RDA), and the creation of machine-readable controlled vocabularies (Library of Congress and RDA). Experimentation with new data models, such as in the Open Library and Freebase, illustrate the value of this approach. After a brief introduction to the concepts that underly the Semantic Web, Coyle will show how the library community is developing the structures necessary for the transformation. This development begins with a domain model (FRBR and other FR's), a standardized definition of data elements (DC, RDA, and others), the definition of relationships (FRBR and RDA), and the creation of machine-readable controlled vocabularies (Library of Congress and RDA). Experimentation with new data models, such as in the Open Library and Freebase, illustrate the value of this approach. After a brief introduction to the concepts that underly the Semantic Web, Coyle will show how the library community is developing the structures necessary for the transformation. This development begins with a domain model (FRBR and other FR's), a standardized definition of data elements (DC, RDA, and others), the definition of relationships (FRBR and RDA), and the creation of machine-readable controlled vocabularies (Library of Congress and RDA). Experimentation with new data models, such as in the Open Library and Freebase, illustrate the value of this approach. the definition of relationships (FRBR and RDA), and the creation of machine-readable controlled vocabularies (Library of Congress and RDA). Experimentation with new data models, such as in the Open Library and Freebase, illustrate the value of this approach. the definition of relationships (FRBR and RDA), and the creation of machine-readable controlled vocabularies (Library of Congress and RDA). Experimentation with new data models, such as in the Open Library and Freebase, illustrate the value of this approach. #karen-coyle #libraries #FRBR #rda #controlled/vocabularies",SWIB,,,,,,,,,,
,2010,ZBW press archive: Linked data with OAI-ORE,Joachim Neubert,"https://swib.org/swib10/vortraege/swib10_neubert.ppt,http://dx.doi.org/10.4016/27053.01",SWIB/SWIB2010/ZBW press archive - Linked data with OAI-ORE.md,"#rdfa,#wikipedia,#dbpedia,#LOD,#zbw,#OAI-ORE,#REST,#swib/2010","The press archive of the ZBW contains around 6 million articles. Up until 2005, folders with structured content on people, companies, subjects and goods were kept there. In order to make the digitized holdings accessible and citable under unique addresses, they are currently being processed as Linked Open Data. The OAI-ORE standard is used, which enables the addressing and description of deeply nested aggregations. The project makes it clear how central problems from the library and cultural heritage world can be solved with ORE. The W3C standard RDFa allowed the straightforward creation of a REST-based web application. Approaches to overcoming performance and availability bottlenecks will be presented in the lecture. At the same time, it is demonstrated how existing resources can be enriched with data from the LOD Cloud, e.g. by integrating abstracts from Wikipedia into the display via PND and DBpedia. #rdfa #wikipedia #dbpedia #LOD #zbw #OAI-ORE #REST",SWIB,,,,,,,,,,
,2010,data.bnf.fr: describing library resources through an information hub,Romain Wenz,"https://swib.org/swib10/vortraege/swib10_wenz.ppt,http://dx.doi.org/10.4016/27093.01",SWIB/SWIB2010/data.bnf.fr - describing library resources through an information hub.md,"#bnf,#france,#rdf,#linked/data,#FRBR,#rda,#ead,#bibliographic,#entity,#swib/2010","In order to make its data more useful on the web, the French national library has designed a new project. This ""data.bnf.fr"" project is still at an early stage. We are only starting to build the information hub that will allow us to gather the data from our various datasets. The BnF wishes to build web pages, gathering resources around the concepts of ""works"" and ""authors"". The project involves transforming existing data, enriching and interlinking the dataset with internal and external resources, and publishing HTML pages for browsing by users and search engines. The raw data would also be accessible in RDF following the principles of linked data architecture. Our purpose is to use common standards and to comply with the requirements of the Semantic Web. In this project, it is both a matter of interoperability with external sources, and of our own different datasets, since we have to align resources from several catalogues. Therefore, trying to provide a significant hub in the ""linked data"" cloud also has strong internal strategic issues, mainly in terms of bibliographic descriptions and choices (eg FRBR, RDA, integration of EAD). Technically speaking, our purpose is to gather data around concepts that focus on bibliographic entities. #bnf #france #rdf #linked/data #FRBR #rda #ead #bibliographic #entity #swib/2010",SWIB,,,,,,,,,,
,2011,An Introduction to Memento and Open Annotation,Herbert van de Sompel,SWIB/SWIB2011/An Introduction to Memento and Open Annotation.md,"#swib/2011,#memento,#framework,#Annotea,#open/annotation,#eHumanities","This session will introduce two recent technologies that are relevant to making scholarly and cultural heritage collections accessible and usable:  
(1) Conceptually, the [Memento framework](http://mementoweb.org/) introduces the time dimension that has been missing from the Web. Technically, it achieves this by introducing content negotiation in the time dimension for resources that have HTTP URIs. This allows seamless, protocol-based access to prior representations of a resource, given its URI and a desired time. Memento also yields an HTTP-based resource versioning approach that offers attractive temporal navigation features for Web resources, and that is truly powerful for leveraging Linked Data that changes over time. Memento is already catching on in the Web Archiving community, and an RFC is in the making to support broad adoption.  
(2) [Open Annotation](http://openannotation.org/) specifies a Web-centric approach aimed at annotating resources and sharing annotations across the boundaries of content collections and annotation clients. When it comes to modeling and representing annotations, Open Annotation is inspired by the W3C Annotea project. But it differs in its approach to share annotations, relying on a publish/discover paradigm that is aligned with Linked Data practices rather than on a client-server protocol as was the case with Annotea. Open Annotation's requirements gathering has focused on - sometimes complex - eHumanities use cases, but broad applicability has remained a priority throughout the effort.",SWIB,,,,,,,,,,,
,2011,Cataloguers as the Ultimate Reasoning Machines - Training Cataloguers to Create Intelligent Linked Library data,Rurik Greenall,SWIB/SWIB2011/Cataloguers as the Ultimate Reasoning Machines - Training Cataloguers to Create Intelligent Linked Library data.md,"#linked/library/data,#marc,#rdf,#cataloguing,#swib/2011,#manuscripts,#cataloging","The linked library data umbrella covers many projects aimed at converting data from MARC and other traditional library formats using mapping processes and automated linking, however, at NTNU University Library we do original cataloging directly in RDF and argue that there is a huge benefit to be had from intelligent, enrichment at source; this means that the cataloguers output information that far outstrips the kind of linking created using automated processes. In fact, this is one of the key reasons we commissioned the development of a system to help our cataloguers do this. This presentation gives details of the training given to our cataloguers, the system we commissioned and the results demonstrated in our discovery platform for historical manuscripts.",SWIB,,,,,,,,,,,
,2011,Documentation of the research process in a library as Linked Data,"Benjamin Zapilko,Brigitte Mathiak",SWIB/SWIB2011/Documentation of the research process in a library as Linked Data.md,"#linked/data,#gesis,#ontologies,#research,#ontology,#swrc,#swib/2011","The presentation of the entire research process as Linked Data not only makes connections between individual entities (authors, publications, research projects, etc.) explicit, but also gives them semantic information about the connection itself. In libraries, the use of Linked Data technologies can facilitate research across the entire research process by creating links to other types of information that are not necessarily documented in the library itself. Historically, different types of information are often not only documented with different, unrelated metadata standards, but also stored physically separate from each other. By using ontologies, these hurdles of data integration can be overcome and originally unconnected data can be semantically linked with each other. At GESIS - Leibniz Institute for the Social Sciences, the institute's own research process is already fully recorded in the library. The prototypical addition of linked data technologies should not only overcome technical and conceptual problems of data integration, but also present the connections within the research process. For this purpose, the established ontology SWRC (Semantic Web for Research Communities), which maps extensive research processes, is used and supplemented with new links to other established ontologies and vocabularies in order to ensure interoperability in the sense of linked data thoughts with other available data.",SWIB,,,,,,,,,,,
,2011,Einführung in Linked Open Data,"Felix Ostrowski, Pascal Christoph",SWIB/SWIB2011/Einführung in Linked Open Data.md,"#swib/2011,#legal,#licenses,#FOAF","The topic of ""Linked Open Data"" has also attracted a great deal of attention in the library world over the past two years. On the one hand, this workshop is intended to provide a basic introduction to the underlying technologies, and on the other hand to illuminate the resulting legal issues. After an introduction to the linked data concept, the RDF model, ontologies and SKOS as well as suitable open data licenses, the participants should apply the knowledge they have gained in practice. To do this, they will create descriptions of themselves using the widely used FOAF ontology. This description should also be linked to an open data license according to linked data principles in order to implement machine-readable licensing.",SWIB,,,,,,,,,,,
,2011,"Enhanced publications, linked data and experiences from the eco4r project",Wolfram Horstmann,"SWIB/SWIB2011/Enhanced publications, linked data and experiences from the eco4r project.md","#oai-ore,#bibliographic/models,#ontology,#enhanced/publications,#eco4r,#project,#long-term-archiving,#scientific-communication,#swib/2011","Research results are now available online in a wide variety of data forms (texts, software, visualizations, microdata) with different degrees of complexity in terms of their structures and models. This offers significant added value in terms of transparency and reuse and improved metadata quality and findability in (semantic) search engines. Despite existing models and prototypes (e.g. frameworks such as OAI-ORE, ontologies for bibliographic models), ""enhanced publications"" do not arise per se. They require documentation that must be provided by the author or a service facility such as the library. In addition, reliable linked data terminology services are required for the representation of the publications and appropriate software support. In the eco4r project, (complex) publications from productive repositories of the project partners were aggregated and visualized in a concrete application scenario. This allows the distributed publications to be compiled under new criteria. In addition to the exchange of complex information units across system boundaries, aspects of long-term archiving are also considered in the project. The lecture highlights the practical results of the project and at the same time questions the feasibility of ""Enhanced Publications"" for productive use in scientific communication.",SWIB,,,,,,,,,,,
,2011,How Linking Changes the Role of Library Data: Examples from the Wider World,Thomas Baker,SWIB/SWIB2011/How Linking Changes the Role of Library Data - Examples from the Wider World.md,"#library-linked-data,#agrovoc,#bibliographic/description,#linked/data,#vocabularies,#swib/2011","Discussion in the W3C Library Linked Data Incubator Group (2010-2011) tended to focus on the benefits of linked-data technology to libraries. This talk explores how library data - datasets, element sets, and value vocabularies - when linked, provide new forms of support to scholarly and cultural communities in the wider world. Well-maintained value vocabularies, their concepts identified by URI and backed by institutional persistence policies, can function as magnets, forming hubs of incoming links from thousands of providers. The global agricultural research community maintains a key thesaurus, AGROVOC, through an effort distributed across multiple language areas. In the library world, the standards underpinning bibliographic description, such as ISBD, FRBR, FRAD, FRSAD, and RDA, are being translated into the language of linked data. Triplified standards provide building blocks for descriptive practice based not on fixed records, but on statements that can be recombined differently and bundled for diverse, even unanticipated, uses - aggregated ""just in time"" instead of being maintained ""just in case"". As for other artifacts of long-term cultural importance, libraries could play a key role in preserving the underlying vocabularies, ensuring their long-term usefulness as the ""footnotes"" of library data.",SWIB,,,,,,,,,,,
,2011,Linked Data Light - link aggregation with BEACON,Jakob Voss,SWIB/SWIB2011/Linked Data Light - link aggregation with BEACON.md,"#beacon,#wikipedia,#linked/data,#swib/2011","One barrier to entry for providing and using links in the context of the Semantic Web is that the associated RDF technologies require some familiarization. For smaller institutions in particular, it is often tedious to first deal with technical aspects such as the configuration of web servers and triple stores if only a manageable number of links are to be published. The BEACON format developed as part of Wikipedia offers a simple alternative here. It should be shown how and where BEACON is already being used, when the use of BEACON makes sense and how links provided with BEACON can be adapted to other linked data applications.",SWIB,,,,,,,,,,,
,2011,Linked data-based web services for economics,Joachim Neubert,SWIB/SWIB2011/Linked data-based web services for economics.md,"#autosuggest,#authority/data,#suggestion/services,#thesauri,#mappings,#rest-api,#api,#swib/2011","Web services with relatively simple programming interfaces offer an opportunity to integrate databases ""on-the-fly"" into your own applications with little effort. Such web services can be integrated in a Web 2.0 manner more easily than with linked data publications that follow ""pure teaching"" (RDF, SPARQL). This offers the libraries as a whole the opportunity to make their data - and in particular their terminology and authority data - reusable with low thresholds. An example for this are autosuggest services for personal names (and underlying identities), another suggestion services for retrieval vocabulary from thesauri, possibly enriched from mappings to other vocabularies. The technical focus - in our case on economics - also increases the relevance of the proposals. The article will describe a REST-oriented API on the one hand and the LOD-based infrastructure behind it on the other.",SWIB,,,,,,,,,,,
,2011,Notes on Bibliographica,William Waites,SWIB/SWIB2011/Notes on Bibliographica.md,"#Bibliographica,#annotation,#relationships,#scholarly/works,#bibliographic/metadata,#science-mapping,#linked/data,#rdf-database,#rdf,#pylons,#project,#united-kingdom,#uk,#swib/2011","Bibliographica was conceived as a project to annotate the relationships between scholarly works and their authors so as to create a semantic map of the history and evolution of discourse in particular domains. The first step towards this was to collect a large amount of ground data, that is bibliographic metadata about the works and authors in question. To this end we have assisted in the publication of linked data version of the British National Bibliography as well as scientific publications from the medline database, some 23 million individual works in all. We have also investigated several strategies for publishing and working with this data, particularly the suitability of using RDF databases to underpin web application frameworks such as Pylons. This presentation gives an overview of our experiences in this endeavor.",SWIB,,,,,,,,,,,
,2011,Ontology-driven scientific research with RODIN,"René Schneider,Fabio Ricci,Javier Belmonte",SWIB/SWIB2011/Ontology-driven scientific research with RODIN.md,"#rodin,#project,#research/project,#ontology,#skos,#france,#open-source,#swib/2011,#scientific/research","The RODIN (= ROUe D'INformation) project, which by definition is an application-oriented research project, aims to implement an alternative portal idea within the framework of E-lib.ch that allows user-defined searches in heterogeneous information sources. Ultimately, two search strategies are combined in RODIN: a simple but user-defined meta-search via widgets, and an ontology-driven advanced search, which is based on bibliographic and encyclopedic ontologies in the SKOS data model and builds on the results of the meta-search. RODIN sees itself as a tool for information specialists and advanced users in an academic context who need a differentiated and differentiating system for exploratory research that can serve as a complement to conventional search engines. After the end of the first project phase (end of 2011), RODIN will initially be available ""out-of-the-box"" as a general web portal in the form developed during the project period; the source code will be made available for download on an open source platform . Based on this, the software can be tailored to individual information portals (""customized version"") and integrate those widgets and ontologies that are of interest for the respective research area on the websites of other portals.",SWIB,,,,,,,,,,,
,2011,Pragmatic Linked Data at the University of Southampton,Christopher Gutteridge,SWIB/SWIB2011/Pragmatic Linked Data at the University of Southampton.md,"#eprints,#OpenOrgGrinder,#rdf,#XSLT,#ARC2,#PHP,#Graphite,#SPARQL,#4store,#swib/2011","The University of Southampton is at the forefront of work in the semantic web and has produced various open and linked data sites and tools over the years ( Blog ). Now it has embarked on a project to make large amounts of the organization's own data available as Linked Open Data, in a sustainable and useful way. Many parts of the University are involved including research, teaching, catering, finance, estates and transport. This has then been used to build a number of useful applications for the University staff, students and visitors. A very pragmatic approach has been taken, and rather than just publish the RDF, a website has been published over the top of the data which creates immediate value from data which was previously buried in spreadsheets. The focus has been on creating an agile, manageable and sustainable system rather than something perfect but brittle. The data comes from a mix of sources; various databases, 4 different EPrints repositories and a large number of google-docs spreadsheets, Example ). All the tools used to build the service are free open source software. We use a viral machine running Ubuntu Linux, store our data in 4-store. The smaller datasets are edited using Google Spreadsheets. We prepare our data using OpenOrgGrinder (created by us but open source) which converts spreadsheets into RDF, XSLT and rapper. To provide useful views of our data we use ARC2 PHP library, Graphite PHP Library (created by us) and Google Maps. The public SPARQL endpoint sits in front of the real endpoint and adds extra features not available from 4store.",SWIB,,,,,,,,,,,
,2011,Pragmatische Ansätze für den Umgang mit 'Enhanced Publications' in existierenden Repository-Umgebungen,"Anouar Boulal,Martin Iordanidis,Jochen Schirrwagen",SWIB/SWIB2011/Pragmatische Ansätze für den Umgang mit 'Enhanced Publications' in existierenden Repository-Umgebungen.md,"#eco4r,#swib/2011,#DFG,#compound-objects,#long-term,#OAI-ORE,#enrichment,#metadata,#best-practice,#overlay-journal","In the DFG project eco4r ""Exposing Compound Objects for Repositories"" a concrete application scenario was implemented as a prototype. Complex publications are aggregated and visualized from the productive repository environments of the project partners (hbz NRW and Bielefeld University Library) in such a way that the distributed publications are compiled under new (e.g. subject-specific) criteria. A so-called overlay journal is created. In addition to the exchange of complex information units across system boundaries, aspects of long-term archiving are also considered. After a short introductory phase on the subject of complex information objects in repository environments, the focus is on practical applications. The participants are accompanied from the installation and configuration to the use of the most important modules developed in the project (OAI-ORE repository plugins for OPUS and Fedora, export interfaces, triple store, metadata enrichment, etc.). The preliminary workshop program is as follows""&#58;"" Lectures on eco4r and project results, projects with similar objectives and an LTA concept for complex information objects; Hands On Sessions""&#58;"" Practical demonstration of the Overlay Journal""&#58;"" from a user's point of view, and installation, configuration and use of modules and components of the Overlay Journal Discussion of 'Best Practices' and presentation and summary of the workshop results",SWIB,,,,,,,,,,,
,2011,Publishing and using loan data from libraries as Linked Open Data,Magnus Pfeffer,SWIB/SWIB2011/Publishing and using loan data from libraries as Linked Open Data.md,"#project,#germany,#library-lending,#vocabulary,#user-behavior,#lending,#mannheim,#swib/2011","In an ongoing project, I am investigating possible use cases for lending data from library systems as linked data together with Kai Eckert from the UB Mannheim and students at the Hochschule der Medien. The first scenario is the statistical comparison of (partial) libraries. Here, the description of the libraries, their inventory and the lending conditions must be modeled as a vocabulary. This vocabulary and its semantics must be based on the existing standards and recommendations for the statistical comparison of libraries and should in any case also allow the mapping of the DBS. The second scenario is the aggregation of loan information at the title level to analyze user behavior. As an application, an alternative ranking for retrieval is in the foreground, which is based on the popularity of the title. Basically, with this type of data, an application for the optimization of one's own inventory and the structure of the inventory would also be conceivable, which would then be completely independent of the library software used and a specific data warehouse solution. The challenge in modeling the second scenario is the various conceivable granularities in which the data is processed. This makes it possible to model and describe each individual lending process. Almost all information recorded in the library system would be retained. However, for retrieval use cases, a coarser mapping that aggregates the data at title level would be desirable. The data models for the mentioned scenarios and an example depiction of real data from the library system of the UB Mannheim are currently being developed. We will make the modeling and the real data available on Mannheim University Library's Linked Data Service over the course of the next few months .",SWIB,,,,,,,,,,,
,2011,RDF-Daten in eigenen Anwendungen nutzen,Jakob Voß,SWIB/SWIB2011/RDF-Daten in eigenen Anwendungen nutzen.md,"#swib/2011,#rdf,#workshop","In recent years, the provision of information as Linked Open Data in RDF has steadily increased. Nevertheless, the question of the benefit of the provision often cannot be answered convincingly. It is doubtful that this will happen with a ""killer application"" that impressively demonstrates the further development of the World Wide Web. Instead, you can already develop your own applications that use RDF and Linked Open Data with manageable effort, without focusing on these technologies as an end in themselves. The workshop aims to show the way from the idea to your own application based on existing RDF data. The focus is less on merging and evaluating large amounts of data (""Big Data"") and more on small, specialized applications (""Long Tail""), such as are regularly required in libraries and other institutions. The workshop is primarily aimed at developers and other interested parties with rudimentary programming skills and/or knowledge of HTML/CSS. With the help of a framework developed at the GBV network center for the simple development of RDF-based web applications, the participants should be able at the end of the workshop to develop their own applications that combine RDF data from different sources and present them prepared for end users . Together we will discuss which hurdles have to be overcome and what opportunities there are for further development. The practical exercises will take place in small teams.",SWIB,,,,,,,,,,,
,2011,The High and Lows of Library Linked Data,Adrian Stevenson,SWIB/SWIB2011/The High and Lows of Library Linked Data.md,"#uk,#united-kingdom,#jisc,#locah,#project,#linked-data,#archival-metadata,#repositories,#library-catalog,#uri-patterns,#enriching,#opne-licenses,#swib/2011","This session will explore the progress of the UK JISC-funded LOCAH Project: Linked Open Copac and Archives Hub . The project is making records from the Archives Hub service and Copac service available as Linked Data. The Archives Hub is an aggregation of archival metadata from repositories across the UK; Copac provides access to the merged library catalogs of libraries throughout the UK, including all national libraries. In each case the aim is to provide Linked Data, so that we make our data interconnected with other data sets. The presentation will cover aspects of data modelling, the selection of vocabularies and the design of URI patterns. It will examine options for enriching the data, to provide links to other datasets. A prototype visualization will be shown, demonstrating how Linked Data can enable researchers to interrogate data in different ways. The presentation will conclude with a look at some of the main opportunities and barriers to the creation and use of Linked Data. The presentation will address a number of the questions posed by SWIB, including: how to produce useful links to and between newly published datasets; where to find LOD applications which show the added value created by linking assets; the value of open licenses to LOD-based infrastructures.",SWIB,,,,,,,,,,,
,2011,The LODUM Project: Transparent Research Based on Linked Library Data,"Carsten Kessler,Tomi Kauppinen,Umut Tas",SWIB/SWIB2011/The LODUM Project - Transparent Research Based on Linked Library Data.md,"#Münster,#germany,#project,#research-results,#research/results,#research/data,#reproducible-research,#bibliographic/data,#cloud-environments,#cloud,#swib/2011","The Linked Open Data University of Münster (LODUM) project aims to improve the visibility and transparency of the university using Semantic Web technologies. The main goal is to improve the accessibility of research results (publications, research data, models, methods and software) to ensure transparent and reproducible research. This linked science approach relies on an improved exchange of results both within and between the various disciplines. The publication, as the predominant way of documenting research results, still plays a central role. LODUM not only uses Linked Data as a new metadata standard for bibliographic data, but also strives to link the publication with all relevant data and models. By providing the data and models in dereferenceable, machine-readable form, it should be possible to trace research results in cloud environments in the future.",SWIB,,,,,,,,,,,
,2011,The Open Citation Corpus and the SPAR Ontologies,David Shotton,SWIB/SWIB2011/The Open Citation Corpus and the SPAR Ontologies.md,"#Open-Citations-Corpus,#PubMed-Central,#biomedical,#open-access,#bibliographic/records,#citations,#spar,#ontology,#open-citation-blog,#open-citation-project,#swib/2011","The Open Citations Corpus is a database of approx. 6.3 million biomedical literature citations harvested from the reference lists of all open access articles in PubMed Central. These contain references to approx. 3.4 million papers, which represent ~20% of all PubMed-listed papers published between 1950 and 2010, including all the most highly cited papers in every biomedical field. The Open Citations Corpus web site allows you to browse these bibliographic records and citations, to select an individual article, and to visualize its citation network in a variety of displays. Details of each selected reference, and the data and diagrams for its citation network, may be downloaded in a variety of formats, while the entire Open Citations Corpus can be downloaded in several formats including RDF and BibJSON.CiTO , FaBiO and other SPAR (Semantic Publishing and Referencing) ontology. Ontologies have been used to encode this information in RDF, after parsing the National Library of Medicine DTD XML obtained from PubMed Central, and after undertaking considerable work to remove the errors that exist in approximately 1% of the literature references. Further information about the Open Citation Corpus, the data processing, and the JISC Open Citations Project that supported this work, is given on the Open Citations Blog.",SWIB,,,,,,,,,,,
,2011,Tipping the Sacred Cow: Thinking Beyond the Journal System,Herbert van de Sompel,SWIB/SWIB2011/Tipping the Sacred Cow - Thinking Beyond the Journal System.md,"#scholarly/communication,#beyond-the-pdf,#electronic-journals,#swib/2011","More than a decade after the emergence of electronic journals, the web-based scholarly communication system still strongly resembles its paper-based predecessor. The growing frustration with this status quo is illustrated by three prominent events in 2011 alone aimed at bringing together thought leaders to reflect on an improved scholarly communication system that better leverages the technical and social capabilities offered by the networked environment: the Beyond the PDF meeting , the Dagstuhl Workshop on the Future of Research Communication , and the Microsoft Research Transforming Scholarly Communication Workshop. Meanwhile, glimpses of eminent changes can already be observed, including the emergence of a machine-actionable layer of scholarly communication in which semantic technologies play a significant role and the growing interest in ""papers"" that are more tightly integrated in the scholarly process and environment . Examples of these changes can mostly be characterized as experimental, and their eventual deployment may still take years. Meanwhile, there remain plenty of opportunities to introduce straightforward improvements aimed at better aligning scholarly communication with established web practices.",SWIB,,,,,,,,,,,
,2011,What needs to happen in a scholarly publishing reform?,Bjorn Brembs,SWIB/SWIB2011/What needs to happen in a scholarly publishing reform?.md,"#scholarly/publishing,#open-access,#digital-library,#swib/2011","Scholarly publishing is fundamentally broken at essentially all levels starting with basic functionality and ranging to access, incentives, attribution, archiving, share/re-use and cost/benefit. What could be the feasible alternatives which would move scholarly publishing closer to a modern, IT-based system? A system which assists scientists in sorting, filtering and discovering relevant scientific findings? Which provides open access to tax-payer-funded research at a reasonable price? Which provides an incentive structure with an attribution system that benefits science and scientists rather than publishers and bureaucrats? I will argue that a natural candidate for developing such a system is the one institution on this planet which has centuries of experience in archiving and making accessible the literature and data of scientists. In addition to centuries of experience, many libraries in our digital age lack a sense of purpose or direction because of it. Creating a world-wide, peer-reviewed, open access, federated digital library of science is exactly the kind of task any modern university library should dream of taking part in. This digital utopia is exactly what scientists today are in desperate need of and libraries would be able to deliver.",SWIB,,,,,,,,,,,
,2011,culturegraph.org - building a hub for Linked Library Data,"Markus Geipel,Adrian Pohl",SWIB/SWIB2011/culturegraph.org - building a hub for Linked Library Data.md,"#culturegraph,#linked/open/data,#DNB,#hbz,#bibliography,#heterogeneity,#identifiers,#swib/2011,#metadata,#metadata/comparison","Culturegraph.org is a linked open data service whose objective is the uniform, reliable and persistent referencing of cultural products. The service is currently being set up in cooperation with the German National Library (DNB - OAI interface) and the University Library Center of the State of North Rhine-Westphalia (hbz) with the support of the Association of Network Systems. The databases of publishers, libraries and library associations are increasingly opening up to the Linked Open Data concept. In addition to the advantages and perspectives that this development brings with it, the heterogeneity of the descriptions and identifiers used is also growing. Another challenge is recognizing records that describe the same resources, since there is a large overlap in the catalogs - especially for bibliographic resources. Finally, authority data is also gaining in importance, especially when it is linked to other data sources. Culturegraph.org addresses these challenges. As an open service platform, Culturegraph.org should offer the possibility of comparing metadata and calculating equivalences. The results are freely available according to the linked open data standards. In further expansion stages, authority data will be added and linked to other data sources relevant to cultural institutions.",SWIB,,,,,,,,,,,
,2012,Building a High Performance Environment for RDF Publishing,Pascal Christoph,"https://swib.org/swib12/slides/Christoph_SWIB12_117.pdf,http://www.scivee.tv/node/55329",SWIB/SWIB2012/Building a High Performance Environment for RDF Publishing.md,"#sparql-performance,#performance,#sparql,#elasticsearch,#indexing,#lobid,#4store,#triplestore,#content/negotiation,#swib/2012,#lod","Linked Open Data can be published in different forms (flat files, RDFa, triple store). In lobid we used an approach centered around a triple store (4store). Different RDF-serializations of resources (turtle, RDF/XML, RDFa-enriched HTML presentations etc.) are generated through SPARQL queries and provided via content negotiation. Running a triple store and providing a SPARQL endpoint allows powerful queries to make the best out of LOD. But then, these queries often take their time, especially if you have a large data pool. While it is possible doing string searches via SPARQL, we found that it is not really performant dealing with lots of data, and language processing is not supported at all. Thus, we are running a search engine (elasticsearch) alongside the triple store to enable fast string searches. This talk is about indexing of data into the triple store and into the search engine in parallel to reap even more benefits from elasticsearch. As elasticsearch indexes data using JSON, this approach makes use of JSON-LD, a JSON serialization for RDF. elasticsearch comes with many features that we are looking for using with our LOD anyway - like high availability, distributed index, near-realtime updates, versioning and fast geo-searching. The talk will highlight how these benefits can be used for LOD.",SWIB,,,,,,,,,,
,2012,Culturegraph Authorities,Markus Geipel,"https://swib.org/swib12/slides/Geipel_SWIB12_115.ppt,http://www.scivee.tv/node/55321",SWIB/SWIB2012/Culturegraph Authorities.md,"#uthority/files,#GND,#BEACON,#Culturegraph,#swib/2012","Authority files play an increasing role in the fields of Semantic Web and semantic search as they provide reliable identifiers for entities, which otherwise would be tedious to identify and disambiguate. The German Universal Authority File, “Gemeinsame Normdatei” (GND) comprises more than entities such as person names, conferences or events, corporate bodies, places or geographic names, subject headings, and works. It is maintained and used by a variety of cultural heritage organizations and thus has the potential to be an entry point for semantic search beyond institutional boundaries. However, as links are still unidirectional, pointing from catalogue data to the GND but not backwards, we currently face a navigational dead end. To add the missing back links is the objective of Culturegraph Authorities. By analyzing the main German library catalogues as well as crosslink data in BEACON files, the respective back links are created. By making the results available online, Culturegraph Authorities brings closer together a vast variety of cultural heritage data, while increasing the visibility of all participating institutions.",SWIB,,,,,,,,,,
,2012,Discovering Links for Metadata Enrichment on Computer Science Papers,"Johann Schaible,Philipp Mayr","https://swib.org/swib12/slides/Schaible_SWIB12_107.pdf,http://www.scivee.tv/node/55328",SWIB/SWIB2012/Discovering Links for Metadata Enrichment on Computer Science Papers.md,"#Silk,#n-triples,#SPARQL,#RDF,#dublin-core,#dcterms,#metadata/enrichment,#enrichment,#silk,#owl/sameAs,#swib/2012","Libraries, which have collections of scientific papers, most of the times show only basic information comprising title, authors, publication date, and an abstract of a paper. The user can utilize this data to manually look up more information about the paper on the web. This is time-consuming though and simply not done by many users. In this paper we demonstrate a 3-step approach using semantic web technology, which consists of integrating Linked Data in order to enlarge metadata of a paper with links to external data sources. For this we use a link discovery tool, in our case Silk. Our initial record for each computer science paper consists of its title, its authors, and its publication date. This information is represented in simple RDF using the dcterms namespace, i.e. dcterms:title, dcterms:creator, and dcterms:date. As external data sources with the highest expectations of finding additional data we identified the DBLP computer science bibliography, the Association for Computing Machinery (ACM), and the Semantic Web Conference Corpus. In the first step we build a connection between our records and each of these data sources using Silk. By defining linkage rules e.g. link dcterms:title in data set 'a' and rdfs:label in data set 'b' and a matching scenario e.g. owl:sameAs, in the second step Silk looks up equal values in both data sources. If it finds an equal value, a link between data sets will be generated. All generated links are stored as n-triples. In the third step we manually add these links to our data set, enriching a paper’s metadata. We also illustrate our experiences with SILK regarding correctness and the handling of RDF dumps and SPARQL Endpoints.",SWIB,,,,,,,,,,
,2012,Encoding Patron Information in RDF,Jakob Voß | Jakob Voss,"https://swib.org/swib12/slides/Vo%C3%9F_SWIB12_103.pdf,http://www.scivee.tv/node/55392",SWIB/SWIB2012/Encoding Patron Information in RDF.md,"#bibliographic/data,#authority/files,#ontologies,#rdf,#swib/2012,#DAIA,#SIOC","Current efforts to publish library data as Linked Open Data (LOD) have focused on bibliographic data, authority files and organizations. It took some time to also publish information about single items and their current availability in particular libraries. Applications that make use of these data sets are just being created. Patron information, consisting of links between library patrons and documents held by libraries, however, is not published yet. This gap exists with good cause because of privacy concerns but also because it is difficult to get patron information out of legacy systems and because there is no encoding in RDF yet. The proposed research report will present an encoding of patron information in RDF that was created for two applications in the GBV library network. The encoding aligns with existing ontologies, namely the Document Availability Information Ontology (DAIA) and Semantically-Interlinked Online Communities (SIOC). It will be shown how patron information from library systems is extracted from (legacy) library systems and how the information is encoded in RDF, among other formats. The patron information can then be combined with other sources, while respecting patron privacy.",SWIB,,,,,,,,,,
,2012,Enrichment of Library Authority Files by Linked Open Data Sources,"Gerd Zechmeister,Helmut Nagy","https://swib.org/swib12/slides/Zechmeister_SWIB12_114.ppt,http://www.scivee.tv/node/55323",SWIB/SWIB2012/Enrichment of Library Authority Files by Linked Open Data Sources.md,"#catalogues,#authority/files,#DNB,#LOD2,#SKOS,#data/quality,#LOD2,#swib/2012,#enrichment","The Linked Data initiative enables institutions to publish and share their data following open standards and to merge, interlink and reuse data provided by numerous sources of the Linked Open Data (LOD) cloud. It might be the next paradigm shift in the library world: Catalogues, for instance, are predestined to make use of publicly available data to enrich (add pictures, geodata etc.) or annotate (add reviews, comments etc.) the content. Dynamically generated mashups of cross-media information would create an added value to library systems and its records. The LOD community in return strongly benefits from these sources as librarians revise and annotate resources, thus ensuring the quality of data. The German National Library recently made a big step forward in their ambitions of providing ""cooperatively maintained German authority files for persons, corporate bodies and subject headings"" as LOD DNB. The subject headings authority files are expressed with SKOS, an open WC3 specification to build thesauri. We evaluated the dataset and identified potential LOD sources to be linked with the authority files. We will show how the technology stack from the LOD2 project can be used to create links and to enrich the subject headings with LOD sources, and demonstrate an example application that presents a mashup of authority files data and matching LOD sources. An outlook to further possible use cases and application scenarios will round off the presentation.",SWIB,,,,,,,,,,
,2012,First Insights into the Library Track of the OAEI,Dominique Ritze,"https://swib.org/swib12/slides/Ritze_SWIB12_109.ppt,http://www.scivee.tv/node/55324",SWIB/SWIB2012/First Insights into the Library Track of the OAEI.md,"#ontology,#ontology/matching,#TheSoz,#OAEI,#matchers,#bibliography,#subject/authority/data,#swib/2012,#STW,#ISWC,#bibliographic/domain","The OAEI is the Ontology Alignment Evaluation Initiative, an annual campaign to evaluate ontology matching systems. To compare different matching systems, various test cases with different purposes are provided by several groups. To see how well current matching systems are able to deal with data especially occurring in libraries, we submitted a library track to the OAEI. In our test case, the task is to match the Thesaurus for Economics (STW) against the Thesaurus for the Social Sciences (TheSoz). As a result, the ontology matching systems create a list of cross-concordances. Since we are in possession of cross-concordances which have been manually generated several years ago, we are able to evaluate the results of the ontology matching systems. In this presentation, we report from the OAEI ontology matching workshop held at the ISWC 2012 and show how state-of-the-art matchers perform on this specific task and whether such systems can be applied in the bibliographic domain to automatically discover cross-concordances. Knowing these concordances is very important since <mark>the subject authority data maintained by libraries are the backbone of the Semantic Web and are able to support the semantic search</mark>.",SWIB,,,,,,,,,,
,2012,Introduction to Linked Open Data,"Felix Ostrowski,Pascal Christoph,Adrian Pohl",https://swib.org/swib12/slides/Christoph_SWIB12_144.zip,SWIB/SWIB2012/Introduction to Linked Open Data.md,"#swib/2012,#legal,#workshop,#RDF,#RDFs,#SPARQL,#vocabularies","This introductory workshop aims to introduce the fundamentals of Linked Data technologies on the one hand, and the basic legal issues of Open Data on the other. The RDF data model will be discussed, along with the concepts of dereferencable URIs and common vocabularies. The participants will continuously create and refine RDF documents to strengthen their knowledge on the topic. Linked Data tenets such as publishing RDF descriptions in a web environment and utilizing Content-Negotiation will be demonstrated and applied by the participants. Aggregating data from several sources and querying this data will showcase the advantages of publishing Linked Data, and RDF Schema will be introduced as an effective way of data integration. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop.  
**Workshop outcomes**: The participants will have created openly licenced RDF descriptions of themselves, published these to a webserver, aggregated the data into a triplestore against which SPARQL queries are then executed. The possibilities of using RDFs to integrate data across vocabularies have been explored.",SWIB,,,,,,,,,,
,2012,LODLAM - Fostering Global Collaboration and Community,Jon Voss,"https://swib.org/swib12/slides/Voss_SWIB12_125.pdf,http://www.scivee.tv/node/55301",SWIB/SWIB2012/LODLAM - Fostering Global Collaboration and Community.md,"#loadlam,#swib/2012,#legal","What happens when hundreds of thousands of archival photos are shared with open licenses, then mashed up with geolocation data and current photos? Or when app developers can freely utilize information and images from millions of books? Like the web of documents that became the World Wide Web, a web of data is the goal of Linked Open Data. Jon will discuss how the cultural, technological, and legal environment is enabling a growing ecosystem of open historical data and cross-pollination across business sectors, as particularly illustrated by the International Linked Open Data in Libraries, Archives and Museums Summit. We'll explore evolving examples of Linked Open Data in several institutions, and how a global community within libraries, archives and museums is beginning to play a critical role in the evolution of the Web.",SWIB,,,,,,,,,,
,2012,Linked Open Library Data in Practice: Lessons Learned and Opportunities for data.bnf.fr,Romain Wenz,"https://swib.org/swib12/slides/Wenz_SWIB12_108.ppt,http://www.scivee.tv/node/55862",SWIB/SWIB2012/Linked Open Library Data in Practice - Lessons Learned and Opportunities for data.bnf.fr.md,"#BnF,#france,#frbr,#data-matching,#matching,#alignments,#modelling-principles,#library/data,#programmatically-matching,#swib/2012","The presentation will give a report on “data.bnf.fr”, the Linked Open Data project of the National Library of France (Bibliothèque nationale de France - BnF). The first purpose of data.bnf.fr is to gather data from several sources: library catalogue, archives and manuscripts, digital collections. Data.bnf.fr has reached a stage of maturity, with millions of resources. Some issues had to be solved in the course of building the project. For implementing the FRBR principles, data matching had to be done programmatically. The alignments rely on roles of authors, so as to be able to match resources at the proper FRBR level. The presentation will explain the general modelling principles. With the “Open data” initiatives led by the French government, it is possible to use an Open Licence. Once data is linked and open, what comes next? First, changes in general use, since people can now find BnF’s resources directly on the Web. Secondly, the data is being used by broader communities. For instance for small public libraries, new procedures are being explored for re-use of the dataset in local catalogues. In the long term, Semantic Web technologies could set a standard for library data, if we keep them linked and open.",SWIB,,,,,,,,,,
,2012,"Linking Data, Linking People",Emmanuelle Bermès,"https://swib.org/swib12/slides/Bermes_SWIB12_126.pdf,http://www.scivee.tv/node/55380","SWIB/SWIB2012/Linking Data, Linking People.md","#linked/graph,#graph,#swib/2012,#meaning,#cooperation,#linked-library-graph","Creating a linked graph of open library data means that libraries have to go global. Global beyond the barriers of countries, languages and continents. Global beyond the challenges of formats and software systems. Global beyond the differences of intellectual property rights and licensing traditions. Global beyond the diversity of domains and data models. Library already are global in many ways, and they have been for many years. International cooperation, shared cataloguing strategies, standardization have been promoted in libraries for decades. So what's new with Linked Data ? What does it mean for libraries to change their old cooperation models and embrace the Linked Data movement ? Such a paradigm change requires not only technology and standards, but also cooperation between institutions and people. Community building is also an important aspect of the creation of an international linked library graph.",SWIB,,,,,,,,,,
,2012,Metadata Provenance,"Kai Eckert,Magnus Pfeffer",https://swib.org/swib12/slides/Eckert_SWIB12_137.zip,SWIB/SWIB2012/Metadata Provenance.md,"#swib/2012,#metadata,#provenance,#licenses,#rdf,#enrichment,#PROV","When metadata is distributed, combined, and enriched as Linked Data, the tracking of its provenance becomes a hard issue. Using data encumbered with licenses that require attribution of authorship may eventually become impractible as more and more data sets are aggregated - one of the main motivations for the call to open data under permissive licenses like CC0. Nonetheless, there are important scenarios where keeping track of provenance information becomes a necessity. A typical example is the enrichment of existing data with automatically obtained data, for instance as a result of automatic indexing. Ideally, the origins, conditions, rules and other means of production of every statement are known and can be used to put it into the right context.  
**Part 1 - Metadata Provenance in RDF:** In RDF, the mere representation of provenance - i.e., statements about statements - is challenging. We explore the possibilities, from the unloved reification and other proposed alternative Linked Data practices through to named graphs and recent developments regarding the upcoming next version of RDF.  
**Part 2 - Interoperable Metadata Provenance:** As with metadata itself, common vocabularies and data models are needed to express basic provenance information in an interoperable fashion. We investigate the PROV model that is currently developed by the W3C Provenance Working Group and compare it to Dublin Core as a representative of a flat, descriptive metadata schema.  
We actively encourage participants to present their own use cases and open challenges at this workshop. Please contact the organizers for details.",SWIB,,,,,,,,,,
,2012,"Old Silos, New Silos, No Silos - From Redundancy to Aggregation or Distribution?",Lukas Koster,"https://swib.org/swib12/slides/Koster_SWIB12_112.pdf,http://www.scivee.tv/node/55381","SWIB/SWIB2012/Old Silos, New Silos, No Silos - From Redundancy to Aggregation or Distribution?.md","#silos,#aggregation,#swib/2012","Traditionally library systems/catalogues have been isolated local systems, thereby creating an enormous redundancy in both data and metadata backends and search frontends. Even in shared cataloguing environments the local subsystems are the real production environments. In recent years we have seen two separate developments that claim to solve the redundancy problem: aggregated (meta)data stores and distributed Linked Open Data networks. Examples of aggregation: content aggregators and publishers’ proprietary databases, discovery layer global indexes, Europeana, Worldcat, etc. An earlier form of distribution, federated search/metasearch, is now gradually abandoned. A number of its inherent problems (performance, relevance ranking, network) are solved by aggregation, but others are not. Do the new silos of aggregation with their limited content solve all our problems, or is a completely open global linked networked model better? The pros and cons of both general models and of hybrid, blended options will be considered. We will also discuss the practical conditions, implications and feasibility of the models, looking at licensing, commercial interests, trust, authority, etc. Last but not least: Can and will libraries have a role in the new data universe that is outside their direct control, and what can these roles look like?",SWIB,,,,,,,,,,
,2012,PhD Workshop,"Atif Latif,Timo Borst",SWIB/SWIB2012/PhD Workshop.md,"#swib/2012,#phd,#workshop,#library-linked-data","The Linked Open Data approach provides a framework for the generation, publishing and sharing of information by means of semantic technologies. It plays a vital role in the realization of the Semantic Web at a global scale by publishing and interlinking diverse data sources on the Web. The access to a huge amount of Linked Data presents exciting opportunities for the next generation of Web-based applications, especially with regard to data hosted and provided by libraries. Facing use cases as depicted by the [Library Linked Data Incubator Group](http://www.w3.org/2005/Incubator/lld/XGR-lld-usecase-20111025/), however there is still a need of Linked Data applications and best practice examples. This PhD workshop will provide an excellent opportunity both for the beginner as well as the senior PhD student to present his or her ideas and receive feedback by experienced researchers and other PhD students working in research areas related to Linked Data based infrastructures and applications in libraries.",SWIB,,,,,,,,,,,
,2012,Simple Semantic Enrichment of Scientific Papers in Social Sciences,"Alexander Garcia,Philipp Mayr,Leyla Jael Garcia","https://swib.org/swib12/slides/garcia_SWIB12_111.pdf,http://www.scivee.tv/node/55327",SWIB/SWIB2012/Simple Semantic Enrichment of Scientific Papers in Social Sciences.md,"#scholarly/papers,#scholarly/communication,#ontologies,#nanopublications,#bibliographic/references,#swib/2012,#enrichment,#Annotation-Ontology,#model","In this paper we present a simple methodology for enriching scholarly papers in the social sciences domain. We are making use of existing technology; resulting from our approach we obtain a publication readily available for the Web of Data. Our scenario is that posed by the journal Methods, Data, Analysis (henceforth MDA); an archive of PDF files, rich bibliographic metadata available, a publication workflow mostly based on word files, no navigation tools tailored for this journal, no previous structure in the content of the journal - authors are free to organize the document as they best consider. As there is a pre-existing archive the solution could not simply consider future publications. Our approach is based on the orchestration of various ontologies; for instance, DOCO, BiRO, CiTO, DC, FOAF, and the Annotation Ontology (AO). We are using the AO for structuring the markup of domain specific concepts as well as for nanopublications derived from community-based annotation – focusing on hypothesis, results, etc. The structure of the document is represented by DOCO; in addition, we are also using BiRO and CiTO for bibliographic references and citation typing. Our model facilitates the exploration of content as well as the formulation of semantic queries expressed in SPARQL. In addition, we are also modelling the relationship between the data and the document. External resources relevant for the document are aggregated as part of the end user environment. The workflow for the pre-existing archive of PDFs starts by converting the PDF to RDF; supporting this task we developed a web service for the “fully automated PDF-to-XML” process.",SWIB,,,,,,,,,,
,2012,Statistical Research Data on the Semantic Web,Daniel Bahls,"https://swib.org/swib12/slides/Bahls_SWIB12_120.ppt,http://www.scivee.tv/node/55391",SWIB/SWIB2012/Statistical Research Data on the Semantic Web.md,"#bibliographic/artefacts,#data/repository,#swib/2012,#economics,#SDMX","At present, efforts are being made to pick up research data as bibliographic artefacts for re-use, transparency and citation. In the field of economics, a large amount of research is based on empirical data, which is often combined from several sources such as data centres, affiliated institutes or self-conducted surveys. A good deal of the data used in empirical research is protected or simply cannot be shared with third parties due to data usage rights, partly because some of the providers are commercial. As a consequence, a researcher is often not allowed to upload the entire data set as a whole to any independent data repository. <mark>Thus, we investigate techniques for fine-grained referencing that enable the exact reconstruction of a researcher's data set and suit an environment of distributed data sources with access restrictions and different curatorial versions of data</mark>. As it motivates the application of Semantic Web technologies, we examine the emerging RDF Data Cube Vocabulary which integrates the SDMX standard for a harmonized representation of statistical data. In addition to statistical data resources, empirical research data sets in economics also comprise scripts for data processing. To support transparency to the highest level, we extend the scope of our research and elaborate a generating model, which enables the reproduction of analyses and results for a given scientific publication. All in all, we aim to lay the grounds for machine-processible descriptions of conducted empirical research to enable the automatic reconstruction of individual statistical data sets and reproduction of results. An overview of the work at its latest stage will be presented in this talk.",SWIB,,,,,,,,,,
,2012,Status Quo and Limitations of Library Linked Data,"Asunción Gómez-Pérez,Phillip Cimiano,Daniel Vila-Suero","https://swib.org/swib12/slides/Vila-Suero_SWIB12_106.pdf,http://www.scivee.tv/node/55386",SWIB/SWIB2012/Status Quo and Limitations of Library Linked Data.md,"#limitations,#library/linked/data,#swib/2012,#library-linked-data","In recent years, many libraries, museums and archives have started to release data as Linked Data (LD). The benefits of publishing library data as LD have recently been summarized by the W3C Incubator Group on Linked Library Data (W3C LLD XG). Several national libraries have started to publish metadata as LD. However, a number of limitations still exist which prevent from bringing the Library Linked Data (LLD) paradigm to its full potential:",SWIB,,,,,,,,,,
,2012,The Library Catalogue as Linked Open Data : How to Do It and What to Do with It,"Asgeir Rekkavik,Benjamin Rokseth","https://swib.org/swib12/slides/Rekkavik_SWIB12_110.pdf,http://www.scivee.tv/node/55304",SWIB/SWIB2012/The Library Catalogue as Linked Open Data - How to Do It and What to Do with It.md,"#norway,#frbr,#frbr-clustering,#swib/2012,#authority/file,#APIs,#API,#Oslo","Oslo Public Library has developed an open source toolkit for producing an RDF representation of the library catalogue and its authority files. In addition to plain conversion of MARC bibliographic data into RDF, the current implementation includes SPARQL methods for FRBR clustering of the library collection, as well as enriching catalogue data with external content, such as cover images and book reviews, from various APIs and Linked Open Data sources.
During the first half of 2012, Oslo Public Library has run two simultaneous projects that demonstrate and take advantage of some of the possibilities an enriched RDF representation of the library catalogue might provide. This has resulted in two working open source service prototypes:",SWIB,,,,,,,,,,
,2012,The Library of Congress's Bibliographic Framework Initiative,Kevin Ford,"https://swib.org/swib12/slides/Ford_SWIB12_128.pdf,http://www.scivee.tv/node/55383",SWIB/SWIB2012/The Library of Congress's Bibliographic Framework Initiative.md,"#library-of-congress,#migration,#marc-migration,#swib/2012,#Bibliographic/Framework/Initiative,#MARC","This presentation will provide a general update about the Library of Congress's Bibliographic Framework Initiative, including a short synopsis of its historical import, information about the attractiveness of Linked Data, and details about the work – models, tools, and findings - completed to date about transitioning from a MARC-based environment to a new bibliographic ecology. The general update about the Bibliographic Framework Initiative will include work and progress since the Library's update at the American Library Association Annual Conference in Anaheim in June. The first phase of the Initiative is complete. Among many objectives set for the initial phase, the most relevant outcome is the presentation of a draft model for community appraisal and on which to further build. Because an open process is desirable and expected, the community will already have seen the draft model, but this forum will provide an opportunity to go into greater detail, present the outcomes as a cohesive whole, and explore the ramifications and future directions stemming from the initial stage. Linked Data methods and strategies are proving to provide a very consistent yet flexible means to communicate data, which has always been one of the main aims of the MARC communication formats.",SWIB,,,,,,,,,,
,2012,Towards an Infrastructure for the Synchronisation of Metadata in Libraries,Christoph Böhme,"https://swib.org/swib12/slides/B%C3%B6hme_SWIB12_113.ppt,http://www.scivee.tv/node/55388",SWIB/SWIB2012/Towards an Infrastructure for the Synchronisation of Metadata in Libraries.md,"#synchronisation,#library/metadata,#swib/2012,#metadata","With LOD information is represented in a giant distributed graph. This graph is constantly changing and evolving but to date no infrastructure has been established for continuously propagating and tracking the changes of nodes and edges in this graph. In libraries, metadata is traditionally distributed and synchronised using protocols such as Z39.50, SRU or OAI-MPH to poll individual datasets for changes. While this works well when only a small number of datasets exchanges data infrequently, it does not when synchronisation happens continuously and at the scale of the LOD graph. However, users expect that data is always up-to-date. For example, a search index which includes external information about the latest headlines is expected to contain not only last week’s headlines but also today’s. Constant polling for changes can constitute a major performance issue in such scenarios. In our contribution we discuss different synchronisation patterns for library metadata and the requirements for a synchronisation infrastructure arising from them. Of particular importance in this discussion is the fact that subscribers are not always interested in all changes to a dataset but only in those affecting a small set of selected records. Related to this is the aspect that changes of library metadata most of the time only affect single records but sometimes large updates touch whole data sets producing change sets which can consist of millions of records at a time. A synchronisation system must be able to handle these different sized payloads and help participating systems to cope with large change sets. We review existing solutions and outline a future solution for the library domain.",SWIB,,,,,,,,,,
,2012,Wikidata - the Wikipedia of Linked Open Data,Daniel Kinzler,"https://swib.org/swib12/slides/Kinzler_SWIB12_127.pdf,http://www.scivee.tv/node/55318",SWIB/SWIB2012/Wikidata - the Wikipedia of Linked Open Data.md,"#Wikidata,#swib/2012,#transclusion","Wikidata is a new project by Wikimedia Deutschland with the goal to create a data repository for Wikipedia and the world. It aims to be Wikimedia Commons for data, allowing Wikipedia editors to put factual information like the population of a city in one central database, instead of having to maintain it as text in dozens or hundreds of languages. This presentation gives an overview of the planned software architecture of Wikidata and how it ties in Wikipedia. We want to explain how we are going to address the many technical and conceptual challenges that arise from the complexity and scale of the data. Among other things, we will describe how data records are transcluded between wikis, and how changes are recorded and propagated throughout the system. Another important focus is the data model Wikidata will use to represent the diversity of knowledge throughout the world.",SWIB,,,,,,,,,,
,2013,AgriVIVO: A Global Ontology-Driven RDF Store Based on a Distributed Architecture,"Valeria Pesce,Johannes Keizer,John Fereira",SWIB/SWIB2013/AgriVIVO - A Global Ontology-Driven RDF Store Based on a Distributed Architecture.md,"#agrivoc,#AgriVIVO,#VIVO,#FOAF,#BIBO,#ORCID,#ontology,#rdf,#swib/2013","The proposed talk will describe the rationale behind and the technical implementation of the AgriVIVO store and portal (www.agrivivo.net). AgriVIVO is an RDF-based and ontology-driven global search portal harvesting from distributed directories of experts, organizations and events in the field of agriculture. It is based on the VIVO open source semantic web application initially developed at Cornell University and now adopted by several institutional and cross-institutional research discovery projects in the US and beyond. AgriVIVO builds on the VIVO ontological model - which includes standard ontologies like the geopolitical ontology for locations, the FOAF vocabulary for people and the BIBO ontology for publications, besides the specialized VIVO core ontology - and extends it to better suit the agricultural domain. One of the objectives for this global directory is to provide authoritative data on people and organizations in the agricultural fields: the next steps will focus on disambiguation techniques and the adoption of universal identifiers, especially ORCID, to univocally identify experts and link them to publications, thus laying the ground for authority data on authors. More in details, the talk will cover: the reasons for choosing VIVO over other solutions; the methodology used to extend the VIVO ontology; the workflows and technical solutions adopted for harvesting from different databases; the work done for harmonizing data coming from systems using different semantic organizations; the architecture of the system; URIs, RDF profiles and Linked Data.",SWIB,,,,,,,,,,,
,2013,Analysis of Library Metadata with Metafacture,Christoph Böhme,https://swib.org/swib13/slides/boehme_swib13_131.pdf,SWIB/SWIB2013/Analysis of Library Metadata with Metafacture.md,"#manufacture,#java,#culturegraph,#project,#metadata/transformation,#indexing,#swib/2013","[Metafacture](http://culturegraph.github.io/) is a versatile Java-based open source toolkit for all metadata related tasks. It was developed in the [Culturegraph](http://culturegraph.org/) project. Since then it has become an important part of the software infrastructure at the German National Library. Core applications of Metafacture are metadata transformation, search indexing and statistical analyses of metadata. Despite originating from the library domain, Metafacture is format-agnostic and has successfully been employed in metadata related tasks in other domains.   In this workshop participants will learn how to use Metafacture in order to analyse large datasets. After a short introduction to Metafacture three types of analyses, which are often encountered in day-to-day work at a library, will be presented: counting distinct data values, quantifying relationships between metadata records and joining metadata records. Participants will have the opportunity to perform these analyses themselves and to discuss their approaches. Prior Experience: No programming experience is required. Participants should have a basic understanding of XML. A library background is not necessary as the analyses presented in the workshop are applicable to other areas as well.",SWIB,,,,,,,,,,
,2013,Application of LOD to Enrich the Collection of Digitized Medieval Manuscripts at the University of Valencia,"Jose Manuel Barrueco Cruz,Cristina García Testal",SWIB/SWIB2013/Application of LOD to Enrich the Collection of Digitized Medieval Manuscripts at the University of Valencia.md,"#valencia,#spain,#lod,#manuscripts,#digitization,#swib/2013","In this presentation we describe a use case of consumption and integration of LOD into a productive library application at the University of Valencia (Spain). The library has an important collection of medieval manuscripts and early printed books that is being digitized at the moment. All digital materials are made available open access as part of the institutional repository. A book reader allows the users to navigate following the physical or logical structure of the work, to access its illustrations or to connect with related documents. In order to go a step forward, the aim of the project being described here was to enrich the experience of the reader by providing additional information extracted from LOD data sets. The idea was that the user could get biographical information about the authors, illustrators, copyist… when accessing the digital version of the books. For that purpose an application was developed to link the book viewer with the LOD sources containing related information. The presentation has two parts. Firstly, we do an analysis of the available LOD sets, the criteria used to select the data sources for the project, the problems faced by the consumer when discovering or searching for LOD sets, and finally the sources selected. In the second part, we describe the algorithm of the application that integrates data extracted from different sources into the book reader. As a test the application was applied to a collection of 92 manuscripts. We make an analysis of the available information and the degree of coverage of the data sources in relation to the number of authors available in the repository.",SWIB,,,,,,,,,,,
,2013,Automatic Creation of Mappings between Classification Systems for Bibliographic Data,Magnus Pfeffer,SWIB/SWIB2013/Automatic Creation of Mappings between Classification Systems for Bibliographic Data.md,"#classification,#classification/systems,#CultureGraph,#mappings,#linked/data,#co-occurrence,#clustering,#swib/2013","Classification systems are an important means to provide topic-based access to large collections. Most library collections, however, are often only partially classified and use local or regional classification systems. Traditionally, manually created mappings between classification systems are used to improve this situation. I propose a different approach to automatically create such mappings: To achieve a large base for the mapping algorithm, bibliographic data from diverse sources that contain items classified by the classification systems is aggregated in a single database. Next, a clustering algorithm is used in order to group individual issues and editions of the same work. The basic idea is that for classification purposes, there is no significant difference across editions. Indexing information can thus be consolidated within the clusters, resulting in a higher proportion of dual-indexed entries. The novel step is that instead of individual catalogue entries, the ""work-level"" clusters are used for an instance-based matching: Statistical analysis creates a co-occurrence table of pairs of classes and high co-occurance of a given pair indicating a match between the two classification systems. This information is aggregated into a complete mapping The approach is implemented on an open-source infrastructure which was mainly developed by the German National Library: CultureGraph.org. In ongoing projects, mappings between several classification systems are being produced. The talk will discuss the approach, the implementation issues and the preliminary results as well as the challenges of publishing the created mappings as linked data.",SWIB,,,,,,,,,,,
,2013,BIBFRAME: Libraries Can Lead Linked Data,"Lars G. Svensson,Julia Hauser,Reinhold Heuvelmann",SWIB/SWIB2013/BIBFRAME - Libraries Can Lead Linked Data.md,"#bibframe,#alamw13,#semantic/web,#linked/data,#edm,#schema-org,#swib/2013","""There won't be any Semantic Web that any mortal can understand until librarians help build it! \#bibframe \#alamw13 \#semanticweb \#linkeddata"" (Uche Ogbuji of zepheira.com via twitter, 2013-01-27). As a result of the increasing level of digital interconnection in the information world, the established formats used by libraries for exchanging data are no longer deemed fit for purpose. Therefore the Library of Congress launched the ""Bibliographic Framework Transition Initiative"" (BIBFRAME) in 2011, aimed at developing the existing formats into a durable format. The purpose is to assess the viability of the formats, bearing in mind the opportunities presented by semantic web technologies. As a member of the ""Early Experimenters Group", the German National Library is actively supporting the initiative as part of its efforts to internationalize the German standards. In the first phase of the BIBFRAME project the main objective of the German National Library is to contribute to the BIBFRAME point papers and to glean experience from prototypical conversion of the German National Library data into the BIBFRAME format. A high potential lies in developing the interoperability with communities from outside the library community and also commercial providers, so that applications can re-use and benefit from the valuable data. Therefore, among other points, the compatibility of the BIBFRAME-model with the Europeana Data Model (EDM) and schema.org is examined. The presentation will give an overview of the BIBFRAME initiative," and provide some insights of the steps taken at the German National Library and in the context of the wider community.""",SWIB,,,,,,
,2013,Building a National Ontology Infrastructure,"Matias Frosterus|Matias Mikael Frosterus,Mirja Anttila,Mikko Lappalainen,Susanna Nykyri,Tuomas Palonen,Sini Pessala",SWIB/SWIB2013/Building a National Ontology Infrastructure.md,"#finland,#ONKI,#ontology,#koko,#yso,#ALLSO,#swib/2013","The National Library of Finland, in collaboration with the Ministry of Finance and the Ministry of Education and Culture, has launched a project called ONKI which aims to build a national-level ontology service. ONKI provides a stable, centralized ontology service that allows for the publication of ontologies, searching for them, and provides common ways of utilizing them through various interfaces. The immediate aim is to enable the use of ontologies in the information retrieval systems in, e.g., libraries, museums, archives, governmental bureaus and other public organizations. Through the use of common definitions and URIs for the concepts, the integration between various content becomes simpler. All the code we produce is open source. The other focus of the ONKI project is building and refining the Finnish General Upper Ontology YSO and its Swedish counterpart ALLSO, which have been in use since the early 2000s. Now we wish to evaluate the current state of YSO and whether its hierarchy serves its envisioned usage. To this end we will compare it to similar work done in other countries and also conduct interviews with various user groups. The final vision is to make YSO a national general upper ontology providing the upper level hierarchy as well as concepts that are common to all domains. This is then complemented by various more specific domain ontologies. We have implemented this structure in the form of KOKO, a combination of YSO and fifteen different domain ontologies ranging from agriculture to health to seafaring. This KOKO is already in use in, e.g., various museums as well as in pilot use in the national broadcasting company YLE.",SWIB,,,,,,,,,,,
,2013,"Catmandu - Library Oriented Extract, Transform, Load tools to publish Linked Open Data","Nicolas Steenlant,Patrick Hochstenbach,Najko Jahn",https://swib.org/swib13/slides/jahn_swib13_128.pdf,"SWIB/SWIB2013/Catmandu - Library Oriented Extract, Transform, Load tools to publish Linked Open Data.md","#etl,#catmandu,#marc,#JSON,#ElasticSearch,#swib/2013","Creating any data oriented application the main task is to import data from various sources, map the fields to a common data model and put it all into a database or search engine. In data-warehousing, these processes are called ETL — Extract, Transform, Load. Catmandu provides a suite of Perl modules to ease the import, storage, retrieval, export and transformation of metadata records.  
After a short introduction in the rationales of Catmandu and presentation of sample applications at the Universities of Lund, Ghent and Bielefeld, participants will be guided to transform MARC records to Linked Data.  
The steps include transforming MARC into a JSON model of choice, storing/indexing the model in ElasticSearch, and exporting/mapping the model as Linked Data.  
_Prior experience:_ We will be using a simplified ETL language. Any programming experience is welcome but not required. A brief tutorial on Catmandu programming can be found [here](http://librecat.org/tutorial/).  
_Requirements:_ Laptop with [VirtualBox](https://www.virtualbox.org/) installed. Organisers will prepare a virtualbox image (Linux guest system) beforehand to be worked with during the workshop.",SWIB,,,,,,,,,,
,2013,CouchDB: A Database for the web,Sven-S. Porst,https: //gist.github.com/ssp/7644684,SWIB/SWIB2013/CouchDB - A Database for the web.md,"#couchdb,#javascript,#RDF,#content-negotiation,#swib/2013","[CouchDB](http://couchdb.apache.org/) is a document database with a http interface, versioned documents, [map/reduce](http://en.wikipedia.org/wiki/MapReduce) and replication. With this web native approach, its ability to define additional output formats in JavaScript, and built-in content-negotiation ability, it is a handy technology to use in a linked data context. In this hands-on workshop participants will learn how to get documents into and out of CouchDB, how to analyse and list them in structured ways using map/reduce, and how to use these features in web applications. Special focus will be put on CouchDB’s abilities at supporting http content-negotiation for serving the appropriate output format. Participants will use their knowledge to create their own database and configure it to implement a linked data service with HTML and RDF output for their documents right in CouchDB. Time permitting, we will go on and explore data updating/synchronisation using CouchDB’s replication feature or creating list output for their data (e.g. Beacon files). Prior experience: Participants will need to work with [http](http://en.wikipedia.org/wiki/Http) headers and request methods, JavaScript and [JSON](http://www.json.com/). These will be introduced briefly in the workshop but basic knowledge of these topics is recommended. Requirements: Participants are expected bring their own computer.",SWIB,,,,,,,,,,
,2013,Cross-Lingual Semantic Mapping of Authority Files,"Nadine Steinmetz,Magnus Knuth,Harald Sack",SWIB/SWIB2013/Cross-Lingual Semantic Mapping of Authority Files.md,"#nem,#entity/mapping,#knowledge/base,#dbpedia,#swib/2013","One essential application of the Semantic Web is Named Entity Mapping (NEM). Named Entities within texts are recognized and annotated with corresponding entities from a knowledge base. Due to ambiguity of natural language additional information about the named entities is needed as context in order to enable disambiguation between several possible entity candidates. However, to enable semantic analysis of texts in different languages, the applied knowledge base should also provide multi-lingual labels. Unfortunately, most knowledge bases extracted from existing authority files do not fulfill these requirements. DBpedia represents an entity hub within the Linked Data cloud, where semantic information is provided by Wikipedia article texts, page links and interlanguage links. Authority files and in particular knowledge bases based on these authority files would clearly benefit from properly linking their entities to DBpedia. In this talk, we introduce the research issue of cross-lingual entity mapping and propose new methods how to perform high precision mappings of existing authority files to DBpedia. Based on already existing mappings of authority files, such as e.g. persons, locations, or organizations, this contribution concentrates on the mapping of entities that are not assigned to one of these classes.",SWIB,,,,,,,,,,,
,2013,"Decentralisation, Distribution, Disintegration - towards Linked Data as a First Class Citizen in Libraryland",Martin Malmsten,"SWIB/SWIB2013/Decentralisation, Distribution, Disintegration - towards Linked Data as a First Class Citizen in Libraryland.md","#linked/data,#libris,#catalogue,#lessons/learned,#sweden,#swib/2013","So, you've converted your data and published it as beautiful Linked Data (LD), now what? As early adopters of Linked Data, libraries have an opportunity to reap the benefits, namely to consume and relate to information in this big, networked graph; to change how the library works with information. This begs the question: is your tech and organisation up to the challenge? Can you deal with changes outside your control without resorting to aggregation and shuffling large batches of records every night? Does your boss even care? In (re)building the new LIBRIS catalogue backend and cataloguing interface we chose Linked Data as the primary technology, making for example MARC something that happens on the outside of the system, rather than being an integral part of it. This keynote will focus on the motivation, design and lessons learned while making Linked Data a first class citizen at the National Library of Sweden.",SWIB,,,,,,,,,,,
,2013,Enhancing an OAI-PMH Service Using Linked Data: A Report from the Sheet Music Consortium,Stephen Davison,SWIB/SWIB2013/Enhancing an OAI-PMH Service Using Linked Data - A Report from the Sheet Music Consortium.md,"#music,#mods,#OAI-PMH,#pilot,#swib/2013","The Sheet Music Consortium (http://digital2.library.ucla.edu/sheetmusic/) is a collaboration of universities and other sheet music repositories to publish an online metadata catalog of sheet music in their digital collections. Through utilization of Linked Open Data principles and standards we aim to improve the normalization of the Consortium's metadata, thereby expanding its impact and our ability to share it more widely and effectively both directly with our users and through automated systems. We outline the steps to take the Sheet Music Consortium metadata from its current incarnation as MODS XML files and publish it as linked open data. The Consortium has developed a plan to publish trustworthy data records that are derived from both harvested and user-contributed metadata. First, we identified four metadata elements to target as sources for linked data. These elements are: names of creators, titles of songs, publishers, and subjects. Second, we concluded that despite the high level of attention this metadata has received in its creation and its inclusion in the Consortium, it still is not sufficiently normalized for export to a linked data standard. We will review our strategies for normalizing the metadata values. Third, we will discuss the various methods of employing linked data in the OAI-PMH context, including schema.org and RDF and RDFS. Lastly, we will present the results of a pilot project focusing on publisher information, to take metadata from the Consortium and present it as linked open data. While this case study is focused on sheet music, the methods discussed are generally applicable in the context of harvested metadata.",SWIB,,,,,,,,,,,
,2013,Exposing Institutional Repositories as Linked Data - a Case-Study,"Vitali Peil,Christian Pietsch,Najko Jahn",SWIB/SWIB2013/Exposing Institutional Repositories as Linked Data - a Case-Study.md,"#Bielefeld,#germany,#pub,#OAI-DC,#MODS,#RDF,#VIVO,#swib/2013","An institutional publication data service such as Bielefeld University's PUB (http://pub.uni-bielefeld.de) is a natural starting point for a linked data network. Institutional repositories are no longer restricted to traditional publications. Some repositories have begun to include research data and their metadata. So does PUB. Since research data and publications are related, it is very natural to present this as linked data. By exposing this data and its relationships through standard interfaces, we have opened up PUB to the larger LOD cloud. This will overcome the restrictions of interoperability between repositories. The PUB software is implemented in the open source Perl framework LibreCat/Catmandu (http://github.com/LibreCat). As a first step, we implemented a content negotiation mechanism, a widely used feature in the semantic web. All standard bibliographic formats are supported: OAI-DC, MODS, BibTeX, RIS, and also the JSON and YAML representations of our internal data structure. The RDF representation of the metadata is using the widely used vocabularies BIBO, DCTERMS and FOAF. Future work will involve the linking of the PUB data to external resources, e.g. authority data of the German National Libary, the cross linking to ORCID, Europe PMC or arXiv. Another developing branch will be the interlinking of publications with research data, either contained in our own repository or in an external one. The contextualization of publications and research data in the University's structure will be feasible as soon as the administrative data is published as linked data using VIVO.",SWIB,,,,,,,,,,,
,2013,From Strings to Things: A Linked Open Data API for Library Hackers and Web Developers,"Fabian Steeg,Pascal Christoph",SWIB/SWIB2013/From Strings to Things - A Linked Open Data API for Library Hackers and Web Developers.md,"#LODLAM,#knowledge/graph,#hbz,#LOD,#API,#JSON-LD,#use/cases,#Metafacture,#Elasticsearch,#swib/2013","""Things, not strings"" is a popular slogan pushed by Google in the context of its knowledge graph. The LODLAM community advocates this idea, in particular in the context of authority data. However, if we want users to catalog and search things, not strings, we need to make it easy to go from strings to things. At hbz, we are working on a LOD API in order to allow this for title and authority data - currently for the hbz union catalog (lobid-resources), the German ISIL registry (lobid-organisations), and the German integrated authority file (GND). The API serves JSON-LD over HTTP. JSON-LD is an attempt to make LOD more accessible to web developers who are not familiar with semantic web concepts. Providing a common HTTP API instead of a SPARQL endpoint or RDF dumps serves the same purpose: to make the data and advantages of LOD available to all web developers, and not to semantic web experts only. In this talk, we will present use cases for the API (like an auto-suggest functionality for authority data), and describe our implementation and the technology stack we use: metadata transformation to N-Triples with the Culturegraph Metafacture toolkit, enrichment and conversion of N-Triples to JSON-LD with Hadoop, indexing JSON-LD with Elasticsearch, and building a web API with the Play framework.",SWIB,,,,,,,,,,,
,2013,"HEAL-Link Activities and Plans on Annotating, Organizing and Linking Academic Content","Nikolas Mitrou,Nikolaos Konstantinou,Dimitrios Kouis,Periklis Stavrou","SWIB/SWIB2013/HEAL-Link Activities and Plans on Annotating, Organizing and Linking Academic Content.md","#union/catalogue,#catalogue,#knowledge/base,#learning/objects,#handbooks,#heal-link,#education,#faceted/search,#thesauri,#swib/2013","The Hellenic Academic Libraries Association (HEAL-Link) offers to its members (50 academic and research libraries in Greece) a wide range of added-value services, including a union catalog with several millions of MARC-coded records. Recent activities of HEAL-Link aim at bringing its member libraries into the digital era. Pioneering among HEAL-Link’s activities is the development of an academic content aggregator in the framework of a newly started project, aiming at the creation of 2,000 ebooks, available to higher education curricula. The vision is to create a semantic knowledge base comprising several thousands of autonomous learning objects, each of important educational value but, most importantly, with the ability to be linked to, combined, and reused, enabling composition of handbooks covering more courses. From the point of view of the student or scholar, it is to his/her benefit to be able to discover and study, not from only one but from various sources, and to compile his/her own textbook out of selected resources. Faceted navigation and search among the learning objects as well as other linked elements on the web at a fine granularity level, by using thesauri and employing educational attributes, like the type and degree of interactivity, the intended user audience and the degree of difficulty, is of paramount importance in the above context. In close collaboration with the NTUA Multimedia Communications & Web Technologies (MCWT) team, the HEAL-Link is actively pursuing research in the Linked Data domain, and specifically interoperability between conventional digital library systems and educational content repositories.",SWIB,,,,,,,,,,,
,2013,Introduction to Linked Open Data,"Felix Ostrowski,Adrian Pohl",https://swib.org/swib13/slides/ostrowski_swib13_132.pdf,SWIB/SWIB2013/Introduction to Linked Open Data.md,"#linked/data,#technology,#rdf,#dereferenceable,#content-negociation,#open-data,#open/data,#swib/2013","This introductory workshop aims to introduce the fundamentals of Linked Data technologies on the one hand, and the basic legal issues of Open Data on the other. The RDF data model will be discussed, along with the concepts of dereferenceable URIs and common vocabularies. The participants will continuously create and refine RDF documents to strengthen their knowledge of the topic. Linked Data tenets such as publishing RDF descriptions in a web environment and utilizing Content-Negotiation will be demonstrated and applied by the participants. Aggregating data from several sources and querying this data will showcase the advantages of publishing Linked Data, and RDF Schema will be introduced as an effective way of data integration. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop.",SWIB,,,,,,,,,,
,2013,Linked Data Publication with Drupal,Joachim Neubert,https://swib.org/swib13/slides/neubert_swib13_112.pdf,SWIB/SWIB2013/Linked Data Publication with Drupal.md,"#drupal,#RDFa,#REST-ful,#swib/2013","Publishing Linked Open Data in a user-appealing way is still a challenge: Generic solutions to convert arbitrary RDF structures to HTML out-of-the-box are available, but leave users perplexed. Custom-built web applications to enrich web pages with semantic tags ""under the hood"" require high efforts in programming. Given this dilemma, content management systems (CMS) could be a natural enhancement point for data on the web. In the case of [Drupal](http://drupal.org/), one of the most popular CMS nowadays, Semantic Web enrichment is provided as part of the CMS core. In a simple declarative approach, classes and properties from arbitrary vocabularies can be added to Drupal content types and fields, and are turned into Linked Data on the web pages automagically. The embedded RDFa marked-up data can be easily extracted by other applications. This makes the pages part of the emerging Web of Data, and in the same course helps discoverability with the major search engines.  \nIn the workshop, you will learn how to make use of the built-in Drupal 7 features to produce RDFa enriched pages. You will build new content types, add custom fields and enhance them with RDF markup from mixed vocabularies. The gory details of providing LOD-compatible ""cool"" URIs will not be skipped, and current limitations of RDF support in Drupal will be explained. Exposing the data in a REST-ful application programming interface or as a SPARQL endpoint are additional options provided by Drupal modules. The workshop will also introduce modules such as Web Taxonomy, which allows linking to thesauri or authority files on the web via simple JSON-based autocomplete lookup, or SPARQL Views as a mighty tool for reaching out to other Linked Data sources. Finally, we will touch the upcoming Drupal 8 version.",SWIB,,,,,,,,,,
,2013,"Linked Data for Libraries: Great Progress, but What Is the Benefit?",Richard Wallis,"SWIB/SWIB2013/Linked Data for Libraries - Great Progress, but What Is the Benefit?.md","#linked/data,#bibframe,#schema.org,#wikidata,#library/linked/data,#swib/2013","SWIB can testify, probably more than any conference series, to the steady progress over several years in the development of linked data for libraries. Progress that is accelerating as we are evolving from individual projects from national and other large libraries towards cooperations and global initiatives around standard approaches and vocabularies. Richard will explore three such initiatives, BIBFRAME – The Bibliographic Framework Initiative from the Library of Congress to apply linked data principles to describe bibliographic resources - a potential replacement for MARC, that takes note of other standards such as RDA & FRBR; Schema.org - an approach, with the help of the W3C Group Schema Bib Extend, which Richard chairs, to apply the generic web vocabulary backed by the major search engines to the task of exposing bibliographic data in an easily consumable format, and; WikiData - The potential for the emerging data format that will underpin all Wikipedia, to represent bibliographic entities like any other. The key question that needs to be addressed however is what is the purpose of these efforts, beyond being a cool thing to do. What are the core problems we are trying to fix? Could efficiencies emerge as linked data, and the principles behind it, start to enter library workflows? How could the future landscape appear for library linked data, and how will it assist the prime basic goal of libraries - to help the potential users of our resources find them and use them. What is the benefit of linked data for libraries?",SWIB,,,,,,,,,,,
,2013,Mash-up for Book Purchasing,Philipp Zumstein,SWIB/SWIB2013/Mash-up for Book Purchasing.md,"#Z39.50,#SRU,#Greasemonkey,#scraping,#swib/2013","An everyday task in the library is to identify and purchase new books. In my talk I will present a mash-up which shows information about a specific book from different sources like the number of libraries holding the book already, price estimation and classifications. Our holding information, i.e. whether we already have the book or some professor has it on his shelf, is crucial here. We made some attempts to also handle our holdings information for e-book-packages or patron driven acquisition. The mashup we created uses linked data as well as the Z39.50/SRU protocol for receiving up-to-date information from libraries and some web scraping methods. In the current implementation the importance of Linked Data is rather small, nevertheless, possible benefits from more linked data are discussed in the talk. Additionally, I will show the use of Linked Data occurring from some newly published titles and how it is possible to link from almost every webpage to our holding information by using an adaption of a Greasemonkey script.",SWIB,,,,,,,,,,,
,2013,Metadata Provenance Tutorial,"Kai Eckert,Magnus Pfeffer",https://swib.org/swib13/slides/eckert_swib13_118.pdf,SWIB/SWIB2013/Metadata Provenance Tutorial.md,"#enrichment,#rdf,#PROV,#swib/2013","When metadata is distributed, combined, and enriched as Linked Data, the tracking of its provenance becomes a hard issue. Using data encumbered with licenses that require attribution of authorship may eventually become impracticable as more and more data sets are aggregated - one of the main motivations for the call to open data under permissive licenses like CC0. Nonetheless, there are important scenarios where keeping track of provenance information becomes a necessity. A typical example is the enrichment of existing data with automatically obtained data, for instance as a result of automatic indexing. Ideally, the origins, conditions, rules and other means of production of every statement are known and can be used to put it into the right context.",SWIB,,,,,,,,,,
,2013,On the Way to a Holding Ontology,"Carsten Klee,Jakob Voss",SWIB/SWIB2013/On the Way to a Holding Ontology.md,"#RDF,#BIBO,#ontologies,#micro-ontologies,#swib/2013","Modelling bibliographic data in RDF is widespread among libraries, thanks to common ontologies such as the Bibliographic Ontology (BIBO) and several years of practice. Holding data, however, is rarely modelled in RDF due to the lack of matching vocabularies. For this reason a new holding data group was formed in April 2013 as part of the German Kompetenzzentrum Interoperable Metadaten (KIM) within the Deutsche Initiative für Netzwerkinformation (DINI). This talk will present activity and work results of this group, including problems raised and respective solutions. Similar to the related KIM group 'Titeldaten (title data)', the holding data group is developing a best practice guide for holding data in RDF. The goal is to figure out, which data best belongs to a holding description and how this data should be modelled as Linked Data. The result is aligned with related efforts such as the DAIA, BIBFRAME, and FRBR with a strict focus on holdings of libraries and similar institutions. The emerging holding ontology is not going to be another ""über-model"" but it will fit into the concept of micro-ontologies, where each of the small ontologies describes an independent domain of data.",SWIB,,,,,,,,,,,
,2013,PhD workshop,"Timo Borst,Atif Latif",SWIB/SWIB2013/PhD workshop.md,"#PhD,#swib/2013","The PhD workshop at SWIB13 provides an excellent opportunity both for the beginner as well as the senior PhD student to present her or his ideas and receive feedback by experienced researchers and other PhD students working in research areas related to Linked Data based infrastructures and applications in libraries. The Linked Open Data approach aims at a framework for the generation, publishing and sharing of information by means of semantic technologies. It plays a vital role in the realization of the Semantic Web at a global scale by publishing and interlinking diverse data sources on the Web. The access to a huge amount of Linked Data presents exciting opportunities for the next generation of Web-based applications, especially with regard to data hosted and provided by libraries. Facing use cases as depicted by the Library Linked Data Incubator Group, however there is still a need of Linked Data applications and best practice examples. For this PhD workshop, we would like to discuss the initial ideas about issues in Linked Open Data that have proven to be both promising and challenging in the context of library use cases and applications.",SWIB,,,,,,,,,,,
,2013,ResourceSync for Semantic Web Data Copying and Synchronization,Simeon Warner,SWIB/SWIB2013/ResourceSync for Semantic Web Data Copying and Synchronization.md,"#Sitemaps,#sync,#ResourceSync,#swib/2013","The web is dynamic, with resources being created, updated, and deleted. Many applications that reuse web resources from remote sources require local copies to meet reliability and performance constraints. Commodity web search services use web crawling to harvest data. However, this alone may not provide low-enough latency, high-enough accuracy, or even be practical for particular datasets. The ResourceSync Framework http://www.openarchives.org/rs introduces a set a capabilities, based on Sitemaps, that a source may implement to enable clients to copy and keep in-sync with its resources. Semantic Web services are often built around local copies of harvested data collected and updated by ad-hoc means. ResourceSync provides a standard web-based way to enable such harvesting. It includes discovery mechanisms and supports a variety of use cases including different size, change frequency, and latency requirement regimes. The talk will give an overview of the framework and then focus on application to the Semantic Web.",SWIB,,,,,,,,,,,
,2013,Semantic Web Technology in Europeana,"Antoine Isaac,Pavel Kats,Péter Király,Yorgos Mamakis,Georgios Markakis,Valentine Charles",SWIB/SWIB2013/Semantic Web Technology in Europeana.md,"#EDM,#Europeana,#ingestion,#swib/2013","In this presentation we will discuss some technological and organisational challenges for implementing the semantic web vision in Europeana, focusing on metadata modelling, ingestion, and dissemination. Our first effort was a new data model (EDM), which has been developed and extensively tested with many Europeana partners over the past years. We have then spent major efforts into building an infrastructure, the Unified Ingestion Manager, supporting EDM, integrating a metadata mapping editor, and being able to harvest data from Europeana providers that refer to third-party Linked Data (thesauri, gazetteers,…). We have also experimented with semantic enrichment, linking objects to external sources: Geonames, GEMET, and DBpedia. With the results we were able to tune our search engine to perform semantic search functions, e.g., query expansion using hierarchies and translated labels. Finally, we have experimented with new ways of disseminating data. We released a pilot Linked Data service. We also publish RDFa/schema.org mark-up on our web portal. This is still a work in progress, and we are involved in the W3C Schema Bib Extend Group to stay aligned with community expectations. Working on the Europeana scale emphasizes the importance of many aspects beyond technology: cooperation, standardization, legal concerns, and more. Also, it shows that applying separate bits of the semantic web vision – as opposed to implementing the full technical stack at once – already brings benefits.",SWIB,,,,,,,,,,,
,2013,Soylent SemWeb Is People! Bringing People to Linked Data,Dorothea Salo,SWIB/SWIB2013/Soylent SemWeb Is People! Bringing People to Linked Data.md,"#RDF,#SoylentGreen,#data/entry,#data/storage,#data/discovery,#data/analysis,#linked/data,#people,#swib/2013","Tim Berners-Lee originally framed the Semantic Web as a depersonalized web of data and machinery humming away hidden in server closets. How much this lifeless, impersonal image contributed to RDF's initial adoption difficulties is open to question, but one clear lesson emerged: like the horrifying foodstuff from the cult 70s film ""Soylent Green", the Semantic Web is ultimately made of people. People model data, sometimes without even knowing they're doing it. People crosswalk data. People design and configure and use data-entry, data-storage, data-discovery, and data-analysis systems. Without all these people, there can be no Semantic Web. So how do we best invite people -- including skeptical people, reluctant people, less-technical people, people committed to different data structures -- to learn about, contribute to," and use Linked Data?""",SWIB
,2013,Specialising the EDM for Digitised Manuscripts,"Steffen Hennicke,Evelyn Dröge,Julia Iwanowa,Kai Eckert,Violeta Trkulja",SWIB/SWIB2013/Specialising the EDM for Digitised Manuscripts.md,"#EDM,#DM2E,#Europeana,#vocabularies,#swib/2013","The “Europeana Data Model” (EDM) serves as a generic semantic umbrella for the integration of heterogeneous metadata schemas from many different knowledge domains. In order to capture and retain semantics of specific knowledge domains, the model allows the integration of specific application profiles. The project “Digitised Manuscripts to Europeana” (DM2E) devised one of the first specialisations for the EDM: The DM2E model is an application profile for schemas about handwritten manuscripts. The model integrates standards as diverse as MARC/XML, TEI, EAD or MAB2 and captures specific semantics of manuscript descriptions in the humanities. Thereby, rich semantic statements about cultural heritage objects can be delivered to Europeana and made visible for a broad audience. By explicitly defining classes for datasets and published data resources, the model additionally adheres to Linked Data principles. Resource descriptions are first-class members of the data model and can be used for later user annotations and provenance tracking. In this presentation, we will provide an insight into the creation of the DM2E model: How can the EDM be specialised, which resources were needed for the manuscript domain and how can existing resources be reused? This can serve as guidance for the creation of other (specialised) vocabularies.",SWIB,,,,,,,,,,,
,2013,"The ""OpenCat"" Prototype: Linking Public Libraries to National Datasets",Agnès Simon,"SWIB/SWIB2013/The ""OpenCat"" Prototype - Linking Public Libraries to National Datasets.md","#BnF,#OpenCat,#project,#swib/2013","Libraries are already part of the linked data. The next step is to make this data useful for public libraries in a long term perspective. The BnF already displays bibliographic information according to the linked data principles, through data.bnf.fr, an open linked data project that gathers resources and information in pages about authors, themes, and works. Now the question is: how can local libraries take advantage of this project to improve their services to patrons? To experiment a realistic and prospective use case, the BnF started OpenCat, a R&D project financed by the Ministry of Culture of France, together with the Public Library of Fresnes, and a software company, Logilab. Starting from the local library’s functional requirements, a prototype has been built. Relying on permanent identifiers and structured data, it gathers: - information from data.bnf.fr: FRBRized data, links to digital documents, illustrations, - information from the local library like shelfmark, availability, links to subject headings, - snippets, timelines and links to external resources such as conferences, biographical information from the Académie Française and other resources available on the linked open data cloud. Thus new opportunities for cooperation between libraries and for creating a new business model among libraries are arising. This open-source prototype can be seen online with a demo. The next step is to try this prototype in other public libraries, with their own resources and to analyse how they can benefit by sharing resources on the Web, via OpenCat.",SWIB,,,,,,,,,,,
,2014,"All knowledge, annotated",Dan Whaley,"SWIB/SWIB2014/All knowledge, annotated.md","#swib/2014,#Open-Annotation,#W3C","This August the W3C chartered the Web Annotation Working Group, based around an RDF data model called Open Annotation, which is rapidly being integrated into new open source tools and software libraries and adopted by a diverse cross-section of scholars, scientists, educators and others. The potential is to create a new layer over the web as we know it, enabling a rich set of interactive capabilities that until now have not been possible. This talk will provide an overview of the history behind annotation as an essential idea of the web, demonstrate some of the ways its being used and suggest plans for further development.",SWIB,,,,,,,,,,,
,2014,Analysis and Transformation of Library Metadata with Metafacture,"Christoph Böhme,Pascal Christoph",SWIB/SWIB2014/Analysis and Transformation of Library Metadata with Metafacture.md,"#swib/2014,#Metafacture,#Culturegraph,#metadata,#workshop","Metafacture is a versatile Java-based open source toolkit for all metadata related tasks. It was developed in the Culturegraph project. Since then it has become an important part of the software infrastructure at the German National Library. Core applications of Metafacture are metadata transformation, search indexing and statistical analyses of metadata. Despite originating from the library domain, Metafacture is format-agnostic and has successfully been employed in metadata related tasks in other domains. In this workshop participants will learn how to use Metafacture in order to analyse large datasets. After a short introduction to Metafacture three types of analyses, which are often encountered in day-to-day work at a library, will be presented: counting distinct data values, quantifying relationships between metadata records and joining metadata records. Participants will have the opportunity to perform these analyses themselves and to discuss their approaches. Prior Experience: No programming experience is required but participants should have advanced computer skills. Participants should have a basic understanding of XML. A library background is not necessary as the analyses presented in the workshop are applicable to other areas as well. Requirements: A laptop with a VirtualBox installation is required (or any other virtualisation environment that can open OVA-files). Alternatively, users can install a custom Eclipse package prior to the workshop.",SWIB,,,,,,,,,,,
,2014,Applying a Dublin Core application profile to the digital Pina Bausch archive for ontology population management and data presentation purposes,"Kerstin Diwisch,Bernhard Thull",SWIB/SWIB2014/Applying a Dublin Core application profile to the digital Pina Bausch archive for ontology population management and data presentation purposes.md,"#swib/2014,#application-profiles,#DCMI","In recent years, it has become common practice in Linked Data application development to utilize application profiles for describing and constraining metadata and structure of these applications. Description Set Profiles (DSP), as proposed by the Dublin Core Metadata Initiative (DCMI) as a means of machine readable formalizations of application profiles, take this development into account. However, even with the use of application profiles, Linked Data application developers are still left with challenges concerning data input or presentation of data, especially for large datasets. An example of a large dataset is the digital Pina Bausch archive which is realized as a Linked Data archive containing data on diverse materials such as manuscripts, choreography notes, programmes, photographs, posters, drawings and videos related to Pina Bausch’s work, adding up to about 20 million triples. To address this topic, we developed and implemented a Description Set Profile in RDF containing metadata structure and constraints for the Pina Bausch archive ontology. Initially, the DSP was mostly used as a controller for an interactive form-based data editor. In the course of the project, we enhanced the DSP with information about frontend data presentation in a form that by now, it is not only used for controlling the data input flow but also as part of a controller for our archive browser. In this talk, we will provide an insight into the development of the DSP and discuss the approach.",SWIB,,,,,,,,,,,
,2014,"Catmandu - Importing, transforming, storing and indexing data should be easy","Johann Rolschewski,Jakob Voß",https://swib.org/swib14/slides/rolschewski_swib14_7.zip,"SWIB/SWIB2014/Catmandu - Importing, transforming, storing and indexing data should be easy.md","#swib/2014,#Catmandu,#MongoDB,#Elasticsearch","[Catmandu](https://github.com/LibreCat/Catmandu/wiki) provides a suite of software modules to ease the import, storage, retrieval, export and transformation of metadata records. Combine Catmandu modules with web application frameworks such as PSGI/Plack, document stores such as MongoDB and full text indexes such as Elasticsearch to create a rapid development environment for digital library services. After a short introduction to Catmandu and its features, we will present the domain specific language (DSL) and command line interface (CLI). Participants will be guided to transform (their) data records to a common metadata model, to store/index it in Elasticsearch or MongoDB and to export it as Linked Data. Prior Experience: We will be using a simplified DSL language. Participants should be familiar with command line interfaces (CLI). Any programming experience is welcome but not required. Requirements: Laptop with VirtualBox installed. Organisers will provide a virtualbox image (Linux guest system) beforehand. Participants should bring their own data (CSV, JSON, MAB2, MARC, PICA+, RDF or YAML).",SWIB,,,,,,,,,,
,2014,"Entification: The Route to ""Useful"" Library Data",Richard Wallis,SWIB/SWIB2014/Entification - The Route to 'Useful' Library Data.md,"#swib/2014,#british-library,#BL,#entitites","'Linked Data is all about identifying 'things' then describing them and their relationships in a web of other 'things', or entities. Many library linked data initiatives have focused on directly transforming records into RDF with little linking between the shared concepts captured within those records or to external authoritative representations of the same things. The British Library, with a linked data version of the British National Bibliography, was an early pioneer in attempting to model real world entities as a foundation for their data model. Similar research within OCLC, that led to the release of entities as open linked data from WorldCat.org, such as Works, has demonstrated the benefits of such an approach. It also demonstrates that there is much more than record-by-record format conversion required to successfully achieve a web of real world entities. Significant data mining processes, the open availability of authoritative data hubs (such as VIAF, FAST, Library of Congress), and the use of flexible and widely accepted vocabularies, all play a necessary part in this success. Richard will explore some of the issues and benefits of creating library data as descriptions of real world entities, and share some insights into the processes required and their results.'",SWIB,,,,,,,,,,,
,2014,Entity Facts - A light-weight authority data service,"Christoph Böhme,Michael Büchner",SWIB/SWIB2014/Entity Facts - A light-weight authority data service.md,"#swib/2014,#germany,#entity-facts,#entities,#GND,#JSON-LD,#enrichment","The German National Library has published a new web service called Entity Facts. The main goal of Entity Facts is to provide aggregated information about entities from various sources in a way that makes it easy to present this data to the user. The information provided by Entity Facts is based on the Integrated Authority File (Gemeinsame Normdatei, GND) - the main authority file used in the German-speaking world - and merged with other sources such as Wikipedia, VIAF or IMDb. The information is provided as machine- and human-readable data in a straightforward and lightweight way via an Application Programming Interface (API). Our intention is to enable reuse of authority data for developers who do not have domain specific knowledge. This is realized through an easy to understand JSON-LD data model, which is providing ready-to-use data. Linking to and merging data from different sources offers new and ever improving possibilities for data enrichments. The infrastructure of the service is designed to extend and update the data sets easily. In our contribution we introduce the main goals, the present status and the features of Entity Facts, we plan to develop in future. The Deutsche Digitale Bibliothek (DDB) acts as a pilot partner and is the first client to use the service in a productive scenario. The data sets provided by Entity Facts (e.g. the data set provided for J. W. v. Goethe) establish the basis for the entity pages about persons in the DDB portal ( J. W. v. Goethe). A short demonstration will illustrate the current functionality. The presentation will close with a road map for the future development of Entity Facts.",SWIB,,,,,,,,,,,
,2014,Introducing RDFa with schema.org for web applications,Dan Scott,SWIB/SWIB2014/Introducing RDFa with schema.org for web applications.md,"#swib/2014,#rdfa,#schema-org,#workshop","Part 1 - Introducing RDFa: While the W3C issued the first RDFa recommendation specification in 2008, it remained a niche method for expressing linked data. However, the RDFa 1.1 specification in 2013 and the creation of RDFa Lite brought renewed interest into this means of uniting the semantic and the document web. This section of the workshop introduces the core concepts of RDFa Lite through a series of hands-on exercises that progressively enrich a sample document with RDFa structured data. Part 2 - Adding schema.org structured data via RDFa to library applications: The schema.org vocabulary was created by the major search engines to provide a common means of publishing metadata about things that normal humans search for, such as events, people, and products. While one advantage of adopting schema.org for your library applications is that your library may benefit from more precise and enriched search results for the resources it holds, you can also enhance access to your organizational information such as locations, operating hours, and employee listings. This section of the workshop introduces the schema.org vocabulary through a set of hands-on exercises addressing use cases that are common to libraries, such as marking up different resource types and publishing organizational information. Part 3 - Configuring a structured data search engine: Google's Custom Search Engine offers a quick to deploy, but also highly configurable search service that can take advantage of structured data. This section of the workshop will lead participants through creating a simple structured data search engine, then introduce some of the refinements that are available for more advanced use cases. Part 4 - Crawling and extracting structured data: For reasons such as privacy or control, it is often preferred to create one's own search engine. This section of the workshop introduces a simple web crawler written in Python that uses RDFLib to extract structured data and store it so that it can be indexed and retrieved. Prerequisites: Attendees are expected to have a working knowledge of HTML. Some familiarity with Python would be helpful for Part 4 but is not essential.",SWIB,,,,,,,,,,,
,2014,Introduction to Linked Open Data,"Felix Ostrowski,Adrian Pohl",https://swib.org/swib14/slides/ostrowski_swib14_45.pdf,SWIB/SWIB2014/Introduction to Linked Open Data.md,"#swib/2014,#workshop,#RDF","This introductory workshop aims to introduce the fundamentals of Linked Data technologies on the one hand, and the basic legal issues of Open Data on the other. The RDF data model will be discussed, along with the concepts of dereferencable URIs and common vocabularies. The participants will continuously create and refine RDF documents to strengthen their knowledge of the topic. Linked Data tenets such as publishing RDF descriptions in a web environment and utilizing Content-Negotiation will be demonstrated and applied by the participants. Aggregating data from several sources and querying this data will showcase the advantages of publishing Linked Data, and RDF Schema will be introduced as an effective way of data integration. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop.",SWIB,,,,,,,,,,
,2014,KOS evolution in Linked Data,Joachim Neubert,SWIB/SWIB2014/KOS evolution in Linked Data.md,"#swib/2014,#SKOS,#vocabularies,#STW,#thesaurus,#versioning","Over time, Knowledge Organization Systems such as thesauri and classifications undergo lots of changes, as the knowledge domains evolve. Most SKOS publishers therefore put a version tag on their vocabularies. With the vocabularies interwoven in the open web of data, however, different versions may be the base for references in other datasets. So, updates by ""third parties"" are required, in indexing data as well as in mappings from or to other vocabularies. Yet answers to simple user questions such as ""What's new?"" or ""What has changed?"" are not easily obtainable. Best practices and shared standards for communicating changes precisely and making them (machine-) actionable still have to emerge. STW Thesaurus for Economics currently is subject to a series of major revisions. In a case study we review the amount and the types of changes in this process, and demonstrate how versioning in general and difficult types of changes such as the abandonment of descriptors in particular are handled. Furthermore, a method to get a tight grip on the changes, based on SPARQL queries over named graphs, is presented. And finally, the skos-history activity is introduced, which aims at the development of an ontology/application profile and best practices to describe SKOS versions and changes.",SWIB,,,,,,,,,,,
,2014,Linked Open Data in Aggregation Scenarios: The Case of The European Library,Nuno Freire,SWIB/SWIB2014/Linked Open Data in Aggregation Scenarios - The Case of The European Library.md,"#swib/2014,#european-library,#MACS,#ontology,#aggregators","The paradigm of Linked Data, brings many new challenges to libraries. The generic nature of data representation used in Linked Data, while it seamlessly allows any community to manipulate the data, also brings many possible paths to its implementation. The European Library Open Dataset is derived from the collections aggregated from member libraries. The dataset is published as Linked Open Data (LOD) and made available under the Creative Commons CC0 license, in order to promote and facilitate the reuse of the data by all communities. This presentation describes the experience of The European Library in the creation of this linked dataset, the data model, and the perspectives on the benefits of linking library data in large aggregation contexts. The dataset includes national bibliographies, library catalogues, and research collections. It addresses the linking of subject heading systems widely used in Europe, by exploiting MACS (Multilingual Access to Subjects), since ontologies are key in many LOD applications, particularly in research. The task of creating LOD is demanding in terms of human and computational resources, and expertise in both information and computer science. Library aggregators provide an organizational environment where conducting LOD activities becomes less demanding for libraries. This kind of organization can leverage on existing information and communication technologies, the centralization of data, and their expertise in both library data and the semantic web.",SWIB,,,,,,,,,,,
,2014,Moving from MARC: How BIBFRAME moves the Linked Data in Libraries conversation to large-scale action,Eric Miller,SWIB/SWIB2014/Moving from MARC - How BIBFRAME moves the Linked Data in Libraries conversation to large-scale action.md,"#swib/2014,#bibframe,#vocabularies,#profiles","As the Library of Congress looked to the future of MARC, they looked to Linked Data principles and Semantic Web standards as the foundation of BIBFRAME. Libraries have an extensive history with MARC as a sophisticated and highly customized descriptive vocabulary with billions of records spread across systems and providers. In order to recognize the value of connecting this legacy in new and contemporary ways, BIBFRAME’s design is intentionally extensible with Profile-based vocabularies, flexible transformation utilities, and iterative linking strategies in mind. The migration from MARC (and other related Library standards) to BIBFRAME offers the most widely actionable opportunity for libraries to adopt Linked Data as a foundation of their Web visibly and internal operations. This session will include a review of practical tools we have used in helping libraries: evaluate their current data, define local data priorities, perform large-scale transformation, create profile-based definitions for original content, identify linking options, move beyond simply representing legacy data to take full advantage of the Linked Data nature of Web vocabularies like BIBFRAME and schema.org. We benefit from looking back at the history of how Libraries have helped shape the Web of Data to the future of how now given these standards, we together can raise the visibility of libraries on the Web.",SWIB,,,,,,,,,,,
,2014,Publish your SKOS vocabularies with Skosmos,"Henri Ylikotila,Osma Suominen",SWIB/SWIB2014/Publish your SKOS vocabularies with Skosmos.md,"#swib/2014,#Skosmos,#Finland,#thesauri,#ontologies,#classifications,#authority/files,#Finto,#ONKI,#API,#REST-API,#SPARQL,#PHP","Skosmos is an open source web-based SKOS browser being developed at the National Library of Finland. It can be used by e.g. libraries and archives as a publishing platform for controlled vocabularies such as thesauri, lightweight ontologies, classifications and authority files. The Finnish national thesaurus and ontology service Finto is built using Skosmos, which was formerly known as ONKI Light. Finto is used by indexers at the National Library and other libraries, as well as other organizations including the Finnish broadcasting company YLE and many museums. It is also used to support vocabulary development processes. Skosmos provides a multilingual user interface for browsing and searching the data and for visualizing concept hierarchies. The user interface has been developed by analysing the results of repeated usability tests. A developer-friendly REST API is also available providing RDF/XML, Turtle or JSON-LD serializations and Linked Data access for utilizing vocabularies in other applications such as annotation systems. Skosmos relies on a SPARQL endpoint as its back-end and is written mainly in PHP. The main benefits of using a SPARQL endpoint is that the data provided by the service is always up to date. This allows fast update cycles in vocabulary development. Skosmos can be configured to suit different types of RDF data. The source code is available under the MIT license.",SWIB,,,,,,,,,,,
,2014,RDF Application Profiles in Cultural Heritage,"Valentine Charles,Evelyn Dröge,Stephanie Rühle,Karen Coyle,Kai Eckert",https: //swib.org/swib14/slides/ploeger_swib14_31.zip,SWIB/SWIB2014/RDF Application Profiles in Cultural Heritage.md,"#swib/2014,#profile,#DCMI,#DM2E,#manuscripts,#Europeana,#EDM,#OAI-ORE,#dublin-core,#mappings,#cultural-heritage,#data-model","The [DCMI RDF Application Profiles Task Group](http://wiki.dublincore.org/index.php/RDF_Application_Profiles/) (RDF-AP) deals with the development of recommendations regarding the proper creation of data models, in particular the proper reuse of existing data vocabularies. The Digitised Manuscripts to Europeana project ( [DM2E](http://www.dm2e.eu/)) was a driving factor in establishing the task group and provides one of the main case studies. Like many other projects in the domain of cultural heritage, DM2E created an own data model based on the Europeana Data Model (EDM), which itself reuses parts of other vocabularies like OAI-ORE and Dublin Core and allows the creation of rich mappings. This mix-and-match of vocabularies and the documentation of project-specific characteristics or even constraints in using the data model is called an application profile. In this workshop, we will first introduce the DM2E model as a typical application profile, as well as the infrastructure developed in the project that determines how the application profile is actually used. The DM2E model not only describes digitised manuscripts, it also is used to model workflows for the transformation of the metadata, as well as representing the provenance of the metadata. Data represented in the DM2E model can be accessed via a SPARQL endpoint and search interfaces. As the DM2E model is backwards compatible with the EDM, the data is also provided in plain EDM for the ingestion to Europeana. The current status of the DCMI RDF Application Profiles Task Group will be presented, which will particularly include a brief overview on the other case studies and the identified requirements for a generally applicable solution. We then want to discuss these requirements and work towards practicable solutions together with the participants, who can reflect all approaches based on their own requirements in various institutions and projects. Concrete topics will be: - Definition of application profiles as “local” versions of existing data models. - How to create an application profile that remains interoperable. - Formulate constraints regarding your data within an application profile. - Linking your data to an application profile. - Validate your data with respect to an application profile.",SWIB,,,,,,,,,,
,2014,Supporting Data Interlinking in Semantic Libraries with Microtask Crowdsourcing,Cristina Sarasua,SWIB/SWIB2014/Supporting Data Interlinking in Semantic Libraries with Microtask Crowdsourcing.md,"#swib/2014,#microtasks,#data/processing,#interlinking,#crowdsourcing","Semantic Web technologies enable the integration of distributed data sets curated by different organisations and with different purposes. Descriptions of particular resources (e.g. events, persons or images) are connected through links that explicitly state the relationship between them. Connecting data of similar or disparate domains, libraries can offer a more extensive and detailed information to their visitors, while librarians have better documentation in their cataloguing activities. Despite the advances in data interlinking technology, human intervention is still a core aspect of the process. Humans, in particular librarians, are crucial both as knowledge providers and reviewers of the automatically computed links. One of the problems that arises in this scenario is that libraries might have limited human resources dedicated to authority control; so, running the time-consuming interlinking process over external data sets becomes troublesome. Microtask crowdsourcing provides an economic and scalabile way to involve humans systematically in data processing. The goal of this talk is to introduce the process of crowdsourced data interlinking in semantic libraries, which is a paid crowd-powered approach that can support librarians in the interlinking task. Several use cases are described to illustrate how our software, which implements the crowdsourced data interlinking process, could be useful to reduce the amount of information that librarians would need to process when enriching their data with other sources, or to obtain a different perspective from potential users. In addition, challenges that become relevant when adopting this approach are listed.",SWIB,,,,,,,,,,,
,2014,Turning three overlapping thesauri into a Global Agricultural Concept Scheme,"Thomas Baker,Osma Suominen",SWIB/SWIB2014/Turning three overlapping thesauri into a Global Agricultural Concept Scheme.md,"#swib/2014,#AGROVOC,#agriculture,#SKOS,#VocBench","AGROVOC Concept Scheme, CAB Thesaurus (CABT), and NAL Thesaurus (NALT) largely overlap in scope (agriculture and agricultural research). This duplication is both inefficient for their maintainers and constitutes a barrier to searching across databases indexed with their terms. Common representation in SKOS makes mapping easier, but it would in principle be more efficient to merge the thesauri into one shared concept scheme to be jointly maintained by the three organizations. A feasibility study has defined a semi-automatic method for mapping among the three thesauri. Confirmed mappings will be used to coin new concepts, with new URIs, for a shared Global Agricultural Concept Scheme (GACS). One key challenge will be to balance the inclusion of diverse concept hierarchies from the source thesauri against a desire to converge on common semantics through editorial intervention. Partners who currently use their thesauri to automatically generate derivative products will need to balance the efficiencies of sharing a concept scheme with the control required for local production processes. GACS will be natively represented as SKOS XL, edited using VocBench software and published using the Skosmos platform (both open source software) under a Creative Commons license. The GACS project aspires to constitute a consortium open to other thesaurus maintainers. The first version of GACS will be available online in time for a presentation of lessons learned at SWIB 2014.",SWIB,,,,,,,,,,,
,2014,Using Graphity Linked Data Platform for Danish newspaper registry. From printed books to Linked Data,"Martynas Jusevicius,Dziugas Tornau",SWIB/SWIB2014/Using Graphity Linked Data Platform for Danish newspaper registry. From printed books to Linked Data.md,"#swib/2014,#denmark,#newspapers,#TEI,#Dydra,#XHTML,#XSLT","Danish Newspapers is a registry of newspapers with historical and factual metadata records. Three printed volumes with metadata about danish newspaper publishing were scanned and text optically recognized, marking it up using TEI XML schema. The XML was converted into XML-based RDF quad format using multiple domain-specific vocabularies (SKOS, BIBO, Time ontology etc.) in a custom XSLT stylesheet. Rich non-structured text was preserved as XHTML literals, with links to images stored as JPG files. The data was stored in Dydra cloud triplestore and presented as a Web application that publishes Linked Data as well as user-friendly and mobile-ready XHTML. It features interactive maps, faceted and text search, autocomplete and complex content creation and editing for authenticated users. URI templates, SPARQL queries, data quality constraints and access control were defined declaratively as RDF data and processed in run-time by Graphity platform, while XSLT stylesheets were used to generate a customized XHTML layout and facets. Graphity processor is open-source and works with any SPARQL 1.1 triplestore, while the commercial platform layer provides multi-tenant features such as access control and faceted search. Linking with external datasources and alignment with standard models were not in scope for this project, as the focus was on data conversion, content presentation and data filtering. The data can be mapped to generic bibliographic vocabularies such as BIBFRAME and EDM using SPARQL Update.",SWIB,,,,,,,,,,,
,2014,Using linked data to annotate semantically the BBC's content,Tom Grahame,SWIB/SWIB2014/Using linked data to annotate semantically the BBC's content.md,"#swib/2014,#BBC,#aggregations","'Linked Data at the BBC emerged as a set of ideas, techniques and technologies to build websites and has gone on to show how those techniques can improve and simplify production workflows, and provide interesting automated aggregations for our audiences. The success of applying the technology to deliver the online coverage of major sporting events has demonstrated the potential for reusing the semantic infrastructure as a central part of the BBC production workflow. To support this decision, the vision of semantic publishing at the BBC evolved towards connecting content around the things that matter to our audiences - those things can be politicians, athletes or musicians, places or organisations, topics of study or events. The BBC produces a plethora of content every day about these things and the content varies from news articles, to programmes, to educational guides, clips and recipes. Because it is commissioned and used in different audience facing products, this content is mastered in separate and disconnected systems, yet – the things that the content is about are the same. By semantically describing and annotating the content with the things it is about, we enable journalists and content editors to access heterogeneous and previously isolated creative works in a unified manner. In this talk I will describe how the BBC Sport's use of Linked Data has evolved from developing a single website covering the 2010 World Cup to supporting the annotation and dynamic aggregation of daily Sports coverage and every major event including London 2012, Sochi 2014 and the 2014 FIFA World Cup. I will also discuss how the same platform and technology approach is being deployed across the BBC in domains as diverse as Education, News, Radio and Music and how a Linked Data approach could be applied to similar challenges in the Library environment.''",SWIB,,,,,,,,,,,
,2014,Weaving repository contents into the Semantic Web,Pascal-Nicolas Becker,SWIB/SWIB2014/Weaving repository contents into the Semantic Web.md,"#swib/2014,#repositories,#digital/objects,#digital-objects,#metadata,#DSpace","Repositories are systems to safely store and publish digital objects and their descriptive metadata. Repositories mainly serve their data by using web interfaces which are primarily oriented towards human consumption. They either hide their data behind non-generic interfaces or do not publish them at all in a way a computer can process easily. At the same time the data stored in repositories are particularly suited to be used in the Semantic Web as metadata are already available. They do not have to be generated or entered manually for publication as Linked Data. In my talk I will present a concept of how metadata and digital objects stored in repositories can be woven into the Linked (Open) Data Cloud and which characteristics of repositories have to be considered while doing so. One problem it targets is the use of existing metadata to present Linked Data. The concept can be applied to almost every repository software. At the end of my talk I will present an implementation for DSpace, one of the software solutions for repositories most widely used. With this implementation every institution using DSpace should become able to export their repository content as Linked Data.",SWIB,,,,,,,,,,,
,2014,When Semantics support Multilingual Access to Digital Cultural Heritage - the Europeana case,"Valentine Charles,Juliane Stiller",SWIB/SWIB2014/When Semantics support Multilingual Access to Digital Cultural Heritage - the Europeana case.md,"#swib/2014,#Europeana,#EDM,#enrichment","For Europeana, the platform for Europe’s digital cultural heritage from libraries, museums and archives, multilingual access is one priority. Breaking down language barriers is an ongoing challenge with many facets as Europeana provides content coming from 36 different countries serving users across the world. For Europeana, multilingual access does not only mean the translation of the interface, but also comprises retrieving, browsing and understanding documents in languages the users do not speak. This talk will present the solutions implemented at Europeana enabling multilingual retrieval and browsing. Europeana leverages the semantic data layer by linking multilingual and open controlled vocabularies to objects. The Europeana Data Model (EDM) allows for semantic and multilingual metadata descriptions and gives support for contextual resources including concepts from “value vocabularies” either coming from Europeana’s network of providers or third-party data sources. To enable retrieval across languages and enhance data semantically, Europeana performs automatic metadata enrichment with external value vocabularies and datasets such as GEMET, GeoNames and DBpedia. Providers are also encouraged to send links from open vocabularies such as AAT, GND, Iconclass and VIAF or from their domain vocabularies following the EDM recommendations for contextual resources, especially when these vocabularies contain labels in different languages. By re-using these vocabularies, Europeana does not only pursue efforts in demonstrating the potential of Linked Open vocabularies by exploiting the semantic relations and translations but also aims at making Europeana truly multilingual.",SWIB,,,,,,,,,,,
,2014,Wikidata: A Free Collaborative Knowledge Base,Markus Krötzsch,SWIB/SWIB2014/Wikidata - A Free Collaborative Knowledge Base.md,"#swib/2014,#Wikidata,#ontology","Wikidata, the free knowledge base of Wikipedia, is one of the largest collections of human-authored structured information that are freely available on the Web. It is curated by a unique community of tens of thousands of editors who contribute in up to 400 different languages. Data is stored in a language-independent way, so that most users can access information in their native language. To support plurality, Wikidata uses a rich content model that gives up on the idea that the world can be described as a set of ""true"" facts. Instead, statements in Wikidata provide additional context information, such as temporal validity and provenance (in particular, most statements in Wikidata already provide one or more references). One could easily image this to lead to a rather chaotic pile of disjointed facts that are hard to use or even navigate. However, large parts of the data are interlinked with international authority files, catalogues, databases, and, of course, Wikipedia. Moreover, the community strives to reach “global” agreement on how to organise knowledge: over 1,000 properties and 40,000 classes are currently used as an ontology of the system, and many aspects of this knowledge model are discussed extensively in the community. Together, this leads to a multilingual knowledge base of increasing quality that has many practical uses. This talk gives an overview of the project, explains design choices, and discusses emerging developments and opportunities related to Wikidata.",SWIB,,,,,,,,,,,
,2014,d:swarm - A Library Data Management Platform Based on a Linked Open Data Approach,"Jens Mittelbach,Robert Glass,Ralf Talkenberger,Felix Lohmeier",SWIB/SWIB2014/d_swarm - A Library Data Management Platform Based on a Linked Open Data Approach.md,"#swib/2014,#resource/discovery,#bibliographic/data,#authority/data,#dswarm,#d-swarm,#dresden,#germany","The rise of the concept of resource discovery, the increasing multiplicity of information channels and the exploding complexity of the technological infrastructure have placed organizational and financial challenges on libraries. Library data has become more heterogeneous, its sources have grown manifold. Bibliographic and authority data, licence and business data, usage data from library catalogues and the global science community (bibliometric data) as well as open data from the WWW constitute the graph that describes the resources managed by libraries. Consequently, there is an increasing need to integrate, normalize, and enrich existing library data sets as well as assure data quality for production and presentation purposes. The Saxon State and University Library Dresden has chosen a new approach of data integration for libraries and other cultural heritage institutions. In the EFRE-funded project, a scalable cloud-based data management platform called d:swarm has been implemented. Featuring an easy-to-use web-based modelling GUI, d:swarm allows for the integration and interlinkage of heterogeneous data sources into an integrated and flexible property graph data storage. As a middleware layer, it runs on top of existing library software infrastructures. Thus, existing library workflows depending on a variety of software solutions can remain untouched while data integration can be flexibly tailored to the needs of the individual institutions. Using d:swarm, feeding a library’s discovery front-end with high-quality normalized data or disseminating Linked Open Data is much easier. The project is published under an open source licence.",SWIB,,,,,,,,,,,
,2014,schema.org: machine-readable cataloguing for the open web,Dan Scott,SWIB/SWIB2014/schema.org - machine-readable cataloguing for the open web.md,"#swib/2014,#schema-org,#vocabulary,#best-practices","The schema.org vocabulary was created by the major search engines (Bing, Google Yahoo!, and Yandex) in 2011 to provide a common means of expressing metadata about popular search topics such as events, people, and products. While schema.org was enthusiastically adopted by web sites hoping for enhanced search results and rankings, libraries have more cautiously integrated the vocabulary. This session examines efforts to use schema.org to provide access points for library resources in major search engines via bibliographic metadata, holdings data, and data about libraries themselves. We highlight advances made by the integrated systems Evergreen and Koha, discovery systems such as Blacklight and VuFind, and repositories such as Islandora and ScholarSphere in publishing schema.org metadata (from unstructured data, to structured data, to linked data). The role of the W3C Schema.org Bibliographic Extension community group in filling gaps in schema.org and documenting best practices for libraries is also discussed. Finally, we show how common workflows (such as creating union catalogues and checking item availability) that currently rely on niche, library-specific protocols can be simplified and built with standard web tools by embracing a truly machine-readable vocabulary.",SWIB,,,,,,,,,,,
,2015,A RESTful JSON-LD Architecture for Unraveling Hidden References to Research Data,"Konstantin Baierer,Philipp Zumstein",SWIB/SWIB2015/A RESTful JSON-LD Architecture for Unraveling Hidden References to Research Data.md,"#swib/2015,#RESTful,#REST,#API,#data/citation,#InFoLiS,#APIs,#data/model,#JSON,#mapping,#MongoDB","'Data citations are more common today, but more often than not the references to research data don't follow any formalism as do references to publications. The InFoLiS project makes those ""hidden"" references explicit using text mining techniques. They are made available for integration by software agents (e.g. for retrieval systems). In the second phase of the project we aim to build a flexible and long-term sustainable infrastructure to house the algorithms as well as APIs for embedding them into existing systems. The infrastructure's primary directive is to provide lightweight read/write access to the resources that define the InFoLiS data model (algorithms, metadata, patterns, publications, etc.). The InFoLiS data model is implemented as a JSON schema and provides full forward compatibility with RDF through JSON-LD using a JSON-to-RDF schema-ontology mapping, reusing established vocabularies whenever possible. We are neither using a triplestore nor an RDBMS, but a document database (MongoDB). This allows us to adhere to the Linked Data principles, while minimizing the complexity of mappings between different resource representations. Consequently, our web services are lightweight, making it easy to integrate InFoLiS data into information retrieval systems, publication management systems or reference management software. On the other hand, Linked Data agents expecting RDF can consume the API responses as triples; they can query the SPARQL endpoint or download a full RDF dump of the database. We will demonstrate a lightweight tool that uses the InFoLiS web services to augment the web browsing experience for data scientists and librarians.'",SWIB,,,,,,,,,,,
,2015,"ALIADA, an Open Source Solution to Easily Publish Linked Data of Libraries and Museums",Cristina Gareta,"SWIB/SWIB2015/ALIADA, an Open Source Solution to Easily Publish Linked Data of Libraries and Museums.md","#swib/2015,#ALIADA,#MARCXML,#LIDOXML,#data/silo,#FRBRoo,#SKOS,#FOAF,#WGS84,#ontologies,#DataHub","'ALIADA is an open source solution designed by art libraries and museums, ILS vendors and experts on Semantic Web to help cultural heritage institutions to automatically convert, link and publish their library and museum data as Linked Open Data. If they can export their metadata as MARCXML or LIDOXML, they can choose ALIADA as their ally in the challenge of liberating cultural institutions from their current data silos and integrating library and museum data onto the Semantic Web. ALIADA uses its own ontology based on FRBRoo, SKOS, FoaF and WGS84, the ontologies most used by the linked open datasets analyzed during the design of the tool. It's expected this ontology to be updated with the new emerging models and vocabularies, such as RDA or BIBFRAME, according to the ALIADA's community demand. ALIADA can be integrated with the current management system in a library or a museum allowing non-expert staff to easily select and import metadata into ALIADA. Once the file is validated, the user can start the ""RDFizer"" to create the triples using the existing mapping templates. All the MARC mappings were not carried out into RDF using FRBRoo ontology because of the complexity of the format. Along with the RDF conversion, ALIADA provides a set of predefined SPARQL queries to check the URIs. The next step in the workflow is the linking to other datasets. ALIADA offers a list of external datasets that can be linked to, including Europeana, DBpedia or VIAF. Finally, ALIADA will show the dataset before publishing it on the DataHub.'",SWIB,,,,,,,,,,,
,2015,"Bringing Your Content to the User, not the User to Your Content – a Lightweight Approach towards Integrating External Content via the EEXCESS Framework","Werner Bailer,Martin Höffernig","SWIB/SWIB2015/Bringing Your Content to the User, not the User to Your Content – a Lightweight Approach towards Integrating External Content via the EEXCESS Framework.md","#swib/2015,#EEXCESS,#project,#mappings,#contolled/vocabularies","This workshop will look at the steps and tools needed to adapt scientific and cultural heritage assets to services which make them available to wide community of users on the platforms they already use, e.g., as a plugin in their web browser or on their blogging environment. The EEXCESS project has developed such a service, and welcomes GLAM institutions to make their data available. Data providers face two basic problems: Provision of their data in various formats in order to match available services; Quality issues while mapping their specific data to other formats. On this background, in the EU funded project EEXCESS a tool chain has been developed to make data provision as simple as possible for the providers.",SWIB,,,,,,,,,,,
,2015,Call for Linked Research,Sarven Capadisli,SWIB/SWIB2015/Call for Linked Research.md,"#swib/2015,#scholarly-communication,#Dokieli","Linked Research is set out to socially and technically enable researchers to take full control, ownership, and responsibility of their own knowledge. This is so that research contributions are accessible to the society at maximum capacity, by dismantling the use of archaic and artificial barriers. It is intended to influence a (paradigm) shift in all aspects of scholarly communication by fostering the use of the native Web stack. Linked Research proposes an acid test to the research community in order to verify, approve, or test the openness, accessibility and flexibility of the approaches for enhanced scholarly communication. Dokieli is a decentralized authoring, annotations, and social interaction tool complying with this initiative. This talk will discuss and demonstrate what works!",SWIB,,,,,,,,,,,
,2015,Catmandu - a (Meta)Data Toolkit,"Johann Rolschewski,Vitali Peil,Patrick Hochstenbach",SWIB/SWIB2015/Catmandu - a (Meta)Data Toolkit.md,"#swib/2015,#Catmandu","Catmandu http://librecat.org/Catmandu/ provides a suite of software modules to ease the import, storage, retrieval, export and transformation of (meta)data records. After a short introduction to Catmandu and its features, we will present the command line interface (CLI) and the domain specific language (DSL). Participants will be guided to get data from different sources via APIs, to transform data records to a common data model, to store/index it in Elasticsearch or MongoDB, to query data from stores and to export it to different formats. The intended audience is Systems librarians, Metadata librarians, and Data managers. Participants should be familiar with command line interfaces (CLI). Programming experience is not required. Required is a Laptop with VirtualBox installed. Organisers will provide a VirtualBox image (Linux guest system) beforehand. Participants can also install their own environment, see here. Participants could bring their own data (CSV, JSON, MAB2, MARC, PICA+, XLS, YAML).",SWIB,,,,,,,,,,,
,2015,Data-Transformation on Historical Data Using the RDF Data Cube Vocabulary,"Sebastian Bayerl,Michael Granitzer",SWIB/SWIB2015/Data-Transformation on Historical Data Using the RDF Data Cube Vocabulary.md,"#swib/2015,#vocabulary,#rdf-data-cube,#RDF,#TEI,#data-cube,#transformations","This work describes how XML-based TEI documents, containing statistical data, can be normalized, converted and enriched using the RDF Data Cube Vocabulary. In particular we focus on a statistical real world data set, namely the statistics of the German Reich around the year 1880, which are available in the TEI format. The data is embedded in complex structured tables, which are relatively easy to understand for humans but they are not suitable for automated processing and data analysis, without heavy pre-processing, due to their varying structural properties and differing table layouts. Therefore, the complex structured tables must be validated, modified and transformed, until they are suitable for the standardized multi-dimensional data structure - the data cube. This work especially focuses on the transformations necessary to normalize the structure of the tables. Performing validation- and cleaning-steps, resolving row- and column-spans and reordering slices are available transformations among multiple others. By combining existing transformations, compound operators are implemented, which can handle specific and complex problems. The identification of structural similarities or properties can be used to automatically suggest sequences of transformations. A second focus is on the advantages, which come by using the RDF Data Cube Vocabulary. Also, a research prototype was implemented to execute the workflow and convert the statistical data into data cubes.",SWIB,,,,,,,,,,,
,2015,Evaluation of Metadata Enrichment Practices in Digital Libraries: Steps towards Better Data Enrichments,"Valentine Charles,Juliane Stiller",SWIB/SWIB2015/Evaluation of Metadata Enrichment Practices in Digital Libraries Steps towards Better Data Enrichments.md,"#swib/2015,#Europeana,#metadata,#metadata/enrichment,#enrichment","In large cultural heritage data aggregation systems such as Europeana, automatic and manual metadata enrichments are used to overcome the issues raised by multilingual and heterogeneous data. Enrichments are based on linked open datasets, which can be very beneficial for enabling retrieval across languages, adding context to cultural heritage objects and for improving the overall quality of the metadata. However, if not done correctly, the enrichments may transform into errors, which propagate to several languages and impacting the retrieval performance and user experience. To identify the different processes that impact the quality of enrichments, Europeana and affiliated projects’ representatives have organised a series of experiments applying several enrichment techniques on a particular dataset constituted of random metadata samples from several data providers from several domains, but mainly from library held cultural heritage digital objects. Comparing and analysing the results shows that selecting appropriate target vocabularies, fine-tuning enrichment rules are as important as defining evaluation methods. The development of flexible workflows will contribute to better interoperability between enrichment services and data, but might make individual enrichment processes more ambivalent. Efforts where users evaluate and correct enrichments as well as the enrichments’ impact on retrieval and user experience also need to be considered. The presentation will show how a better understanding of enrichment methodologies will help cultural heritage institutions and specifically libraries to get the semantics right.",SWIB,,,,,,,,,,,
,2015,HTTP-PATCH for Read-write Linked Data,Rurik Thomas Greenall,SWIB/SWIB2015/HTTP-PATCH for Read-write Linked Data.md,"#swib/2015,#HTTP-PATCH,#Oslo,#REST,#API","It can be argued that HTTP-PATCH is essential to read-write linked data; this being the case, there seems to be no absolute definition for how this should be implemented. In this talk, I present different alternatives for HTTP-PATCH and an implementation based on practical considerations from feature-driven development of a linked-data-based library platform at Oslo public library. Grounded in the work done at Oslo public library, I show how HTTP-PATCH can be implemented and used in everyday workflows, while considering several aspects of specifications such as LD-PATCH, RDF-PATCH, particularly in light of existing efforts such as JSON-PATCH. In the description of the implementation, I pay particular attention to the practical issues of using linked data in REST architecture, the widespread use of formats that do not support hypermedia and blank nodes. The talk views the cognitive constraints imposed by the dominance of the traditional library technology stack and how these colour development of new workflows and interfaces. Further, I provide some thoughts about how specifications like the linked-data platform can be reconciled with modern development techniques that largely shun such specifications, and how we can create read-write interfaces for linked data.",SWIB,,,,,,,,,,,
,2015,Introduction to Linked Open Data,"Felix Ostrowski,Adrian Pohl",SWIB/SWIB2015/Introduction to Linked Open Data.md,"#swib/2015,#workshop,#RDF,#linked-open-data,#linked/open/data,#RDF/data/model,#dereferenceable/URIs,#Open/Data","This introductory workshop aims to introduce the fundamentals of linked data technologies on the one hand, and the basic legal issues of open data on the other. The RDF data model will be discussed, along with the concepts of dereferenceable URIs and common vocabularies. The participants will continuously create and refine RDF documents about themselves including links to other participants to strengthen their knowledge of the topic. Based on the data created the advantages of publishing linked data will be shown. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop.",SWIB,,,,,,,,,,,
,2015,LOD for Applications – Using the Lobid API,"Pascal Christoph,Fabian Steeg",SWIB/SWIB2015/LOD for Applications – Using the Lobid API.md,"#swib/2015,#hbz,#lobid,#API,#GND","It is often correctly noted that many datasets got published in the library world with little or no stories about actual use of these datasets. In this talk we want to highlight some of this usage in the context of the hbz linked open data service lobid (which stands for ""linking open bibliographic data""). The hbz has been experimenting with linked data technology since 2009. In November 2013 the hbz launched a linked open data API via its service lobid. This API provides access to different kinds of data: • bibliographic data from the hbz union catalogue with 20 million records and 45 million holdings, • authority data from the German Integrated Authority File (Gemeinsame Normdatei, GND) with subject headings, persons, corporate bodies, events, places and works, • address data on libraries and related institutions, taken from the German ISIL registry and the MARC organization codes data base. The talk starts with a brief outline of the concept and the technology used behind the lobid API and how lobid itself benefits from other linked data sources. After that it is shown how applications are making use of the lobid API, focusing on the development of the North Rhine-Westphalian bibliography (NWBib) as an example. In the end we provide some lessons learned regarding the transformation of legacy data to linked data and the provision of a web API for it.",SWIB,,,,,,,,,,,
,2015,"Linked Data for Libraries: Experiments between Cornell, Harvard and Stanford",Simeon Warner,"SWIB/SWIB2015/Linked Data for Libraries - Experiments between Cornell, Harvard and Stanford.md","#swib/2015,#LD4L,#linked/data,#ontology,#BIBFRAME","The Linked Data for Libraries (LD4L) project aims to create a Linked Open Data (LOD) model that works both within individual institutions and across libraries to capture and leverage the intellectual value that librarians and other domain experts add to information resources when they describe, annotate, organize, and use those resources. First we developed a set of use cases illustrating the benefits of LOD in a library context. These served as a reference for the development of an LD4L ontology which includes bibliographic, person, curation, and usage information. This largely draws from existing ontologies, including the evolving BIBFRAME ontology. We have prioritized the ability to identify entities within library metadata records, reducing reliance on lexical forms of identity. Whenever possible we seek out persistent global identifiers for the entities being represented — identifiers from established efforts such as ORCID, VIAF, and ISNI for people, and OCLC identifiers for works for example. One group of LD4L use cases explores circulation and other usage data as sources that could improve discovery, and inform collection building. We are exploring the use of a anonymized and normalized metric that may be shared and compared across institutions. Ontology work and software from the LD4L project is available from our Github repository.",SWIB,,,,,,,,,,,
,2015,"Linking Data about the Past Through Geography: Pelagios, Recogito & Peripleo","Rainer Simon,Elton Barker,Leif Isaksen,Pau de Soto Cañamares","SWIB/SWIB2015/Linking Data about the Past Through Geography Pelagios, Recogito & Peripleo.md","#swib/2015,#Pelagios,#gazetteers,#Recogito,#Peripleo,#API","Pelagios is a community-driven initiative that facilitates better linkages between online resources documenting the past, based on the places they refer to. Our member projects are connected by a shared vision of a world in which the geography of the past is every bit as interconnected, interactive and interesting as the present. Pelagios has been working towards establishing conventions, best practices and tools in several areas of ""Linked Ancient World Data"": i. Linking and aligning of gazetteers. Gazetteers are the primary knowledge organization mechanism in Pelagios. In order to foster integration of gazetteers from different communities, we have been developing an RDF profile for publishing gazetteer metadata as Linked Open Data. ii. Tools to aid linking. To simplify the process of linking documents to the places they refer to, we have developed an Open Source geoannotation platform called Recogito. iii. Tools to visualize and navigate. To make the growing pool of data in Pelagios more accessible to everyday, we are working on a search engine called Peripleo. Peripleo will allow the navigation of the interconnected gazetteers that form the backbone of Pelagios, as well as the objects and documents that link to them. iv. Infrastructure for re-use. Data created in Recogito is available under CC terms for bulk download. Peripleo will feature similar capabilities and, in addition, offers a comprehensive JSON API to enable re-use in 3rd party applications and mashups.",SWIB,,,,,,,,,,,
,2015,Maximising (Re)Usability of Library Metadata Using Linked Data,Asunción Gómez Pérez,SWIB/SWIB2015/Maximising (Re)Usability of Library Metadata Using Linked Data.md,"#swib/2015,#datos-bne-es,#metadata,#owl,#re-use","'Linked Data (LD) and related technologies are providing the means to connect high volumes of disconnected data at Web-scale and producing a huge global knowledge graph. The key benefits of applying LD principles to datasets are i. better modelling of datasets as directed labelled graphs, ii. structural interoperability of heterogeneous resources, iii. federation of resources from different sources and at different layers including language annotation, iv. a strong ecosystem of tools based on RDF and SPARQL, v. improved conceptual interoperability due to strong semantic models such as [[OWL]] and shared semantics due to linking and vi. dynamic evolution of resources on the web. In this talk, I will explore challenges related with the (Re)Usability of library linked metadata in the field of cultural heritage and for other purposes. I will argue that for maximizing (re)use of library linked metadata it is crucial to represent core aspects related with Linguistic, Provenance, License, and Dataset metadata. A proper representation of these features using W3C standards and the use of W3C best practices and guidelines for multilingual Linked Open Data: i. produce better library linked metadata that could be used later on for diagnosing and repairing other external resources; ii. facilitate rights management, and consequently the access and reuse of metadata and data delivered under different license schema; iii. enable navigation across datasets in different languages thanks to the exploitation of links across multilingual data; iv. help data providers and data consumers to go a step further when cataloguing, searching and building cross-lingual applications that use open library linked metadata; v. increase exploitation when library linked metadata will be used with licensed (open or closed) linked data in other domains. I will also present approaches that use datos.bne.es library linked metadata with geographical information to produce new insights and innovation.'",SWIB,,,,,,,,,,,
,2015,"Metadata Records & RDF: Validation, Record Scope, State, and the Statement-centric Model",Thomas Johnson,"SWIB/SWIB2015/Metadata Records & RDF Validation, Record Scope, State, and the Statement-centric Model.md","#swib/2015,#RDF,#DPLA,#validation,#model","Advocates for the use of RDF as a model for metadata in the cultural heritage sector have frequently spoken of the death of the “record”. Indeed, the shift from a document-centric approach to one based on identified resources and atomic statements is an important one. Yet current work on validation as well as requirements for day-to-day metadata management and attribution point back to aspects of a record-driven worldview. This session will address some historical views of records, contrasting them with the formal model adopted by RDF 1.1 and commonly accepted best practices for Linked Data. Practical implications of the RDF model will be explored, with questions raised regarding the management of state, mutability, and “record” workflows. A provisional approach for managing RDF resources and graphs in record-like contexts is proposed, with connections to RDF Shapes, DC Application Profiles, and Linked Data Platform. Use cases from the Digital Public Library of America will be presented as illustrative examples.",SWIB,,,,,,,,,,,
,2015,Mistakes Have Been Made,Karen Coyle,SWIB/SWIB2015/Mistakes Have Been Made.md,"#swib/2015,#FRBR,#BIBRAME,#RDA,#catalogs","The cultural heritage data communities are racing forward into the future with FRBR, BIBFRAME, RDA, and other bibliographic models. Unfortunately, these models are weighted down with the long history of bibliographic description, like stones in our pockets. As someone who worked on the cusp between card catalogs and machine-readable data, Coyle looks back on the moments in our recent history when we should have emptied our pockets and moved forward. As one who was there, there are 'mea culpas'. Coyle will also surprise you with the truth about FRBR and some radical thinking about what to do with that past that is holding us back from achieving the future we should be pursuing.",SWIB,,,,,,,,,,,
,2015,Modeling and Exchanging Annotations for Europeana Projects,"Hugo Manguinhas,Antoine Isaac,Valentine Charles,Sergiu Gordea,Maarten Brinkerink",SWIB/SWIB2015/Modeling and Exchanging Annotations for Europeana Projects.md,"#swib/2015,#modeling,#europeana,#annotations,#crowdsourcing,#controlled/tagging,#enrichment,#Open-Annotation","Cultural heritage institutions are looking at crowdsourcing as a new way and opportunity to improve the overall quality of their data and contribute to a better semantic description and link to the web of data. This is also the case for Europeana, as crowdsourcing under the form of annotations is envisioned and being worked on in several projects. As part of the Europeana Sounds project, we have identified the user stories and requirements that cover the following annotation scenarios: open and controlled tagging; enrichment of metadata; annotation of media resources; linking to other objects; moderation and general discussion. The first success on bringing annotations to Europeana is the integration of annotations to Europeana objects made on the HistoryPin.org platform covering both the tagging and object linking scenarios. The next step, will be to help data providers to support annotation at their side, for which we are working with the Pundit annotation tool. As a central point on all the efforts around annotations is an agreement on how these should be modelled in a uniform way for all these scenarios, as it is essential to bring such information to Europeana and in a way that can also be easily exploited and shared beyond our portal. For this, we are using the recent Web Annotation Data Model supported by the Open Annotation community as it is the most promising model at the moment. Due to its flexible design, we have made recommendations on how it should be applied for these scenarios and we are looking for discussion/feedback from the community in the hope that it will help cultural heritage institutions to better understand how annotations can be modelled.",SWIB,,,,,,,,,,,
,2015,RDF.rb & ActiveTriples: Working with RDF in Ruby,Thomas Johnson,SWIB/SWIB2015/RDF.rb & ActiveTriples Working with RDF in Ruby.md,"#swib/2015,#RDF,#ActiveTriples,#SPARQL,#workshop,#Ruby,#ActiveModel","This workshop covers the current state of RDF support in the dynamic Object Oriented Ruby language. We will cover the following tools: Ruby RDF (RDF.rb) is a fully public domain suite of libraries implementing the full RDF model. The core library provides interfaces for working with resources, statements, graphs, and datasets. An extensive network of satellite libraries offer support for many serialization formats, basic reasoning, graph normalization, SPARQL, and persistence to a wide variety of triplestores. ActiveTriples is an Object-Graph-Modeling interface built over Ruby RDF. It supports the ActiveModel interface for integration with Ruby on Rails and similar frameworks. The workshop is recommended for anyone looking for an expressive, accessible toolkit to work with RDF data. Some experience with programming and a basic knowledge of Object Oriented concepts is assumed; experience with Ruby is not expected. Participants should come prepared with Ruby installed on their laptops, and may benefit from working through Ruby in 20 Minutes in advance of the session.",SWIB,,,,,,,,,,,
,2015,Researchers’ Identity Management in the 21st Century Networked World: A Case Study of AUC Faculty Publications,Anchalee Panigabutra-Roberts,SWIB/SWIB2015/Researchers’ Identity Management in the 21st Century Networked World A Case Study of AUC Faculty Publications.md,"#swib/2015,#ORCID,#ResearcherID","This project will explore how American University in Cairo (AUC) faculty members distributed their scholarly and creative works, and how their names are identified in author identifier systems and/or on the Web. The goal is to explore how best to present their data as linked data. The project will use the AUC faculty’s names listed in AUC Faculty Publications: 2012 Calendar Year. Their names will be used to search in author identifier systems to answer;",SWIB,,,,,,,,,,,
,2015,Schema.org Question Time Workshop,Richard Wallis,SWIB/SWIB2015/Schema.org Question Time Workshop.md,"#swib/2015,#schema.org,#compare,#bibframe,#workshop","Schema.org is basically a simple vocabulary for describing stuff, on the web. Since its launch by the major search engines (Google, Bing, Yahoo!, Yandex) in 2011 it has had a meteoric rise to become a de facto vocabulary on the web. How it works; how you use it; can it only be embedded in html; what happens if the search engines drop it; how do I mark up my pages; can I use it for my data like I would any other vocabulary; how is it managed; how applicable is it for bibliographic data; how does it compare with Bibframe; can it be extended; how can I influence its development; how well used is it; what are the benefits of using it — all questions that are often asked about Schema.org. Join this session to hear answers to these questions, ask your own questions, and more. Amongst other things, you will walk through some simple examples of using Schema; how you can participate in the communities surrounding the development and extension of the vocabulary; and discuss how and why it is applicable to libraries and their data. The format of this workshop will mostly be driven by the participants raising questions and topics of concern for discussion in the group, introduced and facilitated by Richard Wallis, Chair of the Schema Bib Extend W3C Community Group. Come along and find out everything* about Schema.org but have never had the chance to ask. (* Richard will do his best attempting to cover everything that can be answered in the session.)",SWIB,,,,,,,,,,,
,2015,The Digital Cavemen of Linked Lascaux,Ruben Verborgh,SWIB/SWIB2015/The Digital Cavemen of Linked Lascaux.md,"#swib/2015,#Lascaux,#SPARQL,#REST,#REST/principles","Some 17,000 years ago, cavemen, cavewomen and cavekids picked up their cavebrushes to paint caveanimals on their cavewalls in a place that eventually would become known as the Lascaux complex. Their cavehands eternalized cavehorses and cavedeer in shady corners, an art form which continues to inspire contemporary artists such as Banksy. Despite the millennia-long deprecation of cave technology (X-caveML 2.0 never really caught on), we can still admire Lascauxian cave art, even though we will probably remain eternally oblivious of its purpose if there ever was any. This sharply contrasts with an Excel 97 sheet named mybooks.xls.bak I tried to open yesterday: perfectly remembering its purpose (my dad was maintaining a list of books he had read), I'm unable to revive the splendid tabular chaos undoubtedly typeset in Times New Roman or worse. 17 years ago somebody made a simple spreadsheet and it's literally less accessible than a 17,000 year old scribble by an unknown caveartist. Not to mention the philistines who are blacking out Banksy's recent works, which date back to last year or so. And certainly don't get me started about sustainable Linked Data. I mean, is there really such a thing? We'll be lucky if any triple at all survives 17 years. Or 17 months, for that matter. Some even have trouble keeping a SPARQL endpoint up for 17 hours. Or minutes. We might not be very good cavemen. This talk combines lessons learned from the Semantic Web, the REST principles, and the Web in general to think about what sustainability for Linked Data could really mean and how we just might achieve it.",SWIB,,,,,,,,,,,
,2016,(Packaged) Web Publication,Ivan Herman,SWIB/SWIB2016/(Packaged) Web Publication.md,"#epub3,#swib/2016,#digital-books,#ebooks,#books,#IDPF,#web/publication","The publication of EPUB3 has been a major step forward for digital publishing. Relying on Web Technologies like HTML, CSS, SVG, and others, EPUB3 offers a solid basis to publish not only digital books, but all sorts of digital publications in a portable, adaptable and accessible manner. However, it is possible to bring the publishing and the Web world even closer together, making the current format- and workflow-level separation between offline/portable and online (Web) document publishing eventually disappear. These should be merely two dynamic manifestations of the same publication: content authored with online use as the primary mode can easily be saved by the user for offline reading in portable document form. Content authored primarily for use as a portable document can be put online, without any need for refactoring the content. Essential features flow seamlessly between online and offline modes; examples include cross-references, user annotations, access to online databases, as well as licensing and rights management. W3C and IDPF have recently started to work towards this vision, exploring the technical challenges to make it a reality; this talk will give an overview on where we are.",SWIB,,,,,,,,,,,
,2016,Catmandu & Linked Data Fragments,"Patrick Hochstenbach,Carsten Klee,Johann Rolschewski",http://jorol.de/talks/2016-SWIB/slides/#1,SWIB/SWIB2016/Catmandu & Linked Data Fragments.md,"#catmandu,#swib/2016,#workshop,#linked/data/fragment,#API,#SPARQL/queries","""Catmandu"" is a command line tool to access and convert data from your digital library, research services or any other open data sets. The ""linked data fragments"" (LDF) project developed lightweight tools to publish data on the web using the Resource Description Framework (RDF). In combination both projects offer an easy way to transform your data to RDF and provide access via a graphical user interface (GUI) and application programming interface (API). We will present all required tools at the workshop. The participants will be guided to transform data to RDF, to host it with a LDF server and to run SPARQL queries against it. The participants should install a virtual machine (VM) as a development environment on their laptops, see here for further information. Audience: Systems librarians, Metadata librarians, Data manager. Expertise: Participants should be familiar with command line interfaces (CLI) and the basics of RDF.",SWIB,,,,,,,,,,
,2016,Entitifying Europeana: building an ecosystem of networked references for cultural objects,"Hugo Manguinhas,Valentine Charles,Antoine Isaac,Timothy Hill",SWIB/SWIB2016/Entitifying Europeana building an ecosystem of networked references for cultural objects.md,"#swib/2016,#europeana,#enrichment,#entities","'In the past years, the number of references to places, peoples, concepts and time in Europeana’s metadata has grown considerably and with it new challenges have arisen. These contextual entities are provided as references as part of the metadata delivered to Europeana or selected by Europeana for semantic enrichment or crowdsourcing. However their diversity in terms of semantic and multilingual coverage and their very variable quality make it difficult for Europeana to fully exploit this rich information. Pursuing its efforts towards the creation of a semantic network around cultural heritage objects and intending in this way to further enhance its data and retrieval across languages, Europeana is now working on a long term strategy for entities. The cornerstone of this strategy is a “semantic entity collection” that acts as a centralised point of reference and access to data about contextual entities, which is based on the cached and curated data from the wider Linked Open Data cloud. While Europeana will have to address the technical challenges of integration and representation of the various sources, it will also have to define a content and curation plan for its maintenance. This presentation will highlight the design principles of the Europeana Entity Collection and its challenges. We will detail our plans regarding its curation and maintenance while providing the first examples of its use in Europeana users' services. We will also reflect on how our goals can fit our partners' processes and how can organizations like national cultural heritage portals and smaller institutions contribute to (and benefit from) such a project as a network.'",SWIB,,,,,,,,,,,
,2016,FREME: A Framework for Multilingual and Semantic Enrichment of Digital Content,"Felix Sasaki,Phil Ritchie,Jan Nehring,Pieter Heyvaert,Kevin Koidl",SWIB/SWIB2016/FREME A Framework for Multilingual and Semantic Enrichment of Digital Content.md,"#freme,#semantic/enrichment,#framework,#swib/2016,#RESTful,#web/services","This workshop introduces FREME, a framework for multilingual and semantic enrichment of digital content. FREME provides 1) a growing set of general knowledge sources and sources from the library and other domains for enrichment; 2) several widely used content formats as input and output of enrichment processes; 3) the ability to re-use published linked data in enrichment pipelines, and 4) the application of standards and best practices like the W3C Web Annotation model or NIF. The purpose of the FREME framework is to give access to so-called e-Services that provide certain enrichment functionalities, like named entity recognition or terminology annotation. The access is given via APIs and graphical user interfaces, e.g. a plugin for the CKEditor. The FREME documentation provides details on how to access the e-Services. The FREME framework is available as a set of configurable components. Most of the components and e-Services are available under Apache 2.0 license and hence suitable for (commercial) use. All codes are on GitHub, and contributions are very welcome. The goal of this workshop is to enable various types of users to work with FREME. We will start with examples for API users who want to deploy the e-Services endpoints relying on the publicly available FREME installation. We then will show how to install FREME on your own server and how to deploy selected components. Finally we will show how to parameterize FREME, e.g. by working with custom data sets for enrichment or by providing your own e-Service. Participants of the workshop should have basic knowledge of linked data and RESTful web services. We will contact registered participants before the workshop to provide use cases for semantic and multilingual enrichment in the realm of SWIB. We then will provide the participants with material to implement the use cases in the public FREME installation, or in their own installation. For the latter case, participants need to fulfill certain hardware requirements that will be shared before the workshop.",SWIB,,,,,,,,,,,
,2016,From MARC silos to Linked Data silos,"Osma Suominen,Nina Hyvönen",SWIB/SWIB2016/From MARC silos to Linked Data silos.md,"#swib/2016,#silos,#MARC,#Fennica,#Viola,#Arto,#Finland","Many libraries are experimenting with publishing their metadata as Linked Data in order to open up bibliographic silos, usually based on MARC records, and make them more interoperable, accessible and understandable to developers who are not intimately familiar with library data. The libraries who have published Linked Data have all used different data models for structuring their bibliographic data. Some are using a FRBR-based model where Works, Expressions and Manifestations are represented separately. Others have chosen basic Dublin Core, dumbing down their data into a lowest common denominator format. The proliferation of data models limits the reusability of bibliographic data. In effect, libraries have moved from MARC silos to Linked Data silos of incompatible data models. Data sets can be difficult to combine, for example when one data set is modelled around Works while another mixes Work-level metadata such as author and subject with Manifestation-level metadata such as publisher and physical form. Small modelling differences may be overcome by schema mappings, but it is not clear that interoperability has improved overall. We present a survey of published bibliographic Linked Data, the data models proposed for representing bibliographic data as RDF, and tools used for conversion from MARC. We also present efforts at the National Library of Finland to open up metadata, including the national bibliography Fennica, the national discography Viola and the article database Arto, as Linked Data while trying to learn from the examples of others.",SWIB,,,,,,,,,,,
,2016,How We Killed Our Most-Loved Service and No One Batted an Eye,"Matias Mikael Frosterus,Mikko Kalle Aleksanteri Lappalainen",SWIB/SWIB2016/How We Killed Our Most-Loved Service and No One Batted an Eye.md,"#swib/2016,#controlled-vocabularies,#vocabularies,#Finland,#thesaurus,#Finto,#APIs,#VESA,#YSA","Controlled vocabularies and IT systems enabling their use have been in the forefront of library work for decades. In the National Library of Finland the national bibliography has been indexed using the YSA general thesaurus since the 1980s. A dedicated browser called VESA was developed in 1999 in order to eliminate the need to publish YSA as a printed document. In user surveys, VESA continually ranked as our most loved service. However, as years went on it became more difficult to integrate VESA’s old code to new environments. When the time came to renew VESA, library world was already buzzing with open linked data, semantic web etc. So it was decided that the new system should provide YSA and other vocabularies as open linked data with the ability to integrate the vocabularies to other systems using modern APIs. In 2013 work begun on the national ontology and thesaurus service Finto slated to replace VESA. Due to VESA being so well-liked, Finto was developed in deep collaboration with the users. Regular usability tests were conducted during the development and in all aspects and features care was taken in order to not put any extra burden on the daily tasks of the annotators. Finto provides the functionalities that VESA did, but also offers various new features and possibilities. An example of an auxiliary feature is the new suggestions system streamlining the process of gathering suggestions for new concepts into Finto vocabularies. Furthermore, the modular design of Finto also allowed us to utilize open APIs in other systems to, e.g., provide direct links to content annotated using a given concept in a vocabulary. We present the lessons learned during the development of a replacement for an extremely well-loved core service of a national library. A particular focus will be on the collaboration with the users during the development process and the migration.",SWIB,,,,,,,,,,,
,2016,IIIF: Linked-Data to Support the Presentation and Reuse of Image Resources,Simeon Warner,SWIB/SWIB2016/IIIF Linked-Data to Support the Presentation and Reuse of Image Resources.md,"#iiif,#apis,#api,#swib/2016,#image/api,#iiif/api","The International Image Interoperability Framework (IIIF) defines simple APIs and data formats to give scholars uniform and rich access to image-based resources hosted around the world. The community aims to develop, cultivate and document shared technologies, including image servers and web clients that provide a world-class user experience in viewing, comparing, manipulating and annotating images. While the framework supports use by non-semantic clients using JSON, all data models and formats are based on linked data. The IIIF thus provides both a rich environment for semantically aware clients, and an opportunity for support within larger linked-data systems. We believe that the rigor of developing within the framework of linked-data has helped in the development of clean semantics and provides a solid foundation for future work and extension. This workshop will briefly introduce the IIIF Image API, for image access and manipulation, and survey current client application. It will then focus on the IIIF Presentation API and the detailed description how resources are organized, related and presented. Participants will have the opportunity to work through some hands-on examples of manipulation of IIIF presentation information, using JSON-LD and linked-data tools, in order to support different interaction and viewing experiences. Audience: developers, systems librarians, those interested in image provision and presentation. Expertise: Participants should be familiar with the basics of linked data and RDF. In order to work through some hands-on examples then you'll need the ability to edit and run simple (Python) programs. Participants not wanting to do this directly could pair with others to work through examples together. Required equipment to work through examples: Laptop with the ability to run Python (2.7 or 3.x) and internet access to download code and modules. I can provide assistance with setup on Linux/OSX but not on Windows machines. (If you would be able and willing to help others in the workshop using Windows then please let me know.)",SWIB,,,,,,,,,,,
,2016,Implementing the IIIF Presentation 2.0 API as a Linked Open Data Model in the Fedora Repository,Christopher Hanna Johnson,SWIB/SWIB2016/Implementing the IIIF Presentation 2.0 API as a Linked Open Data Model in the Fedora Repository.md,"#swib/2016,#iiif,#iiif/api,#linke/open/data/model,#Fedora,#JSON-LD,#RDF/graph","""The IIIF Presentation API specifies a web service that returns JSON-LD structured documents that together describe the structure and layout of a digitized object or other collection of images and related content."" IIIF website The dynamic serialization of IIIF JSON-LD structured manifests via SPARQL CONSTRUCT is an interesting possibility that has great potential for cross-domain discovery and rendering of digitized objects with variable criteria. I have explored this possibility by implementing a data model in the Fedora Commons Repository that matches the specifications of the IIIF Presentation API. Fedora has the facility to index objects via Apache Camel directly to a triplestore. With SPARQL CONSTRUCT, the triplestore can serialize normalized JSON-LD - JavaScript Object Notation Linked Data as a graph. The use of ""ordered lists"" (aka collections) is a fundamental component of JSON-LD and necessary feature of the IIIF manifest sequence which is represented in a canonical RDF graph as a cascade of blank nodes. In order to dynamically create the sequence with SPARQL requires that the data is modelled identically to the IIIF specification. This gist is a representation of a compacted and framed JSON-LD graph that was serialized from a SPARQL query of Fedora metadata. The ability to assemble parts of distinct, disparate and disassociated digital objects on demand in one cohesive presentation becomes a real possibility. For example, the ""range"" object is equivalent to a part of a sequence, like a chapter in a book. With SPARQL, it is possible to target ranges from different ""editions"" based on a metadata specification (i.e. a person, place, or date) and unify them in a manifest object which is then rendered by a client viewer like OpenSeadragon.",ISWC,,,,,,,,,,,
,2016,Improving data quality at Europeana: New requirements and methods for better measuring metadata quality,"Péter Király,Hugo Manguinhas,Valentine Charles,Antoine Isaac,Timothy Hill",SWIB/SWIB2016/Improving data quality at Europeana New requirements and methods for better measuring metadata quality.md,"#swib/2016,#Europeana,#data/quality,#SHACL,#shortcomings","Europeana aggregates metadata from a wide variety of institutions, a significant proportion of which is of inconsistent or low quality. This low-quality metadata acts as a limiting factor for functionality, affecting e.g. information retrieval and usability. Europeana is accordingly implementing a user- and functionality-based framework for assessing and improving metadata quality. Currently, the metadata is being validated (against the EDM XML schema) prior to being loaded into the Europeana database. However, some technical choices with regard to the expressions of rules impose limitations on the constraints that can be checked. Furthermore, Europeana and its partners sense that more than simple validation is needed. Finer-grained indicators for the 'fitness for use' of metadata would be useful for Europeana and its data providers to detect and solve potential shortcomings in the data. Beginning 2016, Europeana created a Data Quality Committee to work on data quality issues and to propose recommendations for its data providers, seeking to employ new technology and innovate metadata-related processes. This presentation will describe more specifically the activities of the Committee with respect to data quality checks: - Definition of new data quality requirements and measurements, such as metadata completeness measures; - Assessment of (new) technologies for data validation and quantification, such as SHACL for defining data patterns; - Recommendations to data providers, and integration of the results into the Europeana data aggregation workflow.",SWIB,,,,,,,,,,,
,2016,Introduction to Linked Open Data,"Jana Hentschke,Christina Harlow,Uldis Bojars",https://docs.google.com/document/d/19y1Zm_bUTK22pIUPd-njVmRGoGYwN16CDExm0vcSALw/edit,SWIB/SWIB2016/Introduction to Linked Open Data.md,"#swib/2016,#workshop,#linked/open/data,#RDF,#dereferenceable/URIs,#vocabularies,#Open/Data","This introductory workshop aims to introduce the fundamentals of linked data technologies on the one hand, and the basic issues of open data on the other. The RDF data model will be discussed, along with the concepts of dereferenceable URIs and common vocabularies. The participants will continuously create and refine RDF documents about themselves including links to other participants to strengthen their knowledge of the topic. Based on the data created, the advantages of modeling in RDF and publishing linked data will be shown. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop. Attendees are not expected to have any technical, RDF or Linked Open Data experience. We do ask that attendees bring a laptop with a modern web browser for participation.",SWIB,,,,,,,,,,
,2016,Linked Data for Production,Philip Evan Schreur,SWIB/SWIB2016/Linked Data for Production.md,"#swib/2016,#LD4P,#BIBFRAME,#LD4L-Labs,#Linked/Data,#Mellon/Foundation,#LOD","The Mellon Foundation recently approved a grant to Stanford University for a project called Linked Data for Production (LD4P). LD4P is a collaboration between six institutions (Columbia, Cornell, Harvard, Library of Congress, Princeton, and Stanford University) to begin the transition of technical services production workflows to ones based in Linked Open Data (LOD). This first phase of the transition focuses on the development of the ability to produce metadata as LOD communally, the enhancement of the BIBFRAME ontology to encompass multiple resource formats, and the engagement of the broader academic library community to ensure a sustainable and extensible environment. As its name implies, LD4P is focused on the immediate needs of metadata production such as ontology coverage and workflow transition. In parallel, Cornell also has been awarded a grant from the Mellon Foundation for Linked Data for Libraries-Labs (LD4L-Labs). LD4L-Labs will in turn focus on solutions that can be implemented in production at research libraries within the next three to five years. Their efforts will focus on the enhancement of linked data creation and editing tools, exploration of linked data relationships and analysis of the graph to directly improve discovery, BIBFRAME ontology development and piloting efforts in URI persistence, and metadata conversion tool development needed by LD4P and the broader library community. The presentation will focus on a brief description of the projects, how they interrelate, and what has been accomplished to date. Special emphasis will be given to extensibility and interactions with the broader LOD community.",SWIB,,,,,,,,,,,
,2016,Linked Open Community,Andromeda Yelton,SWIB/SWIB2016/Linked Open Community.md,"#swib/2016,#open-source,#participation","They say “build it, and they will come”, but what happens if you build it and they don’t? Getting people involved with open source projects takes more than good software or even a compelling use case: it’s about infrastructure, governance, and culture. This talk will cover research, current thinking, and real-world strategies for increasing and diversifying participation in open source projects.",SWIB,,,,,,,,,,,
,2016,Linked Open Data in Practice: Emblematica Online,"Myung-Ja K. Han,Timothy W. Cole,Maria Janina Sarol,Patricia Lampron,Mara Wade,Thomas Stäcker,Monika Biel",SWIB/SWIB2016/Linked Open Data in Practice Emblematica Online.md,"#swib/2016,#Emblematica/Online,#RDFa,#Schema.org,#LOD,#Iconclass/LOD","Emblematica Online allows humanities scholars to seamlessly discover and link to items in a unique virtual emblem corpus distributed across six institutions in the US and Europe. The site supports multi-granular discovery of 1,400+ digitized emblem books and 25,000+ individual emblems from selective emblem books. To better integrate with related digital images and textual resources elsewhere, and to provide additional context for users, the site exploits linked open data (LOD) in two ways. First, as a producer of LOD, it publishes emblem and emblem book metadata as HTML+RDFa with schema.org semantics, making emblem resources more visible and useful in a linked open data context. Second, as a consumer of LOD, it enhances user experiences by utilizing LOD services and resources. For example, using the Iconclass LOD service, Emblematica Online supports multi-lingual browsing of the Iconclass vocabulary and connects users to digital sources elsewhere that share Iconclass descriptors. Also, it provides additional context about authors and contributors, including gender, nationality, and occupation, by reconciling names appearing in emblem metadata with LOD sources, such as the VIAF, DNB, and Wikipedia. This presentation discusses how Emblematica Online publishes its metadata as LOD and improves user experience using LOD sources as well as Emblem ontology development and plans for new services that allow possible reuse of Emblem LOD.",SWIB,,,,,,,,,,,
,2016,Linked Open Development,"Fabian Steeg,Adrian Pohl",https://hbz.github.io/swib16-workshop/,SWIB/SWIB2016/Linked Open Development.md,"#swib/2016,#workshop,#Kanban,#GitHub,#GitHub/API","Increasingly, software in libraries (Software in Bibliotheken, SWIB) is developed as open source software by distributed communities in what could be described as linked open development (LOD). This workshop will introduce you to this way of developing software, its tools, and processes. It will empower you to both set up your own development projects in this way, and contribute to existing projects. Workshop topics are distributed version control basics, open source development workflows, markdown for issues and documentation and workflow visualization with Kanban boards. The current center of the open source community, both in the library world and beyond, is GitHub, a social network for software development. In different exercises, the workshop will introduce you to managing your source code with git, to tracking your issues on GitHub, to integrated development and review tools like Travis CI and Waffle boards, and to using the GitHub API for programmatic access to your data. Audience: developers and librarians involved in software projects; no previous experience needed; requirements: laptop with git, a modern web browser and text editor installed.",SWIB,,,,,,,,,,
,2016,Performing LOD: Using the Europeana Data Model (EDM) for the aggregation of metadata from the performing arts domain,"Julia Beck,Marko Knepper",SWIB/SWIB2016/Performing LOD Using the Europeana Data Model (EDM) for the aggregation of metadata from the performing arts domain.md,"#swib/2016,#europeana,#edm,#germany,#frankfurt,#VuFind,#ECLAP","Imagine a theatre play. There are contributors such as the playwright, director, actors, etc. The play may have several performances with changing casts while actors may contribute to other plays. The play might be based on a drama which also has a screen adaption. All this is documented in manuscripts, photos, videos and other materials. The more relations you find among these performance-related objects, the more it emerges as a perfect use case for linked data. At the University Library Frankfurt am Main, the Specialised Information Service Performing Arts aggregates performing arts-related metadata of artefacts gathered by German-speaking cultural heritage institutions. It is funded by the German Research Foundation and aims to give researchers access to specialized information by providing a VuFind-based search portal that presents the metadata modeled as linked and open data. The Europeana Data Model (EDM) offers a universal and flexible metadata standard that is able to model the heterogeneous data about cultural heritage objects resulting from the data providers’ variety of data acquisition workflows. Being a common aggregation standard in digitization projects a comprehensive collection of mappings already exists. With the amount of delivered manuscript data in mind, the DM2E-extension of EDM was used and further extended by the ECLAP-namespace covering the specific properties for the performing arts domain. The presentation will show real life examples and focus on the modeling as linked data and the implementation within the VuFind framework.",SWIB,,,,,,,,,,,
,2016,Person Entities: Lessons learned by a data provider,John W. Chapman,SWIB/SWIB2016/Person Entities Lessons learned by a data provider.md,"#swib/2016,#OCLC,#WorldCat,#authority-files,#APIs","Continuing the longstanding research program by OCLC in the field of linked data, recent projects have focused on creating sets of entities of high interest for any organization wanting to utilize linked data paradigms. Through intensive mining and clustering of WorldCat bibliographic data, name and subject authority files, and other related data sets, OCLC has produced over 300 million entity representations. These clusters pull together and represent creative works, and persons related to those works. OCLC has engaged with a number of libraries and organizations to create and experiment with this data. A pilot project during October 2015-February 2016 to explore new methods of providing access to Person entities provided a number of new directions and insights. The core purpose of the work is to understand how these entities might best be leveraged to make library workflows more efficient, and to improve the quality of metadata produced in the library sector. This presentation will provide a background on data used in the project, as well as the development of services and APIs to provision the data. It will address challenges and opportunities in the area of creating and managing entities, and ways in which they could be improved and enriched over time.",SWIB,,,,,,,,,,,
,2016,"RDF by Example: rdfpuml for True RDF Diagrams, rdf2rml for R2RML Generation",Vladimir Alexiev,"SWIB/SWIB2016/RDF by Example rdfpuml for True RDF Diagrams, rdf2rml for R2RML Generation.md","#swib/2016,#RDF,#visualization,#rdfpuml,#PlantUML,#GraphViz,#puml,#rdf2rdb","RDF is a graph data model, so the best way to understand RDF data schemas (ontologies, application profiles, RDF shapes) is with a diagram. Many RDF visualization tools exist, but they either focus on large graphs (where the details are not easily visible), or the visualization results are not satisfactory, or manual tweaking of the diagrams is required. We describe a tool *rdfpuml* that makes true diagrams directly from Turtle examples using PlantUML and GraphViz. Diagram readability is of prime concern, and rdfpuml introduces various diagram control mechanisms using triples in the puml: namespace. Special attention is paid to inlining and visualizing various Reification mechanisms (described with PRV). We give examples from Getty CONA, Getty Museum, AAC (mappings of museum data to CIDOC CRM), Multisensor (NIF and FrameNet), EHRI (Holocaust Research into Jewish social networks), Duraspace (Portland Common Data Model for holding metadata in institutional repositories), Video annotation. If the example instances include SQL queries and embedded field names, they can describe a mapping precisely. Another tool *rdf2rdb* generates R2RML transformations from such examples, saving about 15x in complexity.",SWIB,,,,,,,,,,,
,2016,Swissbib goes Linked Data,"Felix Bensmann,Nicolas Prongué,Mara Hellstern,Philipp Kuntschik",SWIB/SWIB2016/Swissbib goes Linked Data.md,"#swib/2016,#RESTful,#API,#metadata,#Swiss,#bibliographic/data,#open-license,#open/license","The project linked.swissbib.ch aims to integrate the Swiss library metadata into the semantic web. A Linked Data infrastructure has been created to provide on the one hand a data service for other applications and on the other hand an improved interface for the end user (e.g. a searcher). The workflow for the development of this infrastructure involves basically five steps: (1) data modeling and transformation in RDF, (2) data indexing, (3) data interlinking and enrichment, (4) creation of a user interface and (5) creation of a RESTful API. The project team would like to highlight some challenges faced during these stages, and the means found to solve them. This includes for example the conception of various use cases of innovative semantic search functionalities to give specifications for data modelling, data enrichment and for the design of the search index. Data processing operations such as transformation and interlinking must be highly scalable, with the aim of an integration in the workflow of the already existing system. Wireframes have been made to realize early usability evaluations. Finally, negotiations have been undertaken with the various Swiss library networks to adopt a common open license for bibliographic data.",SWIB,,,,,,,,,,,
,2016,TIB|AV-Portal - Challenges managing audiovisual metadata encoded in RDF,"Jörg Waitelonis,Margret Plank,Harald Sack",SWIB/SWIB2016/TIBAV-Portal - Challenges managing audiovisual metadata encoded in RDF.md,"#swib/2016,#TIB,#AV-Portal,#entity-linking","The TIB|AV-Portal provides access to high quality scientific videos from the topic area of technology/engineering, architecture, chemistry, information technology, mathematics, and physics in English as well as German language. A key feature of the portal is the use of automated video analysis technologies further enhanced by semantic analyses to enable pinpoint and cross lingual searches on video segment level and to display content-based filter facets for further exploration of the steadily increasing number of its video resources. Based on text-, speech- and image recognition text-based metadata are automatically extracted from the videos and mapped to subject specific GND subject headings via named entity linking. This results in an enrichment of the reliable authoritative metadata by time-based metadata from video analysis. In the talk, we present the strategy and implementation for the RDF-based metadata export of the TIB|AV-Portal to illustrate encountered challenges as well as to justify adopted solutions. This includes the ontology design, balancing the best possible compromise between granularity, simplicity, extensibility and sustainability. Since the data is partially generated by an automatic process it may contain errors or might be incomplete. Accordingly a closer inspection of the data quality is mandatory. Therefore, the main focus of the talk is on data cleansing methods to ensure the best possible quality with reasonable effort. This includes the presentation of requirements as well as the comparison of different approaches ranging from semi-automated methods to manual editing and override. We further demonstrate additional application scenarios based on the semantically annotated data, such as e. g. content based recommendations and exploratory search.",ISWC,,,,,,,,,,,
,2016,Towards visualizations-driven navigation of the scholarship data,"Christina Harlow,Muhammad Javed,Sandy Payette",SWIB/SWIB2016/Towards visualizations-driven navigation of the scholarship data.md,"#swib/2016,#VIVO,#visualizations,#D3","One of the key goals of Cornell University Library (CUL) is to ensure preservation of the scholarly works being published by the Cornell faculty members and other researchers. VIVO is an open source and semantic technologies driven application that enables the preservation and open access of the scholarship across institutions. Driven by different needs, users look at VIVO implementation at Cornell from different viewpoints. The college requires the structure data for reporting needs. The library is interested in preservation of the scholarship data. University executives are interested in identifying the areas where they should invest in the forthcoming future. First, these viewpoints do not completely overlap with each. Second, current user interface represents the scholarship data in the list view format. Such representation of the scholarship data is not easy to use and consumable by the users. In this presentation, we present our ongoing work of integration of D3 visualizations into the VIVO pages. Such visualizations are constructed on the fly based on the underlying RDF data. A visualization-driven approach provides an efficient overview of the huge linked data network of interconnected resources. These visualizations are intuitive for the users to interact and offer the ability to visualize and navigate through the large linked data network. We discuss the performed (data) gap analysis as well as a few of the visualizations in detail and their integration into the VIVO framework.",SWIB,,,,,,,,,,,
,2016,Using LOD to crowdsource Dutch WW2 underground newspapers on Wikipedia,"Olaf Janssen,Gerard Kuys",SWIB/SWIB2016/Using LOD to crowdsource Dutch WW2 underground newspapers on Wikipedia.md,"#swib/2016,#newspapers,#Delpher,#triple-store,#Wikipedia,#WW2","During the second World War some 1.300 illegal newspapers were issued by the Dutch resistance. Right after the war as many of these newspapers as possible were physically preserved by Dutch memory institutions. They were described in formal library catalogues that were digitized and brought online in the ‘90s. In 2010 the national collection of underground newspapers – some 200.000 pages – was full-text digitized in Delpher, the national aggregator for historical full-texts. Having created online metadata and full-texts for these publications, the third pillar 'context' was still missing, making it hard for people to understand the historic background of the newspapers. We are currently running a project to tackle this contextual problem. We started by extracting contextual entries from a hard-copy standard work on Dutch illegal press and combined these with data from the library catalogue and Delpher into a central LOD triple store. We then created links between historically related newspapers and used Named Entity Recognition to find persons, organisations and places related to the newspapers. We further semantically enriched the data using DBPedia. Next, using an article template to ensure uniformity and consistency, we generated 1.300 Wikipedia article stubs from the database. Finally, we sought collaboration with the Dutch Wikipedia volunteer community to extend these stubs into full encyclopedic articles. In this way we can give every newspaper its own Wikipedia article, making these WW2 materials much more visible to the Dutch public, over 80% of whom uses Wikipedia. At the same time the triple store can serve as a source for alternative applications, like data visualizations. This will enable us to visualize connections and networks between underground newspapers, as they developed over time between 1940 and 1945.",SWIB,,,,,,,,,,,
,2016,Who is using our linked data?,"Corine Deliot,Neil Wilson,Luca Costabello,Pierre-Yves Vandenbussche",SWIB/SWIB2016/Who is using our linked data.md,"#swib/2016,#BNB,#British-Library,#Analytics","The British Library published the first Linked Open Data iteration of the British National Bibliography (BNB) in 2011. Since then it has continued to evolve with regular monthly updates, addition of new content (e.g. serials) and new links to external resources (e.g. International Standard Name Identifier (ISNI)). Data is available via deferenceable URIs, a SPARQL endpoint and RDF dataset dumps. There has been clear value to the Library in its linked data work, e.g. learning about RDF modelling and linked data. However, like many linked open data publishers, the Library has found it challenging to find out how the data has been used and by whom. Although basic usage data are captured in logs, there is currently no widely available tool to extract Linked Open Data insights. This makes it challenging to justify continued investment at a time of limited resourcing. This talk will report on collaboration between Fujitsu Laboratories Limited, Fujitsu Ireland and the British Library in the development of a Linked Open Data Analytics platform. The aim of the project was twofold: to examine Linked Open BNB usage and to potentially develop a tool of interest to the wider Linked Open Data community. We will describe the analytics platform and the functionality it provides as well as demonstrate what we found out about the usage of our data. Over the period under consideration (April 2014-April 2015) usage of the Linked Open BNB increased, and there was a discernible growth in the number of SPARQL queries relative to HTTP queries. Usage patterns were traced to the addition of new metadata elements or to linked data tuition sites or events.",SWIB,,,,,,,,,,,
,2016,d:swarm - A Data Management Platform for Knowledge Workers,"Thomas Gängler,Thomas Gersch,Christof Rodejohann",SWIB/SWIB2016/dswarm - A Data Management Platform for Knowledge Workers.md,"#data/management,#etl,#swib/2016,#d-swarm,#etl","d:swarm is a data management platform intended for knowledge workers, e.g., system librarians or metadata librarians. Right now, the focus of this application is on realizing one of the most import steps of an ETL task - the transformation part. Our handy d:swarm Back Office UI can be utilized to create and verify mappings of a certain data source (with help of a sample dataset) to a certain target schema (e.g. schema.org bib extension). This GUI simply runs as a web application in your browser. Besides, one can apply the created and verified mappings with help of the d:swarm Task Processing Unit to process larger amounts of data (i.e. the real datasets). Afterwards, the mapped/resulted data can be imported into various data stores and exported to different formats, like search engine indices (e.g. Solr) or Linked Data. Workshop participants will learn how to use and interact with the d:swarm Back Office UI and d:swarm Task Processing Unit. Furthermore, some background about the design and architecture of the whole d:swarm application will be imparted. Finally, we will show how one can share (self-made) mappings rather easily with the rest of the (library) world. This workshop should treat a common, full ETL workflow of library data processing, i.e. preprocessing and/or harvesting of source data (optional), data mapping with help of d:swarm Back Office UI + applying the mappings to larger amounts of data with help of the d:swarm Task Processing Unit, loading the resulting data into a data store (to be concrete search engine index) + exporting as Linked Data, showing an application (catalogue frontend) that retrieves data from that datastore and uses Linked Data. Audience: systems librarians, metadata librarians, knowledge workers, data mungers. Expertise: Participants could be familiar with ETL Tools (GUI), e.g., OpenRefine or Talend. Required: Laptop with VirtualBox installed. Organizers will provide a VirtualBox image (Linux guest system) beforehand. Participants can also install their own environment. Programming experience: Not required. (However, domain knowledge and/or knowledge about the data formats themselves can and should be an advantage).",SWIB,,,,,,,,,,,
,2017,(Meta)data Management with knime,"Magnus Pfeffer,Kai Eckert",SWIB/SWIB2017/(Meta)data Management with knime.md,"#swib/2017,#KNIME,#APIs,#enrichment,#data/processing,#analysis","KNIME has established itself over the past years as an open platform for all kinds of data processing and analysis. In this workshop, the participants can try their hands at using knime for basic data processing tasks like loading data from files, transforming data into an intermediate format and saving data into files. In the second half, the focus will be on more advanced tasks like using web apis to enrich local data. Target audience: Persons with some knowledge on data format in the bibliographic domain who want to learn about a different approach to data processing. knime is using a graphical ui and is quite intuitive. Knowledge of programming languages is helpful, but for many tasks not required at all.",SWIB,,,,,,,,,,,
,2017,A distributed Network of Heritage Information,Enno Meijers,SWIB/SWIB2017/A distributed Network of Heritage Information.md,"#swib/2017,#Dutch,#network,#glam,#thesauri,#Proof-of-Concept","The Dutch Digital Heritage Network (NDE) started in 2015 by the national cultural heritage institutions as a joint effort to improve the visibility, usability and sustainability of the cultural heritage collections maintained in the GLAM institutions. One of the goals is the realization of a distributed network of heritage information that no longer depends on aggregation of the data. This talk will focus on our approach for developing a new, cross-domain, decentralized discovery infrastructure for the Dutch heritage collections. A core element in our strategy is to encourage institutions to align their information with formal Linked Data resources for people, place, periods, concepts and to publish their data as Linked Open Data. The NDE program works on making all relevant terminology sources available as Linked Data and provide facilities for term alignment and building new thesauri. Another important goal is to provide means for browsing the collections in a cross-domain, user centric fashion. Based on possible relevant URIs identified in the user queries we want to be able to browse the available Linked Data in the cultural heritage network. The bi-directional use of Linked Data without aggregation is still a technological challenge. We decided to build a registry that records the back links for all the URIs used in our network. Next to Linked Data definitions of organizations and datasets we will also record fingerprints of the object descriptions. This information will provide the back links which make it possible to navigate from a term URI to the objects that have a relation with this term. We are currently developing a Proof-of-Concept and will show the first results at the SWIB conference.",SWIB,,,,,,,,,,,
,2017,"Authoring, Annotations, and Notifications in a decentralised Web with Dokieli",Sarven Capadisli,"SWIB/SWIB2017/Authoring, Annotations, and Notifications in a decentralised Web with Dokieli.md","#swib/2017,#dokieli,#workshop","'In this event we will explore dokieli's core principles, architectural and design patterns through demonstrations and discussions. Its sociotechnical design decisions will be justified based on freedom of expression, decentralisation, interoperability, and access. The demonstrations will look into use cases involving authoring articles, annotations, and social interactions in a decentralised manner. This workshop is for the brave ones ;) that want to get a feel of just how far existing native Web technologies and recommendations – Linked Data, read-write Web, WebID, Web Annotations, Linked Data Notifications – can go and help researchers, librarians, annotators, and developers.'",SWIB,,,,,,,,,,,
,2017,BIBFRAME Pilot,"Ray Denenberg,Nate Trail,Sally McCallum",SWIB/SWIB2017/BIBFRAME Pilot.md,"#swib/2017,#linked-data,#bibframe,#pilot,#GitHub,#ontology","The Library of Congress has begun a second Pilot simulating the cataloging environment with Linked Data, a triple store, and the BIBFRAME data model. A great deal of information resulting from the Pilot will soon be available and the session will report on it. The Pilot and the whole BIBFRAME development are carried out in an open environment with specifications, conversion tools, and system components made downloadable via a web site or GitHub as they are developed. The BIBFRAME 2 ontology is also available as an OWL file and many of the controlled vocabularies used for the data, such as subjects and names, are available in RDF. The vocabulary services have been operational for over 5 years and were basic building blocks for this further development. The Pilot has taken a total-environment approach by converting the whole of the Library of Congress MARC catalog to RDF according to the BIBFRAME data model which the 60 catalogers in the Pilot “catalog against” as they create new descriptions of items. Pilot catalogers are specialists (with various language assignments) who deal with monographs, serials, moving image, recorded sound, still image, cartography, and music. The presentation will discuss the tools used (what they did well and less well), aspects of converting a very large file to a very different data model, RDF and ontology issues, and cataloger efficiencies and problems in the new environment. We will also share thoughts on how this effort may fit into the global Linked Data environment, including how it can benefit from further engagement with other communities and services.",SWIB,,,,,,,,,,,
,2017,"BIBFRAME Use: Vocabulary, Conversion, Reconciliation","Ray Denenberg,Nate Trail,Wayne Schneider,Leif Andresen","https: //swib.org/swib17/slides/andresen_bibframe-use.pdf,https: //swib.org/swib17/slides/denenberg_bibframe-ontology-patterns.pdf,https: //swib.org/swib17/slides/trail_bibframe.pdf,https: //swib.org/swib17/slides/schneider-bibframe-conversion-reconciliation.pdf","SWIB/SWIB2017/BIBFRAME Use Vocabulary, Conversion, Reconciliation.md","#swib/2017,#workshop,#BIBFRAME,#MARC,#conversion","This Workshop will be an opportunity to probe topics related to BIBFRAME like ontology, MARC data conversion, and merge and match of BF data converted from MARC to produce Works, Instances, and related resources. A portion of the time will treat ontology patterns and the BIBFRAME approach. The leaders have real experience in all these aspects of BIBFRAME and are interested in an exchange of ideas related to issues they have encountered. The Pilot 2 that the Library of Congress is currently immersed in has dealt with the whole continuum from vocabulary to conversion specs to conversion to merge and match to edit.",SWIB,,,,,,,,,,
,2017,DOREMUS : Doing Reusable Musical Data,"Rodolphe Bailly,Jean Delahousse,Raphael Troncy",SWIB/SWIB2017/DOREMUS  Doing Reusable Musical Data.md,"#swib/2017,#DOREMUS,#FRBRoo,#BnF,#project,#ontology,#vocabularies,#conversion,#interlinking","DOREMUS is a research project that aims i) to propose an extension of the FRBRoo ontology to specifically describe musical resources and ii) to publish linked open datasets of several large catalogs coming from the French National Library (BnF), the Philharmonie of Paris and the French public service radio broadcaster (Radio France), that are originally described in MARC and ad-hoc XML formats. In the perspective of data interoperability between those datasets, the DOREMUS project has also produced controlled vocabularies describing keys, modes, derivations, musical genres, medium of performances by re-using existing resources (e.g. IAML vocabularies, Rameau), or creating new ones. We will present the results of DOREMUS project, including the ontology, the controlled vocabularies, the tools for data conversion and interlinking and examples of reuse of those results in a music recommendation system.",SWIB,,,,,,,,,,,
,2017,De l’Une à l’Autre: Towards Linked Data in Special Collections Cataloging,"Regine Heberlein,Joyce Bell,Lidia Santarelli,Jennifer Baxmeyer,Peter Green",SWIB/SWIB2017/De l’Une à l’Autre Towards Linked Data in Special Collections Cataloging.md,"#swib/2017,#LD4P,#Princeton,#Derrida,#modeling,#vitrolib,#LD4L-Labs,#API","A member of the Mellon-funded Linked Data for Production (LD4P) initiative, LD4P at Princeton is participating in defining linked data ontologies specifically for the description of special collections materials. It uses a hand-selected set of 525 items from the Library of Jacques Derrida – all of which bear inscriptions by persons who gifted the books to Derrida or his household – to investigate tools, workflows, and data models that will make the creation of linked descriptive data for annotated material viable in a production environment. To this end, LD4P at Princeton is exploring modeling these inscriptions on an adaptation of the (W3C Web Annotation Data Model) and linking them to bibliographic data converted to (BIBFRAME 2.0) or related models. Since the VitroLib ontology editor, which was developed at Cornell University as part of the LD4L-Labs initiative, is designed to use (bibliotek-o; a supplement to BIBFRAME developed by LD4L Labs and the LD4P Ontology Group), LD4P at Princeton anticipates using bibliotek-o at least part of the time. The presentation will give an overview of the workflows and tools the group has developed, demonstrate the current data model, and discuss our current thinking on implementation issues such as front- and back-end interfaces, API's and the curation of external data sources, and lessons learned along the way.",SWIB,,,,,,,,,,,
,2017,Every Collection is a Snowflake,George Oates,SWIB/SWIB2017/Every Collection is a Snowflake.md,"#swib/2017,#inconsistencies,#wellcome-library,#visualisation,#accuracy","The promise of pristine linked data is powerful and compelling, but in practice, we’re working with data created by humans, which is full of inconsistencies and gotchas. Instead of trying to force computers to understand these foibles, we’ve been working directly with data creators and using data visualisation to see the grain of individual catalogues including Open Library and the fabulous Wellcome Library collection. This work has gently challenged myths of data completeness and accuracy, and even helped data creators see their own collection data in a new light.",SWIB,,,,,,,,,,,
,2017,Finnish National Bibliography Fennica as Linked Data,Osma Suominen,SWIB/SWIB2017/Finnish National Bibliography Fennica as Linked Data.md,"#swib/2017,#marc,#Fennica,#conversion,#bibframe,#schema-org,#reconciling,#HDT,#clustering,#finland","The National Library of Finland is making our national bibliography Fennica available as Linked Open Data. We are converting the data from 1 million MARC bibliographic records first to BIBFRAME 2.0 and then further to the Schema.org data model. In the process, we are clustering works extracted from the bibliographic records, reconciling entities against internal and external authorities, cleaning up many aspects of the data and linking it to further resources. The Linked Data set is CC0 licensed and served using HDT technology. The publishing of Linked Data supports other aspects of metadata development at the National Library. For some aspects of the Linked Data, we are relying on the RDA conversion of MARC records that was completed in early 2016. The work clustering methods, and their limitations, inform the discussions about potentially establishing a work authority, which is a prerequisite for real RDA cataloguing. This presentation will discuss lessons learned during the publishing process, including the selection and design of the data model, the construction of the conversion pipeline using pre-existing tools, the methods used for work clustering, reconciliation and linking as well as the infrastructure for publishing the data and keeping it up to date.",SWIB,,,,,,,,,,,
,2017,High Quality Linked Data Generation,"Anastasia Dimou,Ben de Meester,Pieter Heyvaert,Ruben Verborgh",SWIB/SWIB2017/High Quality Linked Data Generation.md,"#swib/2017,#quality,#complicated,#metadata,#RML,#RML-Mapper,#RML-Workbench,#RML-Editor,#RML-Validator","Linked Data allows the description of domain-level knowledge that is understandable by both humans and machines. Nevertheless, as machines are intolerant of unexpected input, the quality of the underlying Linked Data largely determines the success of the envisaged Semantic Web. Despite, though, the significant number of existing tools, generating Linked Data by incorporating heterogeneous data from multiple sources and different formats into the Linked Open Data cloud remained complicated, let alone generating their metadata. Raw data values are expected to be used as extracted, while when data transformations occur, they remain coupled and case-specific in separate not-reusable systems. Moreover, quality assessment is performed after Linked Data is published and adjustments are manually – but rarely – applied, while the violations root is not identified. In this talk, we present a sustainable semantic-driven approach, based on the RML toolchain (RML Mapper, RML Workbench, RML Editor and RML Validator), which we adopt to address the aforementioned shortcomings and enables data owners to generate high quality Linked Data by themselves. This way, we facilitate and automate the generation of high quality Linked Data with accurate, consistent and complete metadata, offering a granular, sustainable and generic solution that shortens the Linked Data generation workflow, and achieves higher integrity within Linked Data.",SWIB,,,,,,,,,,,
,2017,Improving Named Entity Recognition in the Biodiversity Heritage Library with Machine Learning,"Katie Mika,Alicia Esquivel",SWIB/SWIB2017/Improving Named Entity Recognition in the Biodiversity Heritage Library with Machine Learning.md,"#swib/2017,#taxonomy,#workflows,#biodiversity","Scientific names are important access points to biodiversity literature and significant indicators of content coverage. The Biodiversity Heritage Library (BHL) mines its content using the open source Global Names Recognition and Discovery (GNRD) tool from the Global Names Architecture (GNA) suite of machine learning and named entity recognition algorithms, to extract scientific names to index and attach to page records. The 2017 BHL National Digital Stewardship Residents (NDSR) are working collaboratively on a group of projects designed to deliver a set of best practices recommendations for the next version of the BHL digital library portal. NDSR Residents Katie Mika and Alicia Esquivel will discuss (i.) BHL and the significance of taxon names, (ii.) the current workflow, proposed improvements, and example workflows for linking content across scientific names including semantic linking to biodiversity aggregators such as Encyclopedia of Life and the Global Biodiversity Information Facility, (iii.) how to use scientific names for content analysis, and (iv.) optimizing manuscript transcription of archival content, which introduces problems like outdated and common names, misspellings, and antiquated taxonomies to GNA tools. Authors invite questions, comments, and discussion from audience members as the Residents prepare to submit their final recommendations at the end of the year.",SWIB,,,,,,,,,,,
,2017,Integrating Distributed Data Sources in VIVO via Lookup Services,"Tatiana Walther,Martin Barber,Anna Kasprzik",SWIB/SWIB2017/Integrating Distributed Data Sources in VIVO via Lookup Services.md,"#swib/2017,#RDF,#GND,#API,#VIVO","Recording information about countries, conferences, organizations and concepts in a Linked Data application like VIVO means at the first stage an initial import of a large number of data items, which beforehand must be transformed into RDF and manually enriched with persistent identifiers, geographic position, short description, and multilingual labels. Collecting, enriching and converting such an amount of information cost considerable temporal and administrative efforts. Storage of the amount of data can slow down the performance, responsiveness and reasoning processes of an application. Lookup services, already developed for VIVO, DSpace-CRIS, Linked Data for Libraries (LD4L) and other projects are aimed to facilitate the integration of external authority data. Whereas some vocabularies and data sources like EuroVoc and Wikidata offer a SPARQL endpoint, other authority data sources such as the Integrated Authority File of the German National Library (GND) provide only data dumps. Our objective is to enable a combined access to external sources via a single interface, using Named Entity Recognition tools, APIs and SKOSMOS in the background. Beside concepts we would also provide integration of such data items as events, organizations and languages, supplemented with additional information, which requires mappings between source and target systems in order to insert and display attributes and relations of the selected entities. Furthermore we investigate the automated transferring of the changes made in external vocabularies to the data in the target system. This presentation outlines our achievements and lessons learned concerning the integration of semantically structured and enriched data from distributed sources via lookup services, similar to the external vocabulary services in VIVO and related projects.",SWIB,,,,,,,,,,,
,2017,Integrating LOD into Library’s Digitized Special Collections,"Myung-Ja Han|Myung-Ja K. Han,Deren Kudeki,Timothy W. Cole,Jacob Jett,Caroline Szylowicz",SWIB/SWIB2017/Integrating LOD into Library’s Digitized Special Collections.md,"#swib/2017,#metadata,#transformation,#enrichment,#special-collection","Management of digitized special collections requires ‘special’ curation; integrating Linked Open Data (LOD) into these collections also requires customized approaches different from those used for general library collections. With support from the Andrew W. Mellon Foundation, the Linked Open Data for Digitized Special Collections project is examining challenges and opportunities of LOD for three digitized special collections held at the University of Illinois. We began by transforming legacy metadata into RDF, using schema.org semantics to identify classes of entities conflated in original metadata accounts, e.g., visual artworks, person entities, theater productions, published works, and added links for linked data sources. These new LOD accounts were then embedded as JSON-LD - JavaScript Object Notation Linked Data in HTML pages to make the LOD visible to harvesting agents. To provide users more context, client-side JavaScript (relying on JQuery and Mustache.js) generates a mash-up of local descriptions and properties fetched from LOD providers like viaf.org and dbpedia.org in real-time. Links to further context are provided for person entities (playwrights, composers, actors, directors), venues (theaters), works (plays), and performances. This presentation will showcase the challenges of transforming special collection metadata into LOD and the opportunities to meet users’ needs by using LOD enrichment.",SWIB,,,,,,,,,,,
,2017,Introduction to Linked Open Data,"Christina Harlow,Uldis Bojars,Huda Jaliluddin Khan","https://swib.org/swib17/slides/harlow_introduction-lod.pdf,https://swib.org/swib17/slides/wilcox_%20introduction-lod.pdf",SWIB/SWIB2017/Introduction to Linked Open Data.md,"#swib/2017,#linked-open-data,#workshop","This introductory workshop aims to introduce the fundamentals of linked data technologies on the one hand, and the basic issues of open data on the other. The RDF data model will be discussed, along with the concepts of dereferenceable URIs and common vocabularies. The participants will continuously create and refine RDF documents about themselves including links to other participants to strengthen their knowledge of the topic. Based on the data created, the advantages of modeling in RDF and publishing linked data will be shown. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop.",SWIB,,,,,,,,,,
,2017,Linking the Data: Building effective Authority and Identity Lookup,"Huda Jaliluddin Khan,E. Lynette Rayle,David Eichmann,Simeon Warner,Dean Krafft",SWIB/SWIB2017/Linking the Data Building effective Authority and Identity Lookup.md,"#swib/2017,#LD4P,#lookup,#reconciliation,#vitrolib","The Mellon Foundation-funded Linked Data for Libraries Labs (LD4L-Labs) and Linked Data for Libraries Production (LD4P) projects are exploring the library community’s transition to Linked Open Data. Authority and identity lookups are integral to cataloging workflows and provide excellent opportunities for exploring how to leverage the power of Linked Data for reconciliation and more effective lookups. Central questions in this work include the implementation, performance and reliability of lookup services; what multiple authority lookups mean with respect to reconciliation; and user interface design. We are currently experimenting with caching mechanisms that address the issues of access, reliability, and speed, including an approach that uses a Linked Data Fragments server for caching and serving up authority results. We are also exploring the implementation of a containerized and potentially mirrored search index populated using Jena and Fuseki to execute SPARQL queries against Linked Data sources. We are extending the Questioning Authority (QA) work being developed for the Hydra repository to enable more configurable Linked Data lookups that can be used across different technology stacks. VitroLib, an experimental cataloging tool being developed using the Vitro platform, integrates QA lookups to support catalogers searching for and linking to authorities. We will demonstrate the lookup configuration and integration into both the Hydra repository and VitroLib. All the software is open source.",SWIB,,,,,,,,,,,
,2017,Managing Assets as Linked Data with Fedora,David John Wilcox,"https://swib.org/swib17/slides/wilcox_fedora.pdf,https://swib.org/swib17/slides/wilcox_%20extended-services.pdf",SWIB/SWIB2017/Managing Assets as Linked Data with Fedora.md,"#swib/2017,#Fedora,#APIs,#API,#workshop","Fedora is a flexible, extensible, open source repository platform for managing, preserving, and providing access to digital content. Fedora is used in a wide variety of institutions including libraries, museums, archives, and government organizations. Fedora 4 introduces native linked data capabilities and a modular architecture based on well-documented APIs and ease of integration with existing applications. Recent community initiatives have added more robust functionality for exporting resources from Fedora in standard formats to support complete digital preservation workflows. Both new and existing Fedora users will be interested in learning about and experiencing Fedora features and functionality first-hand. Attendees will be given pre-configured virtual machines that include Fedora bundled with the Solr search application and a triplestore that they can install on their laptops and continue using after the workshop. These virtual machines will be used to participate in hands-on exercises that will give attendees a chance to experience Fedora by following step-by-step instructions. Participants will learn how to create and manage content in Fedora in accordance with linked data best practices and the Portland Common Data Model. Attendees will also learn how to import resources into Fedora and export resources from Fedora to external systems and services as part of a digital curation workflow. Finally, participants will learn how to search and run SPARQL queries against content in Fedora using the included Solr index and triplestore.",SWIB,,,,,,,,,,
,2017,Perspectives on using Schema.org for publishing and harvesting Metadata at Europeana,"Nuno Freire,Richard Wallis,Antoine Isaac,Valentine Charles,Hugo Manguinhas",SWIB/SWIB2017/Perspectives on using Schema.org for publishing and harvesting Metadata at Europeana.md,"#swib/2017,#Europeana,#schema-org,#recommendations","Providing access to Europe’s digital cultural heritage is Europeana’s core mission. To do so, Europeana has progressively adopted the principles of Linked Data for representing, aggregating and enriching the metadata it collects and is now looking at the emerging web technologies to refine its services. Over the past years we have followed very closely the development of Schema.org and looked into its potential for increasing the visibility of cultural heritage data on the web and their consumption by search engines. We will present a set of recommendations for publishing Europeana metadata using the Schema.org vocabulary and report on the status of implementation. We address the representation of the embedded metadata as part of the Europeana HTML pages and sitemaps so that the re-use of this data can be optimized. We also produce a Schema.org representation of Europeana resources described with the Europeana Data Model (EDM), being the richest as possible and tailored to Europeana’s realities and user needs as well the search engines and their users. In an aggregation context, interoperability with Schema.org can also mean that Europeana can use Schema.org data as a source for its own data services. We will discuss how Schema.org data available in cultural heritage organizations’ websites can be used as a method to provide metadata for ingestion to Europeana and present our first Schema.org harvesting experimentations.",SWIB,,,,,,,,,,,
,2017,Practical Data Provenance in distributed Environment or: implementing Linked Data Broker using Microservices Architecture,"Joonas Kesäniemi,Stefan Negru,João da Silva",SWIB/SWIB2017/Practical Data Provenance in distributed Environment or implementing Linked Data Broker using Microservices Architecture.md,"#swib/2017,#ATTX,#project,#brokers,#data-brokers,#data/brokers","Maintaining some sort of data provenance, i.e. the data about the actions that have led the target data to its current state, is an integral feature of a system acting as a data broker. After all, brokering can be seen as an activity that transforms external data sources, which one might not have any control over, into new data source. This transformation can involve complex processing steps, which all contribute to the provenance data. Keeping track of who did what, why and when, is therefore necessary in order be able to ascribe responsibility of, e.g. data quality, to the right (human or software) entity. We have been developing a data broker solution based on semantic web technologies that is flexible and extendable both in terms of incoming and outgoing data, as well as the cloud based infrastructural resources employed to operate the broker instance. Our solution consists of components implementing different types of services such as workflow and graph management, processing, distribution and provenance. We present the result of the ATTX project, which provides a set of software components that can be used to build scalable data brokers that work on linked data. We will cover issues and implementation related to modeling, acquisition, exposing and using provenance information produced by services that comprise the ATTX data broker instance.",SWIB,,,,,,,,,,,
,2017,Practical Linked Data Annotations on IIIF Image Resources,"Michael Appleby,Tom Crane,Glen Robson,Simeon Warner","https://swib.org/swib17/slides/appleby_mirador-hands-on.pdf,https://swib.org/swib17/slides/appleby_open-annotation-data-model.pdf,https://swib.org/swib17/slides/robson_brief-introduction-to-iiif.pdf,https://swib.org/swib17/slides/robson_introduction-search-api.pdf,https://swib.org/swib17/slides/robson_sas-simple-annotation-server.pdf,https://swib.org/swib17/slides/robson_visualising-annotations.pdf,https://swib.org/swib17/slides/warner_annotations.pdf,https://swib.org/swib17/slides/warner_presentation-api.pdf",SWIB/SWIB2017/Practical Linked Data Annotations on IIIF Image Resources.md,"#swib/2017,#workshop,#IIIF,#API","The International Image Interoperability Framework (IIIF) defines a set of common application programming interfaces that support interoperability between image repositories. The IIIF APIs are designed to provide a common method for accessing high resolution images and to enable the development of sophisticated client applications that allow image comparison, manipulation, and annotation. A growing community of cultural heritage institutions is publishing content using these APIs, which are based on linked data models and specify JSON-LD serializations that are easily consumed by clients. The ease of access to content afforded by IIIF has accelerated the development of both automated and interactive workflows for enrichment using the Open Annotation Data Model, which plays a fundamental role in the IIIF Presentation API.",SWIB,,,,,,,,,,
,2017,"The bibliotek-o Framework: Principles, Patterns, and a Process for Community Engagement","Steven Folsom,Jason Kovari,Rebecca Younes","SWIB/SWIB2017/The bibliotek-o Framework Principles, Patterns, and a Process for Community Engagement.md","#swib/2017,#ontology,#BIBFRAME,#bibliotek-o,#framework,#ontology/principles","This presentation provides a detailed description of ontology development efforts undertaken by Linked Data for Libraries Labs and Linked Data for Production partners to extend BIBFRAME 2.0 and enhance with alternative models, which have yielded the bibliotek-o framework (available: bibliotek-o GitHub Repository and bibliotek-o website). The framework includes the bibliotek-o ontology alongside well-established ontologies, building off BIBFRAME as its core. We are not creating a competitor to BIBFRAME; instead, our intention is to demonstrate select alternative models for consideration by the community and BIBFRAME architects as development continues in future versions of BIBFRAME. We will discuss motivations and focus on bibliotek-o modeling patterns, notably areas of deviation from BIBFRAME; in doing so, we will demonstrate how we believe that these models provide queryable patterns and align with ontology principles and best practices. Further, we will discuss efforts around development of an application profile and MARC-to-bibliotek-o mapping for use in aligned, in-development tooling for metadata production and conversion of legacy data. Our goal is to promote open development of bibliotek-o, including community engagement, feedback, collaboration, testing and adoption. To encourage engagement with the SWIB community, we will provide pointers to the various types of bibliotek-o documentation that are available and outline our strategy to engage with the community. With greater participation from the community we can hopefully begin to converge around a shared set of practices with a clear process for iterative improvements.",SWIB,,,,,,,,,,,
,2017,Unlocking Citations from tens of millions of scholarly Papers,Dario Taraborelli,SWIB/SWIB2017/Unlocking Citations from tens of millions of scholarly Papers.md,"#swib/2017,#citations,#I4OC,#open-citation","Citations are the foundation for how we know what we know. Until recently, the idea of creating a freely accessible repository of citation data – representing how scholarly works cite each other – has been hampered by restrictive and inconsistent licenses and by the lack of comprehensive, machine-readable data sources: for decades, references have been locked inside PDFs or proprietary databases. Launched in April 2017, the Initiative for Open Citations (I4OC) has made nearly half of all indexed scholarly references freely available to everyone with no copyright restrictions. The percentage of indexed scholarly works with open reference data was 1% before the launch of the I4OC: as of July 2017, over 16 million scholarly works have open references available as machine-readable public domain data. There’s now momentum and a growing number of organizations, scholarly societies, funders, and publishers in support of the unconstrained availability of scholarly citation data. However, this is just the beginning of a journey to build high-quality scientific commons. In this talk, I’ll present how the I4OC was created, its current vision and challenges. I'll showcase examples of real-world applications demonstrating how data unlocked by the initiative can be reused to accelerate scientific discovery and the broader impact of scholarship knowledge.",SWIB,,,,,,,,,,,
,2017,Visual Concept Detection and Linked Open Data at the TIB AV-Portal,"Felix Saurbier,Matthias Springstein",SWIB/SWIB2017/Visual Concept Detection and Linked Open Data at the TIB AV-Portal.md,"#swib/2017,#TIB,#metadata,#spatio-temporal,#named-entity-linking,#TensorFlow,#authority/files","The German National Library of Science and Technology (TIB) researches and develops methods of automated content analysis and semantic web technologies to improve access to its library holdings and allow for advanced methods of information retrieval (e.g. semantic and cross-lingual search). Regarding scientific videos in the TIB AV-Portal spatio-temporal metadata is extracted by several algorithms analysing (1) superimposed text, (2) speech, and (3) visual content. In addition, the results are mapped against common authority files and knowledge bases via a process of automated Named Entity Linking and published as Linked Open Data to facilitate reuse and interlinking of information. Against this background the TIB constantly aims to improve its automated content analysis and Linked Open Data quality. Currently, extensive research in the fields of deep learning is conducted to significantly enhance methods of visual concept detection in the AV-Portal – both in terms of detection rates and coverage of subject-specific concepts. Our solution applies a state-of-the-art deep residual learning network based on the popular TensorFlow framework in order to predict and link visual concepts in audio-visual media. The resulting predictions are mapped against authority files and expressed as RDF-Triples. Therefore, in our presentation we would like to demonstrate how research in the field of machine learning can be combined with semantic web technologies and transferred to library services like the AV-Portal to improve functionality and provide added value for users. In addition we would like to address the question of data quality assessment and present scenarios of metadata reuse.",SWIB,,,,,,,,,,,
,2017,Will you be my bf: forever? Analysing Techniques for Conversion to BIBFRAME at the University of Alberta,"Ian Bigelow,Sharon Farnel",SWIB/SWIB2017/Will you be my bf forever Analysing Techniques for Conversion to BIBFRAME at the University of Alberta.md,"#swib/2017,#alberta,#bibframe,#AACR,#RDA","The University of Alberta is actively trying to ramp up for Linked Data through local experimentation, research and partnerships with other institutions. Though BIBFRAME is still in development, several transformation tools have already been created, and with many libraries thinking about planning for moving to Linked Data it would seem timely to compare approaches to moving legacy MARC data to BIBFRAME. Setting aside the question of whether BIBFRAME should be the approach for libraries to move to Linked Data, this investigation aimed at comparing two tools for converting MARC to bf:2.0: A) LC MARC to BIBFRAME XSLT: An XSLT 1.0 application aimed at converting MARC to RDF/XML released in March 2017; and B) Casalini SHARE Virtual Discovery Environment: A project by Casalini Libri and @Cult to develop a Linked Data discovery environment, including a conversion tool for MARC to bf:2.0 RDF. Through the comparison and analysis of these transformation tools several topics will be explored: Comparison of underlying development models and performance of the tools; Comparison of data element conversion and impact for discovery; Impact of content standard on conversion efficacy (AACR vs. RDA); Implications for conversions for various formats (monographs, serials, et cetera); URI enrichment pre/post conversion; In house and vendor workflow implications.",SWIB,,,,,,,,,,,
,2018,Adding your own stuff to Wikidata,"Jakob Voß,Joachim Neubert",https://www.wikidata.org/wiki/Wikidata:Events/SWIB_2018,SWIB/SWIB2018/Adding your own stuff to Wikidata.md,"#swib/2018,#Wikidata,#QuickStatements,#modeling,#workshop","'The hands-on tutorial will introduce to usage and collaboration in Wikidata with focus on data import and mapping of existing data sets with Wikidata. Basic concepts of Wikidata, how it can be edited and queried are first explained with exercises. We will then work on connecting a common example dataset to the central database. Typical tools - such as Mix'n'match or QuickStatements - and workflows are shown and tested. The focus of the workshop is not only on technical aspects but on questions of data modeling, data provenance, references, and policies. Basic experience with Wikidata, Wikipedia, and SPARQL is helpful but not required. Participants are asked to create a Wikimedia account in advance and to bring a laptop with them.'",SWIB,,,,,,,,,,
,2018,Annif: leveraging bibliographic metadata for automated subject indexing and classification,Osma Suominen,SWIB/SWIB2018/Annif leveraging bibliographic metadata for automated subject indexing and classification.md,"#swib/2018,#annif,#indexing,#classification,#REST,#API,#Finland","Manually indexing documents for subject-based access is a very labour-intensive intellectual process. A machine could perform similar subject indexing much faster. However, an algorithm needs to be trained and tested with examples of indexed documents. Libraries have a lot of training data in the form of bibliographic databases, but often only a title is available, not the full text. We propose to leverage both title-only metadata and, when available, already indexed full text documents to help indexing new documents. To do so, we are developing Annif, an open source tool for automated indexing and classification. After feeding it a ACRONYMS/SKOS vocabulary and existing metadata, Annif knows how to assign subject headings for new documents. It has a microservice-style REST API and a mobile web app that can analyse physical documents such as printed books. We have tested Annif with different document collections including scientific papers, old scanned books and current e-books, Q&A pairs from an ""ask a librarian"" service, Finnish Wikipedia, and the archives of a local newspaper. The results of analysing scientific papers and current books have been reassuring, while other types of documents have proved more challenging. The new version currently being developed is based on a combination of existing NLP and machine learning tools including Maui, fastText and Gensim. By combining multiple approaches, Annif can be adapted to different settings. The tool can be used with any vocabulary and with suitable training data, documents in many different languages may be analysed. With Annif, we expect to improve subject indexing and classification processes especially for electronic documents as well as collections that otherwise would not be indexed at all.",SWIB,,,,,,,,,,,
,2018,Applying Linked Data technologies as a Backend infrastructure for scientific search portals,"Benjamin Zapilko,Katarina Boland,Dagmar Kern",SWIB/SWIB2018/Applying Linked Data technologies as a Backend infrastructure for scientific search portals.md,"#swib/2018,#infrastructure,#research,#ontology","In recent years, Linked Data became a key technology for organizations in order to publish their data collections on the web and to connect it with other data sources on the web. With the ongoing change in the research infrastructure landscape where an integrated search for comprehensive research information gains importance, organizations are challenged to connect their historically unconnected databases with each other. In an online survey with 337 social science researchers in Germany, we found evidence that researchers are interested in links between information of different types and from different sources. However, in current scientific portals this is often not yet reflected. In this presentation, we present how Linked Open Data technologies can generally be used to build a backend infrastructure for scientific search portals. This backend infrastructure is set as an additional layer between unconnected non-RDF data collections and makes the links between datasets visible and usable for retrieval via a search index. To address occurring heterogeneity with vague links between datasets, a research data ontology is used in addition for representing different versions and aggregations of research datasets. The LOD backend infrastructure is in use at the search portal of GESIS. The in-use application of our approach has been evaluated in this scientific search portal for the social sciences by investigating the benefit of links between different data sources in a user study. The source code of this project is publicly available.",SWIB,,,,,,,,,,,
,2018,Automation and standardization of semantic video annotations for large-scale empirical film studies,"Henning Agt-Rickauer,Christian Hentschel,Harald Sack",SWIB/SWIB2018/Automation and standardization of semantic video annotations for large-scale empirical film studies.md,"#swib/2018,#annotations,#video,#ontology,#vocabularies","The study of audio-visual rhetorics of affect scientifically analyses the impact of auditory and visual staging patterns on the perception of media productions as well as the conveyed emotions. By large-scale corpus analysis of TV reports, documentaries and genre-films of the topos “political crisis”, film scientists aim to follow the hypothesis of TV reports drawing on audio-visual patterns in cinematographic productions to emotionally affect viewers. However, localization and description of these patterns is currently limited to micro-studies due to the involved extremely high manual annotation effort. The AdA Project presented here, therefore, pursues two main objectives: 1) creation of a standardized annotation ontology based on Linked Open Data principles and 2) semi-automatic classification of audio-visual patterns. Linked Open Data annotations enable the publication, reuse, retrieval, and visualization of data from film studies based on standardized vocabularies and Semantic Web technology. Furthermore, automatic analysis of video streams allows to speed up the process of extracting audio-visual patterns. Temporal video segmentation, visual concept detection and audio event classification are examples for the application of computer vision and machine learning technologies within this project. The ontology as well as the created semantic annotations of audio-visual patterns are published as Linked Open Data in order to enable reuse and extension by other researchers. The annotation software as well as the extensions for automatic video analysis developed and integrated by the project are published as open source as we envision these tools to be useful for general deep semantic analysis of audio-visual archives.",SWIB,,,,,,,,,,,
,2018,Building a better repository with Fedora,David Wilcox,https://github.com/fcrepo4/fcrepo4,SWIB/SWIB2018/Building a better repository with Fedora.md,"#swib/2018,#fedora,#workshop","Fedora is a flexible, extensible, open source repository platform for managing, preserving, and providing access to digital content. Fedora is used in a wide variety of institutions including libraries, museums, archives, and government organizations. The latest version of Fedora introduces native linked data capabilities and a modular architecture based on well-documented APIs and ease of integration with existing applications. This year the Fedora community will publish a formal specification of the Fedora REST API that includes alignment with modern web standards such as the Linked Data Platform, Memento, and Web Access Control. This workshop will provide an opportunity for new and existing users to get hands-on experience working with Fedora features and standards-based functionality. Attendees will be given pre-configured virtual machines that include Fedora bundled with the Solr search application and a triplestore that they can install on their laptops and continue using after the workshop. The VM requires a laptop with at least 4GB of RAM. Participants will learn how to create and manage content in Fedora in accordance with linked data best practices and the Portland Common Data Model. Attendees will also learn how to exercise standards-based functionality such as versioning using Memento and authorization using Web Access Control. Finally, participants will learn how to search and run SPARQL queries against content in Fedora using the included Solr index and triplestore. This is an introductory workshop - no prior Fedora experience is required, though some familiarity with repositories will be useful.",SWIB,,,,,,,,,,
,2018,"Capturing cataloger expectations in an RDF editor: SHACL, lookups and VitroLib","Steven Folsom,Huda Khan,Lynette Rayle,Jason Kovari,Rebecca Younes,Simeon Warner","SWIB/SWIB2018/Capturing cataloger expectations in an RDF editor SHACL, lookups and VitroLib.md","#swib/2018,#LD4L,#LD4L-Labs,#SHACL,#VitroLib","The Linked Data for Libraries Labs (LD4L-Labs) and Linked Data for Production (LD4P) projects have created an RDF editor by involving the catalogers at every stage of a user centered design process. Faced with the challenge of developing an RDF editor which would support the desired data outputs that were loosely defined during ontology development, it became clear application profiles were required as an added layer on top of the formal definitions of the selected ontology terms. Through the lens of two use cases (cataloging LP records and rare monographs) we will discuss how the project involved a blend of catalogers, technologists, and ontologists to build VitroLib, a working prototype editor optimized for specific content types and with integrated lookup services. We will describe findings from cataloger assessment of the search behavior and display of information about entities across datasets from our work on lookup services based on the Questioning Authority gem. Continuing preliminary work discussed at SWIB 2017, we will provide details on the construction of SHACL in support of form building and the translation process from SHACL into configuration required for VitroLib. We will highlight challenges, including examples where SHACL semantics do not translate directly to VitroLib form definitions, the need for extensions to SHACL in support of forms, and the lack of best practices in the nascent SHACL community.",SWIB,,,,,,,,,,,
,2018,Connecting the dots of Linked Data of resource collections,Thorsten Liebig,SWIB/SWIB2018/Connecting the dots of Linked Data of resource collections.md,"#swib/2018,#data/silos,#knowledge/graphs,#challenging","Libraries and museums around the globe are transforming their catalogs and records into Linked Data to foster linkage and navigation across data silos. This in return significantly enriches their own assets with valuable information stemming from related sources. The result is a large Knowledge Graph of cross-linked resources typically based on standards such as RDF or OWL. However, navigating and querying large graphs is challenging. Sure, many retrieval tasks are best served by standard user interfaces based on forms and fields. Despite that data providers and users often complain about poor tool support for explorative navigation through complex cross-linked library data. In fact, there should be something in between query forms and SQL/SPARQL query syntax. We will discuss tool support for visually analyzing and querying large LOD volumes with the help of example data from museums and the scholarly domain for providers and users. This includes interactive network rendering approaches, faceted search and other visualization paradigms that promise to ad hoc understand, analyze and track graph-based data as a whole. Relevant benchmark criteria in this respect are among others: - Scalability of visualization approach and user orientation (data provider & user) - User guidance during data exploration (data provider & user) - Support in detection of data patterns or flaws to increase data quality (data provider) - Presumed knowledge of the data schema or query languages (user)",SWIB,,,,,,,,,,,
,2018,Documenting and preserving programming languages and software in Wikidata,"John Samuel,Katherine Thornton,Kenneth Seals-Nutt",SWIB/SWIB2018/Documenting and preserving programming languages and software in Wikidata.md,"#swib/2018,#wikidata,#wdprop","The digital landscape is evolving very fast. Programming languages as well as softwares once taught in universities and previously well-used among developers may not have the same acceptance among the new generation of developers. Initiatives like the Open Preservation Foundation, and Software Heritage play an important role to document, study and preserve these softwares for future generations. With the creation of Wikidata, the game has now changed. Wikidata provides an easy way to document and describe digital solutions using linked open data. Properties to describe various aspects of programming languages, softwares and mobile applications are being continuously proposed, created and supported by the Wikidata community. It is now increasingly becoming a central hub for linked data sources, thus enabling users to get a complete picture of a given digital artifact, especially for complementary information like dependencies, versions etc. We will present WDProp that can help both new and regular Wikidata contributors to get the latest information on Wikidata supported languages, datatypes, properties as well as community curated projects for finding relevant properties. To further facilitate this process, we also introduce the portal Wikidata for Digital Preservation. This free software portal allows people to quickly contribute to Wikidata. The interface guides users in contributing data in alignment with current data models for the domain of computing.",SWIB,,,,,,,,,,,
,2018,Engaging information professionals in the process of authoritative interlinking,"Lucy McKenna,Christophe Debruyne,Declan O'Sullivan",SWIB/SWIB2018/Engaging information professionals in the process of authoritative interlinking.md,"#swib/2018,#interlinking,#survey,#ontology,#linked/data,#LAM","'Through the use of Linked Data (LD), Libraries, Archives and Museums (LAMs) have the potential to expose their collections to a larger audience and to allow for more efficient user searches. Despite this, relatively few LAMs have invested in LD projects and the majority of these display limited interlinking across datasets and institutions. A survey was conducted to understand Information Professionals' (IPs') position with regards to LD, with a particular focus on the interlinking problem. The survey was completed by 185 librarians, archivists, metadata cataloguers and researchers. Results indicated that, when interlinking, IPs find the process of ontology and property selection to be particularly challenging, and LD tooling to be technologically complex and unsuitable for their needs. Our research is focused on developing an authoritative interlinking framework for LAMs with a view to increasing IP engagement in the linking process. Our framework will provide a set of standards to facilitate IPs in the selection of link types, specifically when linking local resources to authorities. The framework will include guidelines for authority, ontology and property selection, and for adding provenance data. A user-interface will be developed which will direct IPs through the resource interlinking process as per our framework. Although there are existing tools in this domain, our framework differs in that it will be designed with the needs and expertise of IPs in mind. This will be achieved by involving IPs in the design and evaluation of the framework. A mock-up of the interface has already been tested and adjustments have been made based on results. We are currently working on developing a minimal viable product so as to allow for further testing of the framework. We will present our updated framework, interface, and proposed interlinking solutions.'",SWIB,,,,,,,,,,,
,2018,From LOD to LOUD: making data usable,"Fabian Steeg,Adrian Pohl,Pascal Christoph",https: //hbz.github.io/swib18-workshop/,SWIB/SWIB2018/From LOD to LOUD making data usable.md,"#swib/2018,#LOUD,#APIs,#JSON-LD,#OpenRefine","Linked Open Usable Data (LOUD) extends Linked Open Data (LOD) by focussing on use cases, being as simple as possible, and providing developer friendly web APIs with JSON-LD. The term was coined by Rob Sanderson. This workshop will introduce you to the basic concepts of LOUD, web APIs, and JSON-LD. You'll learn how to publish and document data as LOUD, and how to use that data in different contexts. In this workshop, we will: (1) Convert RDF data into usable JSON-LD (2) Index and query the data with Elasticsearch (3) Create a simple web application using the data (4) Visualize the data with Kibana (5) Document the data using Hypothesis annotations (6) Use the data with OpenRefine. Audience: librarians and developers working with linked open data. Requirements: Laptop with Elasticsearch 6.x, OpenRefine 2.8, a text editor, web browser, and a command line with cURL and jsonld.js via node.js. As an alternative, we'll also provide a fully configured virtual machine to workshop participants.",SWIB,,,,,,,,,,
,2018,Integrating library metadata in a semantic web research environment for university collections,Martin Scholz,SWIB/SWIB2018/Integrating library metadata in a semantic web research environment for university collections.md,"#swib/2018,#digitization,#data/modelling,#CIDOC,#CRM,#ontology","The university library of Erlangen-Nürnberg harbors not only historic manuscripts and monographs but also valuable paintings, graphics, coins, etc. These cultural objects form part of the university’s rich and heterogeneous collections, ranging from anatomy and archaeology to school history and zoology. Embedded in a broader digitization strategy, the project ""Objekte im Netz” develops a documentation and research platform together with a data model based on semantic web technologies and guidelines to provide means to ensure homogeneous data modelling and minimum data quality across the collections. Eventually, they shall provide scholars the means to work on transdisciplinary research questions, too. The tool builds upon the open source WissKI software, a virtual research environment for cultural heritage that natively stores data using RDF. The common data model is implemented as an OWL ontology that extends the CIDOC CRM and acts as the integrating link between the collections, and also to external sources. This presentation will focus on the integration of the university library's metadata of historic graphics: The metadata can be harvested as MARCXML from different systems and has to be transformed into RDF data aligned with the CIDOC CRM. Following a sketch of the general set-up, an outline of the practical integration steps will be given with a discussion of different technical and data mapping approaches, among others a mapping from BIBFRAME to CIDOC CRM. The discussion is accompanied by remarks on challenges and hindrances met so far, like the lack of officially coined URIs and ontology mismatches.",SWIB,,,,,,,,,,,
,2018,Introduction to Linked Open Data,"Christina Marie Harlow,Simeon Warner,Camille Villa",https://swib.org/swib18/slides/0_warner_intro-lod.pdf,SWIB/SWIB2018/Introduction to Linked Open Data.md,"#swib/2018,#workshop,#lod","This introductory workshop aims to introduce the fundamentals of linked data technologies on the one hand, and the basic issues of open data on the other. The RDF data model will be discussed, along with the concepts of dereferenceable URIs and common vocabularies. The participants will continuously create and refine RDF documents about themselves including links to other participants to strengthen their knowledge of the topic. Based on the data created, the advantages of modeling in RDF and publishing linked data will be shown. On a side track, Open Data principles will be introduced, discussed and applied to the content that is being created during the workshop.",SWIB,,,,,,,,,,
,2018,Libraries and their communities: participation from town halls to mobile phones,Mia Ridge,SWIB/SWIB2018/Libraries and their communities participation from town halls to mobile phones.md,"#swib/2018,#public,#crowdsourcing,#libraries","The rise of the internet has provided both opportunities and challenges for libraries. Focusing on the recent history of public participation in libraries enabled by crowdsourcing built on networked, digital platforms, this talk will also consider how libraries have evolved to provide new forms of access to their collections with open data and new working practices.",SWIB,,,,,,,,,,,
,2018,"Linked data implementations — who, what, why?",Karen Smith-Yoshimura,"SWIB/SWIB2018/Linked data implementations — who, what, why.md","#swib/2018,#OCLC,#survey","Prompted by the interest among metadata managers within the OCLC Research Library Partnership in the potential of linked data applications to make new, valuable uses of existing metadata, OCLC Research conducted an International Linked Data Survey for Implementers in 2014 and 2015, receiving responses from a total of 90 institutions in 20 countries. Curious about what might have changed in the past three years since the last survey, and eager to learn about new projects or services that format metadata as linked data or make subsequent uses of it, OCLC Research repeated the survey between 17 April and 25 May 2018. The survey questions were mostly the same so we could more easily compare results. This presentation will summarize the 2018 survey results, and focus on comparing them with the results of the previous two surveys, including: 1) Which institutions have implemented or are implementing linked data and for what purpose. What linked data sources these institutions are consuming, and why. Which linked data sources are cited more or less frequently than in the previous surveys? Have motivations changed? 3) What data are these institutions publishing as linked data, and why. Are there different types of data being published as linked data since 2015? Have the drivers for publishing linked data changed? 4) What barriers have implementers had to address, and how would they advise others who are considering starting a project or service that consumes and/or publishes linked data. 5) A sampling of linked data projects or services in production to represent a variety of different uses, scales, domains, and maturity, especially those described as ""successful in achieving the desired outcome(s).” The surveys provide a partial view of the linked data landscape, as the analysis is confined to the implementers who responded, primarily from the library domain.",SWIB,,,,,,,,,,,
,2018,Linking YSO and LCSH for better subject access,"Satu Niininen,Osma Suominen",SWIB/SWIB2018/Linking YSO and LCSH for better subject access.md,"#swib/2018,#YSO,#ontology,#lcsh,#Finna,#finland","Linking concept schemes enables dynamic ways to reuse already existing metadata across linguistic and organisational barriers. This presentation outlines the lessons learned in the process of translating the General Finnish Ontology (YSO) into English and linking it to the Library of Congress Subject Headings (LCSH). The process involved manual translation of all 30,000 YSO concepts and establishing links to LCSH whenever an applicable equivalent was available. This meant connecting the indexing languages of two very different cultural spheres. Different practices in concept scheme construction also posed a set of challenges as the structure (e.g. hierarchy and view on precoordination) and intended use of concept schemes can vary significantly. It was a logical choice to do linking and translation simultaneously as both tasks include the same initial steps of specifying the scope and definition of each concept. Consulting LCSH also assisted in establishing more functional translations by clarifying how the concepts are used and understood in an English-speaking context. Out of all YSO concepts 44% have links to LCSH, but of the 100 most used LCSH concepts at the Finna.fi service, 69% are linked from YSO. In the future, the mappings can be used for generating Finnish YSO concepts to records with pre-existing LCSH annotations. An early experiment demonstrated that for such records, around half of the LCSH subjects seen in bibliographic records could be automatically converted to YSO concepts thanks to the mappings, despite the fact that the links from YSO cover only a fraction (less than 5%) of LCSH concepts.",SWIB,,,,,,,,,,,
,2018,Powering Linked Open Data applications with Fedora and Islandora CLAW,David Wilcox,SWIB/SWIB2018/Powering Linked Open Data applications with Fedora and Islandora CLAW.md,"#swib/2018,#Fedora,#islandora,#claw,#repository,#REST,#API,#Drupal,#Toronto,#Palladio","Repositories have traditionally focused on storing content and metadata for use by local applications and services, but this is a poor fit for the world of linked open data. Fedora, the flexible, extensible, open source repository platform, has been designed and implemented as not just a repository but a linked data server. This has been accomplished primarily through alignment with the Linked Data Platform recommendation from the W3C, but Fedora also has a formally specified REST API that aligns with a variety of modern web standards, such as Memento, Web Access Control, and Activity Streams 2.0. This focus on linked data and web standards has allowed Fedora to serve as a reliable repository that also powers web-based linked open data applications. The latest version of Islandora, codenamed CLAW, integrates Fedora with Drupal 8, the popular content management system. CLAW takes full advantage of Fedora’s linked data capabilities while also leveraging Drupal’s powerful network of contributed modules to provide a modern, web-based repository platform that enables linked open data applications. This can be seen in production at the University of Toronto Scarborough (UTSC), where CLAW has been used to build a site that provides Palladio visualizations and exposes a SPARQL endpoint for complex RDF queries. This presentation will provide an overview of the latest versions of Fedora and Islandora CLAW with a focus on the linked data and web-based features and functionality. It will also use the UTSC site as an example of how Fedora can power linked open data applications. (Source code: fcrepo4, CLAW)",SWIB,,,,,,,,,,,
,2018,Sharing RDF data models and validating RDF graphs with ShEx,"Katherine Thornton,Tom Baker,Eric Prud'hommeaux,Andra Waagmeester",SWIB/SWIB2018/Sharing RDF data models and validating RDF graphs with ShEx.md,"#swib/2018,#modeling,#RDF,#Wikidata,#PyShEx,#workshop","Participate in a hands-on workshop about Shape Expressions (ShEx), a concise, formal, modeling and validation language for RDF structures. When reusing RDF graphs created by others, it is important to know how the data is represented. Current practices of using human-readable descriptions or ontologies to communicate data structures often lack sufficient precision for data consumers to quickly and easily understand data representation details. We provide concrete examples of how we use ShEx as a constraint and validation language that allows humans and machines to communicate unambiguously about data assets. We will provide an overview of the ShEx language and related tooling. We will introduce the Javascript and Python implementations in RDF data validation workflows applied contexts. We will walk participants through a data modeling example drawn from the bibliographic domain. We will also demonstrate a validation workflow drawn from the domain of computing, where we will use ShEx to validate entity data from Wikidata. The workshop will take approximately four hours: (1) Overview of ShEx (60 mins) (2) Data Modeling with ShEx (40 mins) (3) Data Validation with ShEx (40 mins) (4) Hands-on exploration of the tools (60 mins). Participants in this workshop will understand the basics of creating a ShEx schema, install either ShEx.js or PyShEx on their local machines, and gain experience using the Online Validator to test entity data from Wikidata to a ShEx schema. Links to software we will use in the workshop: ShEx.js, PyShEx, ShEx Online Validator Audience/Requirements: This workshop is for anyone interested in validating RDF data. A working knowledge of RDF is sufficient background for participation. Please bring your own laptop computer.",SWIB,,,,,,,,,,,
,2018,Supporting LCSH subject indexing with LOD in a Nigerian university library,"Babarinde Ayodeji Odewumi,Adetoun Adebisi Oyelude",SWIB/SWIB2018/Supporting LCSH subject indexing with LOD in a Nigerian university library.md,"#swib/2018,#LCSH,#nigeria","Navigating through the peculiar challenges of a third world country could be sort of an art, particularly towards producing quality work in heavily under-funded libraries. With limited financial resources and limited internet connectivity, the University of Ibadan (UI) Library has had to rely on the generosity of organizations, who provide datasets or metadata in various forms (e.g. LOD, MARC, SRU, etc.), to build tools that can support the classification and cataloguing of items especially those published in Nigeria. One of such is the creation of a Search tool using an LOD from the Library of Congress: the LC Subject Headings (LCSH) Dataset. The developers of the University’s Integrated Library System (UIILS) were able to use this dataset to create a Web Service within the UIILS that allows cataloguing staff search through the LCSH entries, even when there is a downtime internet connectivity and access to updated print copies are not possible. This search tool provides the staff with the Classification Number for an item based the subjects they search for. We'll walk through the processes involved from setting up the Apache Jena and Fuseki Server, to generating SPARQL queries from search parameters, processing of query responses, how those responses are displayed to cataloguing staff and how those responses can LINK to other queries.",SWIB,,,,,,,,,,,
,2018,"The Semantic Web: vision, reality and revision",James Hendler,"SWIB/SWIB2018/The Semantic Web vision, reality and revision.md","#swib/2018,#vision,#semantic-web,#machine-readable,#metadata,#google,#unrealized","In 2001, James Hendler joined Web inventor Tim Berners-Lee and their colleague Ora Lassila in writing an article describing a vision for the Semantic Web. The paper, which appeared in Scientific American, has been widely cited and led to much work in both academia and industry aimed at adding machine-readable text to the Web. Now, nearly 20 years later, Google reports that machine-readable metadata is found on over 40% of their crawl and knowledge graph technology, which also grew from this vision, is now a big business used by major organizations around the world. Also growing out of that vision has been the use of linked data in many applications particularly including collection management in libraries, museums and video archiving applications. However, despite this success, much of the original vision of the Semantic Web remains unrealized. In this talk, he discusses what was in the original vision, what has occurred and, most importantly, what still remains to be done if we are truly to recognize the full potential of the Semantic Web.",SWIB,,,,,,,,,,,
,2018,Transformations for aggregating Linked Open Data,"Lukas Koster,Ivo Zandhuis",SWIB/SWIB2018/Transformations for aggregating Linked Open Data.md,"#swib/2018,#aggregation,#data-aggregation,#data/aggregation,#transformation","'Linked Open Data is usually provided as-is. Institutions make choices how to model the data, including properties, blank nodes and uris, for valid reasons. If you want to combine data, there are generally two options: 1) do a distributed query and inference on the data 2) aggregate the data into a new, single endpoint. Distribution enables the use of all available data structures, aggregation enables more easy-to-use data and better performance. For aggregation it is good practice to do transformations to obtain the needed convenience. We give an overview of the transformation types needed, learned in the AdamNet Library Association project AdamLink, a collaboration of the Amsterdam City Archives, Amsterdam Museum, University of Amsterdam Library, Public Library of Amsterdam and International Institute of Social History. The objective is to create a linked open data infrastructure connecting the member institutions’ collections on the topic of ""Amsterdam", targeted at reuse by researchers, teachers, students, creative industry and general public. We discuss the (dis)advantages of creating an aggregation vs. distribution of queries. Every transformation type should solve a distribution problem to be useful. But transformation probably reduces querying-options on the data. We therefore need to get the best trade-off between complexity and usability. An interesting option to investigate is to apply a caching node mechanism," that could combine the best of both worlds. We distinguish 6 types of transformation: Mapping ontologies - Mapping and adding thesauri and authority lists - Mapping and adding object-types - Adding our own statements - Restructuring data - Data-typing. We will illustrate the transformations with real examples. We will also discuss the issues with feeding back the enriched data in the cache or aggregation to the original data sources.'""",SWIB,,,,,,
,2018,"Wikibase: configure, customize, and collaborate","Stacy Allison-Cassin,Dan Scott",https://stuff.coffeecode.net/2018/wikibase-workshop-swib18.html,"SWIB/SWIB2018/Wikibase configure, customize, and collaborate.md","#swib/2018,#Wikibase,#workshop","Originally developed for the Wikidata project, Wikibase ""is a collection of applications and libraries for creating, managing and sharing structured data."" It offers a multilingual platform for linked open data, including a human-friendly editing interface, a SPARQL endpoint, and programmatic means of loading and accessing data, making it a potential match for libraries that like Wikidata's platform but want to maintain a local store of linked open data. In this workshop, we will discuss how a local Wikibase instance can support library needs, and work through exercises that include: Setting up a local Wikibase instance - Adding users - Creating custom classes and properties - Adding and editing entries - Loading data in bulk - Querying the data - Integrating data with external applications. Prerequisites: Participants will need a laptop to install Wikibase and run through the exercises. The Wikibase virtual machine requires 4GB of RAM, so a remote virtual machine such as an Amazon Web Services or Google Cloud instance might be an alternative.",SWIB,,,,,,,,,,
,2018,data.bnf.fr as a sandbox for FRBRization: automated work creation in data.bnf.fr,"Sébastien Peyrard,Etienne Cavalié,Aude Le Moullec-Rieu,Raphaëlle Lapôtre",SWIB/SWIB2018/data.bnf.fr as a sandbox for FRBRization automated work creation in data.bnf.fr.md,"#swib/2018,#BnF,#RDF,#enrichment","The French national library uses the data.bnf.fr online platform as an interface to disseminate its metadata on the semantic web. Source metadata includes notably, but is not limited to, MARC records to be converted and published as RDF triples, with authority records used as the basis for data.bnf.fr landing pages; the RDF graph is also enriched with alignments with external resources or FRBR relations between internal entities. 2018 is a game changer as a number of works displayed in data.bnf.fr will not be grounded on existing authority records, but will be automatically computed from clusters of bibliographic records. The first corpus for this will be the French textual works from the XXth and XXIst centuries, for which several hundred thousands works will be created. These works will be available as web pages available on the data.bnf.fr public interface, and as RDF data retrievable through the data.bnf.fr SPARQL Endpoint. In the long run, such works will be uploaded as MARC authority records in the main catalog and will comply with the traditional workflow where data.bnf.fr feeds on the BnF catalogue to generate its entities with persistent identifiers and records. In the meantime, the BnF must find answers to a number of questions: - Which data can reliably be used to group together bibliographic records as manifestations of a same work? - Which data can be used from those bibliographic records to compute work-level metadata? - What identifier can we use to identify such works that will not be persistent until they are uploaded in the BnF catalogue? - How can we promote this new metadata and inform its reuse by communicating about their specific nature and limits? The contribution will present methods and tools, steps and questions the data.bnf.fr team encountered in this process.",SWIB,,,,,,,,,,,
,2019,20 million URIs and the overhaul of the Finnish library sector subject indexing,"Matias Frosterus,Jarmo Saarikko,Okko Vainonen",SWIB/SWIB2019/20 million URIs and the overhaul of the Finnish library sector subject indexing.md,"#swib/2019,#finland,#YSA,#LCSH,#SKOS,#Melinda","The library sector of Finland has been using the General Finnish Thesaurus YSA and its Swedish language counterpart Allärs for over thirty years. YSA is the most widely used thesaurus in Finland and comprises of some 36,000 concepts. Lately, the National Library of Finland has been developing the General Finnish Ontology YSO, a multilingual successor of YSA and Allärs built according to linked data principles. YSO has been linked to the Library of Congress Subject Headings, over a dozen Finnish vocabularies, and Wikidata. This year, the development of YSA and Allärs was frozen and the Finnish libraries are switching to using YSO and linked data en masse. This presentation describes the process of the switch and the lessons learned. First, we needed two sets of conversion rules: one for converting the SKOS YSO into MARC authority records to support the subject indexing processes and another one to convert the millions of bibliographic records from YSA and Allärs annotations to YSO. Devising the rules turned out to be a very complex task and we formed an expert group with representation from various types of libraries. The conversion extends from the national union catalog Melinda to the local library databases employing various library systems. To this end we developed open source conversion programs and made them available to libraries and library system providers. Aside from the conversion, we also added YSO URIs to the MARC records making linking and updates simpler in the future.",SWIB,,,,,,,,,,,
,2019,Automated subject indexing with Annif,"Osma Suominen,Mona Lehtinen,Juho Inkinen,Anna Kasprzik",SWIB/SWIB2019/Automated subject indexing with Annif.md,"#swib/2019,#Annif,#Finland,#automation,#REST,#API,#workshop","Due to the proliferation of digital publications, intellectual subject indexing of every single literature resource in institutions such as libraries is no longer possible. For the task of providing subject-based access to information resources of different kinds and with varying amounts of available metadata, it has become necessary to explore possibilities of automation. In this hands-on tutorial, participants will be introduced to the multilingual automated subject indexing tool Annif as a potential component in a library’s metadata generation system. By completing exercises, participants will get practical experience on setting up Annif, training algorithms using example data from the organizing institutions NLF and ZBW, and using Annif to produce subject suggestions for new documents using both the command line interface and the web user interface and REST API provided by the tool. The tutorial will also introduce the corpus formats supported by Annif so that participants will be able to apply the tool to their own vocabularies and documents. Participants are requested to bring a laptop with at least 8GB of RAM and at least 20 GB free disk space. The organizers will provide the software as a preconfigured virtual machine. No prior experience with the Annif tool is required, but participants are expected to be familiar with subject vocabularies (e.g. thesauri, subject headings or classifications) and subject metadata that reference those vocabularies.",SWIB,,,,,,,,,,,
,2019,Controlled vocabulary mapping with Cocoda,Jakob Voß,SWIB/SWIB2019/Controlled vocabulary mapping with Cocoda.md,"#swib/2019,#workshop,#Cocoda,#mapping,#APIs","During the last years we developed the web application Cocoda for creating and managing mappings between library classification schemes, authority files, knowledge graphs, and similar knowledge organization systems. The workshop will introduce you to the technical background of Cocoda and vocabulary mappings. You will learn how to set up and configure your own instance of Cocoda and related services, and how to integrate additional vocabularies. We will explore the JSKOS data format and APIs, discuss integration of additional data sources and brainstorm about quality assurance and usability. The participants are required to bring their own computer with NodeJS (at least version 8), git, and basic practical knowledge of processing JSON files. Participants are encouraged to bring their own vocabularies, mappings, and/or data sources.''",SWIB,,,,,,,,,,,
,2019,Cool and the BnF gang: some thoughts at the Bibliothèque nationale de France about handling persistent identifiers,Raphaëlle Lapôtre,SWIB/SWIB2019/Cool and the BnF gang some thoughts at the Bibliothèque nationale de France about handling persistent identifiers..md,"#swib/2019,#BnF,#PID,#RAMEAU,#indexing,#SPAR,#digital/preservation","“Cool URI’s don’t change,” as Tim Berners Lee wrote in 1998, implying that URIs should remain the same as long as possible. Still, cool URIs are also supposed to use web protocols to communicate information about the object they identify. This second requirement somehow contradicts the first, as web protocols are also mutable. Indeed, the recent transition of a lot of web resources towards the secure version of HTTP for indexing purposes brought change in URIs. At the Bibliothèque nationale de France (BnF), this recent evolution of web technologies, combined with a revision of our indexing language RAMEAU, prompted us to start a thinking process about what in URIs should constitute durability - other than the immutability of the URI design. For now, the BnF’s answer is twofold: a URI should be considered completely stable only if the identified resource is somehow digitally preserved. As the BnF has a tool for digital preservation at its disposal (SPAR), it should also have a tool that keeps track of changes in URIs. Furthermore, the institution should communicate about its policy regarding the future of the URIs it is handling, even if this policy implies possible mutation or disappearing of the identifier: thus, reliability of the identifier wouldn’t solely depend on the permanence of its design, but also on the trustable transparency of the institution with regards to its commitment in maintaining identifiers.",SWIB,,,,,,,,,,,
,2019,Data modeling in and beyond BIBFRAME,Tiziana Possemato,SWIB/SWIB2019/Data modeling in and beyond BIBFRAME.md,"#swib/2019,#Share-VDE,#BIBFRAME,#bibliographic/data","Share-VDE has reached its production phase, with over 20 libraries involved in the analysis, conversion, enrichment and publication of their data (originally in MARC21) according to BIBFRAME. Unlike some software components, the deliverables (library datasets and the Cluster Knowledge Base) are open source and accessible as dump and/or as SPARQL end-point, available to enhance other databases. Share-VDE uses external sources in various formats (VIAF, ISNI, Wikidata, LC data etc.) in the enrichment process. Share-VDE tools are evolving and aim to allow librarians wider and more direct interaction with bibliographic data expressed in Linked Data. The CKB editor and the URI Registry, two of the main tools for this direct interaction with data in LD, permit data to be validated, updated, controlled and maintained, ensuring a quality that massive and automated procedures cannot fully guarantee. The CKB Editor will be released in open source as an RDF/BIBFRAME Editor. Since the CKB is released in two versions (Postgres database, for internal use, and RDF, for external use), the update procedures will connect with both versions, using APIs to align them through automatic and manual procedures. Further analysis is also being carried out to verify how the BIBFRAME ontology complies to the requirements of a collective catalogue, intended as a catalogue produced by the integration of independent library catalogues. In this context, the SuperWork and Master Instance concepts implemented in Share-VDE are considered necessary extensions of the BIBFRAME ontology to better identify and qualify more specific entities. The presentation will touch upon these complex issues, typical of the current technological environment, but still conditioned by the dominant cataloguing tradition. Discussions and analysis of these topics, shared with the Library of Congress, will be presented.",SWIB,,,,,,,,,,,
,2019,Design for simple application profiles,"Karen Coyle,Tom Baker",SWIB/SWIB2019/Design for simple application profiles.md,"#swib/2019,#DCMI,#Dublin-Core,#ShEx,#SHACL","'The Dublin Core Metadata Initiative has long promoted the notion of semantic interoperability on the basis of shared global vocabularies, or namespaces, selectively used and constrained for specific purposes in application profiles. For twenty years, the Dublin Core community has used application profiles for requirements ranging from building consensus about metadata structures and content within communities of practice to serving as templates for metadata creation and, more recently, as a basis for conformance validation and quality control. The emergence in the past few years of new validation languages (ShEx and SHACL) have provided an impetus for re-examining the long-elusive goal of making it easier for content experts to develop actionable profiles with user-friendly interfaces without having to rely on IT experts. A DCMI Application Profiles Interest Group started in April 2019 aims at creating a core model for simple application profiles that can meet the most common, straightforward use cases.'",SWIB,,,,,,,,,,,
,2019,Digital sources and research data: linked and usable,"Florian Kräutli,Esther Chen",SWIB/SWIB2019/Digital sources and research data linked and usable.md,"#swib/2019,#digital/research,#digital/knowledge,#Knowledge/Graph,#CIDOC,#graph,#ResearchSpace,#Metaphactory","We present the Max Planck Digital Research Infrastructure for the Humanities (MP-DRIH), which we developed to address an immediate need: the ability to maintain digital sources and research outputs in ways that they remain not only accessible, but also usable in the long term. Our ambition is to close the digital research lifecycle: to make sure that digital research outputs can be discovered, accessed, and reused. We achieve this through the adoption of a common model to represent our digital knowledge, and the implementation of linked open data technologies for data storage and exchange. At the centre of our infrastructure is a Knowledge Graph, which makes all our digital artefacts – be they sources, annotations or entire research databases – centrally accessible. Key challenges are to bring data from various sources together in ways that retain the original context and detail, and to provide users with an environment in which they are able to make sense of this vast information resource. We address these challenges by harmonising all input data to a common model (CIDOC-CRM) that does not compromise the data's original expressivity. The resulting graph becomes usable through ResearchSpace, a software system based on the semantic data platform Metaphactory. We have successfully implemented a pilot project to evaluate the feasibility of our approach and have implemented a production-ready first version of the entire system together with our partners.",SWIB,,,,,,,,,,,
,2019,Empirical evaluation of library catalogues,Péter Király,SWIB/SWIB2019/Empirical evaluation of library catalogues.md,"#swib/2019,#MARC,#user-tasks,#metadata,#FRBR","The library community is in the transition period from Machine Readable Cataloguing (MARC) to some linked data based metadata schema. MARC is complex both as data structure and as semantic structure. This complexity leads to a wide range of errors. When we transform records from MARC to semantic schemas, we should not suppose that we have structurally and semantically perfect records. The aim of this presentation is to call attention to the typical issues revealed by an investigation of 16 library catalogues. The most frequent issue types are usage of undocumented schema elements, then improper values in places where a value should be taken from a dictionary, or should match to other strict requirements. MARC has a number of features which makes validation a challenge such as intensive use of information compressing and encoding techniques, versions without versioning, dependency on internal and external data dictionaries, it is huge (the “core” MARC21 has about 3000 semantic data elements, while other versions defined several hundreds more), and the standard itself is not a machine-readable rule set. Some of these errors might block the transformation of the records, some others might survive in the new structure as well if we do not fix them. The research aims to detect different issues of the metadata records. The foremost of them are those which do not fit the rules defined by the standard. Organized by structure the tool detects issues on record level, in control fields, in data fields, in indicators and in subfields. It also calculates completeness, Thompson-Traill completeness, runs a functional analysis based on the FRBR defined “user tasks” and provides a web based user interface. In the research process a tool has been built which contains an (exportable) object model of the standard.",SWIB,,,,,,,,,,,
,2019,"Forever in between : similarities and differences, opportunities and responsibilities in the LODLAM universe",Saskia Scheltjens,"SWIB/SWIB2019/Forever in between  similarities and differences, opportunities and responsibilities in the LODLAM universe.md","#swib/2019,#cultural-heritage,#LODLAM,#digital-strategy,#open-science,#open/science","Digital cultural heritage data from libraries, archives and museums is often seen as homogeneous, sharing similar semantic technological challenges and possible solutions. While this indeed might be the case, there are also differences that are interesting to take a closer look at. Current interdisciplinary research, technological innovations and societal changes also pose new challenges and opportunities. Can the LODLAM world live up to the expectations? Should it? And why? This talk will cover research, current projects and real-world examples from the library and museum world with a focus on digital strategy, networked infrastructure and open science concepts.",SWIB,,,,,,,,,,,
,2019,From raw data to rich(er) data: lessons learned while aggregating metadata,Julia Beck,SWIB/SWIB2019/From raw data to rich(er) data lessons learned while aggregating metadata.md,"#swib/2019,#cultural-heritage,VuFind,#preprocessing,#modeling,#EDM,#interlinking,#enrichment","'In the project Specialised Information Service (SIS) Performing Arts metadata from German-speaking cultural heritage institutions from the performing arts domain is aggregated in a VuFind-based search portal. As the gathered metadata tends to be very heterogeneous due to various software solutions, workflows and material types in the participating GLAM institutions, not only the differences in data format and standardization are a challenge for modeling data as LOD. This talk also highlights how the differences in scope and detail of description, the lack of a common vocabulary and the handling of entities are tackled in order to make the collections linked and searchable. Over the years, we learned that thorough analysis of the delivered metadata in cooperation with the data providers is key in improving the search experience for users. The current workflow from raw to linked data basically involves four steps: (1) thorough analysis and documentation of the delivered data, (2) preprocessing of the raw data for more interoperability, (3) modeling and transforming the preprocessed title and authority data in EDM, (4) interlinking and enrichment of the entities. Though the resulting enriched metadata can not always be given back to the data provider's in-house database, this workflow includes the creation of best practice documents for the performing arts GLAM community based on the results of the analysis. We will focus on the impact of the data's heterogeneity on the workflow by describing the different, possibly data provider-specific, stages of the process and their limitations.'",SWIB,,,,,,,,,,,
,2019,"Hands-on IIIF: how to install, configure and prepare simple IIIF services",Leander Seige,"SWIB/SWIB2019/Hands-on IIIF how to install, configure and prepare simple IIIF services.md","#swib/2019,#IIIF,#API","In the last years, the International Image Interoperability Framework has developed into a widespread standard for making digital images of cultural objects available on the basis of Linked Open Data principles. The aim of this hands-on workshop is to walk the participants through a complete installation of a IIIF server in a virtual machine to make images available according to the IIIF Image API and deliver the corresponding metadata according to the IIIF Presentation API. Participants can bring up to ten own high-resolution images of their choice including simple metadata to convert them into the appropriate IIIF formats during the workshop. The organizers will provide access to centrally hosted virtual machines for the participants. The hands-on session starts with a ready-made installation of a basic Linux operating system. During the workshop all necessary server components will be installed and configured by the participants. This includes the configuration of web- and proxyservers, HTTPS and CORS. The provided images will be converted into tiled pyramidal images. IIIF manifests and collection files will be generated. Each step will be explained in detail and the participants will receive support in case of technical difficulties. Finally, participants will install IIIF viewers on their virtual machines in order to view IIIF images. It will be possible to access each others IIIF server via HTTPS in order to demonstrate the advantages of IIIF’s interoperability. Participants should have basic knowledge of Linux, the shell and SSH. The images should be available under a Creative Commons license or similar free conditions. Participants should bring their own laptop with an SSH client preinstalled.",SWIB,,,,,,,,,,,
,2019,In and out: workflows between library data and linked-data at the National Library of Spain,Ricardo Santos,SWIB/SWIB2019/In and out workflows between library data and linked-data at the National Library of Spain.md,"#swib/2019,#datos,#linked-data,#FRBR,#Wikidata,#authority/records,#VIAF,#API","Datos.bne.es is the linked-data based catalogue at the National Library of Spain. It is built upon the MARC21 library records as the main source of data, and transformed into RDF through a pipeline of analysis, defragmentation, and data re-clustering into the FRBR-based data model. On top of this, a new entity-driven “catalogue” is built for the use of general public and for enhanced discovery from search engines. Datos.bne is an experimental development, making room for testing unconventional workflows for metadata production and creation, breaking up the sometimes rigid conventions taking place in national libraries. This presentation will explore some of the most prominent features in this workflows, and discuss pros and cons found along the way, or how this may influence library metadata production in the near future. One of the most recent features has been the massive data ingestion from Wikidata into 80.000-odd person library records, including properties as gender, birth place, occupation, language, field of work or membership. The matching between library records and Wikidata records was made based on Wikidata identifiers present on authority records. These identifiers were initially extracted from VIAF identifiers datadump file and loaded into the library authority records. After that initial massive load, catalogers have been routinely adding more wikidata URIs when available. After a file was extracted from the library authority file, containing BNE Ids and Wikidata IDs, the library technological partner for this venture got the data through the Wikidata API. After extensive quality check-up, massive modifications, and alignment with the library subject terms, the data were succesfully loaded into the library records, and will be processed for its use in the datos platform, closing the feedback circle.",SWIB,,,,,,,,,,,
,2019,Introduction to Jupyter Notebooks,"Magnus Pfeffer,Kai Eckert",SWIB/SWIB2019/Introduction to Jupyter Notebooks.md,"#swib/2019,#Jupyter-Notebook,#workshop,#IPython","Jupyter Notebook is an open source web application for creating and sharing “live documents” that can contain code and the results from its execution besides traditional document elements like text or images. Originally being developed as part of the IPython project, it is now independent of Python and supports a long list of different programming languages, including JavaScript, Ruby, R and Perl. These live documents are uniquely suited to create teaching materials and interactive manuals that allow the reader to make changes to program code and see the results within the same environment: program outputs can be displayed, visualisation graphics or data tables can be updated on-the-fly. To support traditional use cases, static non-interactive versions can be exported in PDF, HTML or LaTeX format. For data practitioners, Jupyter Notebooks are ideal to perform data analyses or transformations, e.g., to generate Linked Open Data, where the workflow documentation is part of the implementation. Single lines of code can be added or changed and then executed without losing the results of prior parts of the code. Visualizations can be generated in code and are directly embedded in the document. This makes prototyping and experimenting highly efficient and actually a lot of fun. Finally, Jupyter Notebooks are an ideal platform for beginners, as they can execute code line by line and immediately see how changes affect the result. This workshop requires no prior knowledge of Jupyter Notebooks or the Python programming language; only basic programming and HTML/Markdown knowledge is required. Please bring your own laptop. Agenda: Part I: Introduction (Local installation of the necessary programming environment ++ Using existing documents ++ Creating documents with rich content ++ Notebook extensions) Part II: Case studies (Using Jupyter Notebook in teaching data integration basics ++ Using Jupyter Notebook to develop, test and document a data management workflow with generation of RDF) Part III: Advanced topics (Server installation and use ++ Version control ++ Using different language kernels)",SWIB,,,,,,,,,,,
,2019,Introduction to OpenRefine,"Owen Stephens,Felix Lohmeier",SWIB/SWIB2019/Introduction to OpenRefine.md,"#swib/2019,#OpenRefine,#workshop","'This workshop will introduce the OpenRefine software to participants. The purpose and functionality of OpenRefine will be introduced and participants will use OpenRefine through a range of hands-on exercises to examine, clean, link and publish a data set.'",SWIB,,,,,,,,,,,
,2019,Lessons from representing library metadata in OCLC research’s Linked Data Wikibase prototype,Karen Smith-Yoshimura,SWIB/SWIB2019/Lessons from representing library metadata in OCLC research’s Linked Data Wikibase prototype.md,"#swib/2019,#OCLC,#Wikibase,#Passage,#project,#reconcile,#bibliographic/data,#authority/data","This presentation highlights key lessons from OCLC Research’s Linked Data Wikibase Prototype (“Project Passage”), a 10-month pilot done in 2018 in collaboration with metadata specialists in 16 U.S. libraries. Our Wikibase prototype provided a framework to reconcile, create, and manage bibliographic and authority data as linked data entities and relationships. We chose to install a local Wikibase instance for its built-in features and generating entities in RDF without requiring technical knowledge of linked data; we could focus on what participants needed beyond the initial set of capabilities. It served as a “sandbox” for participants to experiment with describing library and archival resources in a linked data environment. Participants showcased “use cases”: non-English descriptions, visual resources, archives, a musical work, and events. Among the lessons learned:",SWIB,,,,,,,,,,,
,2019,NAISC: an authoritative Linked Data interlinking approach for the library domain,"Lucy McKenna,Christophe Debruyne,Declan O'Sullivan",SWIB/SWIB2019/NAISC an authoritative Linked Data interlinking approach for the library domain.md,"#swib/2019,#librarians,#RDF,#ontologies,#PROV-O","At SWIB 2018, we presented our early stage work on a Linked Data (LD) interlinking approach for the library domain called NAISC – Novel Authoritative Interlinking of Schema and Concepts. The aim of NAISC is to meet the unique interlinking requirements of the library domain and to improve LD accessibility for domain expert users. At SWIB 2019 we will present our progress in the development of NAISC including an improved graphical user-interface (GUI), user testing results, and a demonstration of NAISC’s interlink provenance components. NAISC consists of an Interlinking Framework, a Provenance Model and a GUI. The Framework describes the steps of entity selection, link-type selection, and RDF generation for the creation of interlinks between entities, such as people, places, or works, stored in a library dataset to related entities held in another institution. NAISC specifically targets librarians by providing access to commonly used datasets and ontologies. NAISC incudes interlink provenance to allow data users to assess the authoritativeness of each link generated. Our provenance model adopts PROV-O as the underlying ontology which we extended to provide interlink specific data. An instantiation of NAISC is provided through a GUI which reduces the need for expert LD knowledge by guiding users in choosing suitable link-types. We will present NAISC and demonstrate the use of our GUI as a means of interlinking LD entities across libraries and other authoritative datasets. We will also discuss our user-evaluation processes and results, including a NAISC usability test, a field test/real-word application of NAISC, and a review of the interlink quality. Finally, we will demonstrate our provenance model and discuss how the provenance data could be modelled as LD develops over time.",SWIB,,,,,,,,,,,
,2019,Proposing rich views of linked open data sets : the S-paths prototype and the visualization of FRBRized data in data.bnf.fr,"Raphaëlle Lapôtre,Marie Destandau,Emmanuel Pietriga",SWIB/SWIB2019/Proposing rich views of linked open data sets  the S-paths prototype and the visualization of FRBRized data in data.bnf.fr.md,"#swib/2019,#BnF,#open/data,#SPARQL,#FRBR","During the year 2011, the National Library of France (BnF) launched its open data service, the data.bnf.fr project. Alongside a SPARQL endpoint providing a dynamic query service, an interface also aims at directing users among the BnF collections thanks to FRBRized metadata. In 2017, the idea of proposing a visual recommendation system based on Linked Open Data technologies emerged from discussions with data.bnf.fr users. As this idea implied visualizing large and complex datasets, the data.bnf.fr team decided to partner with the Human Computer Interaction research team ILDA (Interacting with Large Data). This collaboration contributed to the design of S-paths, an interactive data visualization interface that aims at providing meaningful insights about Linked Datasets. S-Paths allows users to navigate linked open data more intuitively by systematically presenting the most readable view for a given set or subset of similar entities. The main obstacles encountered during this experiment include the heterogeneity of the data, which challenged the system’s selection algorithm, as well as performance issues when querying SPARQL endpoints, partly related to the complexity of the FRBR model. Despite those difficulties, S-paths proved very useful to reveal defects in data sources, visualize modeling specificities, and show trends in the data that can be used for communication towards end users.",SWIB,,,,,,,,,,,
,2019,Publishing Linked Data on Data.Bibliotheken.nl,René Voorburg,SWIB/SWIB2019/Publishing Linked Data on Data.Bibliotheken.nl.md,"#swib/2019,#KB,#DBN,#Bibliotheken,#RDF,#IFLA-LRM,#PREMIS","'In 2018, KB - National Library of the Netherlands (KB), published Data.Bibliotheken.nl (DBN), an online linked data publication environment. It was argued that linked data would help the KB to publish not just 'on the web' but also 'in the web', allowing others to reuse and link to KB data. Although a growing interest exists in extending linked data principles to core library registries, for example the catalogue, the practice of publishing linked data at the KB involved following a cascade of export, modelling or remodelling, conversion, selection and transformation steps. Various conversion routes were followed, depending on the data, systems and tools at hand. Presented will be the steps that were applied, leading to the RDF published at DBN. Besides these technical steps, perhaps most crucial in creating linked data is the semantic modelling or the design principles to be adhered to. It is crucial since the model applied may greatly impact the usability of the output. Moreover, it complex since it requires people with differing backgrounds to collaborate, to understand and acknowledge each others expertise and perspectives and to arrive at a common language and model. This complex step hasn't reached it conclusion yet. The design principles behind the data now at DBN may be summarized as generally established best practices plus a schema.org serialization. To further enhance and extend the published data, a more elaborate set of principles is required. Those principles will not to be applied to data at DBN only. As a 'generic KB entity model for content' it is thought of as the unifying metadata model for KB content related metadata. This work-in-progress model, based on IFLA-LRM and PREMIS, will be presented. Regardless whether DBN has fulfilled its goals or not, it appears that embarking on the linked data trail has resulted in beneficial spin off.''",SWIB,,,,,,,,,,,
,2019,SkoHub: KOS-based content syndication with ActivityPub,"Adrian Pohl,Felix Ostrowski",SWIB/SWIB2019/SkoHub KOS-based content syndication with ActivityPub.md,"#swib/2019,#OER,#SkoHub,#KOS,#hbz,#graphthinking","For a long time, openness movements and initiatives with labels like “Open Access”, “Open Educational Resources” (OER) or “Linked Science” have been working on establishing a culture where scientific or educational resources are by default published with an open license on the web to be read, used, remixed and shared by anybody. With a growing supply of resources on the web, the challenge grows to learn about or find resources relevant for teaching, studies, or research. Current approaches provided by libraries for publishing and finding open content on the web are often focused on repositories as the place to publish content. Those repositories provide (ideally standardized) interfaces for crawlers to collect and index the metadata in order to offer search solutions on top. Besides being error-prone and requiring resources for keeping up with changes in the repositories, this approach also does not take into account how web standards work. In short, the repository metaphor guiding this practice obscures what constitutes the web: resources that are identified by HTTP URIs. In this presentation, we describe the SkoHub project being carried out in 2019 by the hbz in cooperation with graphthinking GmbH. The project seeks to implement a prototype for a novel approach in syndicating content on the web. In order to do so, we make use of the decentralized social web protocol ActivityPub to build an infrastructure where services can send and subscribe to notifications for subjects defined in knowledge organization systems (KOS, sometimes also called “controlled vocabularies”). While the repository-centric approach favours content deposited in a repository that provides interfaces for harvesting, with SkoHub any web resource can make use of the notification mechanism.",SWIB,,,,,,,,,,,
,2019,Smart Data for Digital Humanities,Marcia Zeng,SWIB/SWIB2019/Smart Data for Digital Humanities.md,"#swib/2019,#smart-data,#digital-humanities","Smart data, a concept aligned with big data, can be simply explained as making sense out of big data, or, turning big data into actionable data. While the many “V”s of big data (volume, velocity, variety, variability, veracity) are crucial, the “V”alue of such data relies on the ability to achieve big insights from smart data—the trusted, contextualized, relevant, cognitive, predictive, and consumable data at any scale. The smart data concept is essential in relation to the role of libraries, archives, and museums (LAMS) in supporting Digital Humanities (DH) research. Rapid development of the digital humanities field creates a demand for bigger and smarter historical and cultural heritage data carried by information-bearing objects (textual or non-textual, digitized or non-digitized), which would typically not be obtainable through web crawling, scraping, real-time streams, mobile-analytics, or agile development methods. Increased funding for research in DH and innovative semantic technologies have enabled LAMs to advance their data into smart data, thus supporting deeper and wider exploration and use of data in DH research. In this talk, the speaker shares an understanding of the what, why, how, who, where, and which data, in relation to smart data and digital humanities. Particular attention is given to the distinctive roles big data and smart data play in the humanities, which is seemingly marked by a methodological shift rather than a primarily technological one. It is the speaker’s belief that smart data will have extraordinary value in digital humanities.",SWIB,,,,,,,,,,,
,2019,Target vocabulary maps,Niklas Lindström,SWIB/SWIB2019/Target vocabulary maps.md,"#swib/2019,#Sweden,#union-catalogue,#RDFS,#OWL,#reasoners,#vocabulary,#XSLT,#SPARQL,#vocabulary/map,#shortcomings","'The union catalogue of the National Library of Sweden now has a core based on linked data structures. In the course of our continued development we have begun exploring the feasibility of semantic technologies (RDFS and OWL) for more wide and open-ended data integration. OWL inferencing is sometimes considered a foundational mechanism for automatic semantic interoperability. But we need to go about it differently in order for semantic mappings to become a practical tool for effective data integration. OWL reasoners are rarely used when automating ingestion or publication of linked data. Reasoners aren't readily targeted at specific applications, but expand all possible implications of a set of statements, yielding rather unwieldy data. Meanwhile the web of data at large continues to grow, and between organizations more and more integration needs crop up. These needs commonly have to be solved right now, thus requiring us to write up more custom integration code, with little reuse even within one organization. It is not uncommon to device custom mappings between RDF vocabularies as a part of these often complex ETL pipelines. Sometimes, these use SPARQL, sometimes XSLT, sometimes custom code with various non-portable dependencies. We're exploring an arguably simpler approach to address a given set of described use cases. It is based on preprocessing of vocabulary data, scanning their mappings and creating a target map from each known property and class to a predefined selection of desired target properties and classes. This computed ""target vocabulary map"" is then used when reading input data using the known terms, to produce the selected target description. This yields more predictive results tailored for conceived use cases. This presentation will elaborate on these considerations and explore the proposed solution, including limitations and possible shortcomings.'",SWIB,,,,,,,,,,,
,2020,Annif and Finto AI: DIY automated subject indexing from prototype to production,"Osma Suominen,Mona Lehtinen,Juho Inkinen",SWIB/SWIB2020/Annif and Finto AI DIY automated subject indexing from prototype to production.md,"#swib/2020,#Annif,#Finto,#subject-indexing,#REST,#API,#thesaurus","The first prototype of Annif (annif.org), the multilingual automated subject indexing tool, was created at the National Library of Finland in early 2017. Since then, the open source tool has grown from an experiment into a production system. Through its REST API it has been integrated, into the document repositories of several university libraries, the metadata workflows of the book distributor Kirjavälitys Oy that serves publishers, bookshops, libraries and schools, into the Dissemin service for publishing academic papers in open repositories, and into the automated subject indexing service Finto AI (ai.finto.fi) that was launched in May 2020 as a companion to the Finto thesaurus and ontology service. In the meantime, we have organized workshops and tutorials around Annif and automated indexing as well as grown an international community of users and developers. This presentation looks at the current state of Annif, the lessons learned during its development, how it has been received by different communities and what the next steps will look like.",SWIB,,,,,,,,,,,
,2020,AutoSE@ZBW: Building a productive system for automated subject indexing at a scientific library,"Anna Kasprzik,Moritz Fürneisen,Christopher Bartz",SWIB/SWIB2020/AutoSE@ZBW Building a productive system for automated subject indexing at a scientific library.md,"#swib/2020,#ZBW,#machine-learning,#automated-subject-indexing,#indexing,#metadata,#API,#statistics,#neural-networks","At ZBW we have been developing prototype machine learning solutions for an automated subject indexing in the context of applied research for several years now. However, and as of 2019 these solutions were yet to be integrated into the metadata management system and into the subject indexing workflows at ZBW. It turns out that building a corresponding software architecture is a challenge on another level which requires additional resources on top of those for academic research as well as additional expertise. In order to create a productive system that makes these machine learning solutions usable in practice and that allows a continuous development we need to look at aspects such as user and data interfaces, suitable development and test environments, system stability, modularity and continuous integration. After a strategic reorientation and preparation phase in 2019, in 2020 we have created a first proof-of-concept version of a productive system that suggests subject terms for resources from our holdings and which makes these suggestions available via an API for example to the DA-3: a tool for assisted subject indexing based on suggestions from external sources that is currently evaluated in our library network. In parallel, we have been experimenting with more advanced statistical algorithms as well as with neural networks. Our software architecture is supposed to allow the integration of new methods as smoothly as possible without interruptions to the service. This presentation sums up our first steps towards a productive system, first lessons learned, and projects some milestones for the way ahead.",SWIB,,,,,,,,,,,
,2020,Automated subject indexing with Annif,"Osma Suominen,Mona Lehtinen,Juho Inkinen,Anna Kasprzik,Moritz Fürneisen",SWIB/SWIB2020/Automated subject indexing with Annif.md,"#swib/2020,#Annif,#metadata,#REST,#API,#workshop,#vocabularies","'Due to the proliferation of digital publications, intellectual subject indexing of every single literature resource in institutions such as libraries is no longer possible. For the task of providing subject-based access to information resources of different kinds and with varying amounts of available metadata, it has become necessary to explore possibilities of automation. In this hands-on tutorial, participants will be introduced to the multilingual automated subject indexing tool Annif (annif.org) as a potential component in a library’s metadata generation system. By completing exercises, participants will get practical experience on setting up Annif, training algorithms using example data, and using Annif to produce subject suggestions for new documents using the command line interface, the web user interface and REST API provided by the tool. The tutorial will also introduce the corpus formats supported by Annif so that participants will be able to apply the tool to their own vocabularies and documents. The tutorial will be organized using the flipped classroom approach: participants are provided with a set of instructional videos and written exercises, and are expected to attempt to complete them on their own time before the tutorial event, starting at least a week in advance. The actual event will be dedicated to solving problems, asking questions and getting a feeling of the community around Annif. Participants are instructed to use a computer with at least 8GB of RAM and at least 20 GB free disk space to complete the exercises. The organizers will provide the software as a preconfigured VirtualBox virtual machine. Alternatively, Docker images and a native Linux install option are provided for users familiar with those environments. No prior experience with the Annif tool is required, but participants are expected to be familiar with subject vocabularies (e.g. thesauri, subject headings or classification systems) and subject metadata that reference those vocabularies. The workshop Automated subject indexing with Annif is not recorded. Workshop material with step by step walkthroughs and video recordings for self-learning is available online. For more information on Annif, see also the Annif homepage and wiki. Last but not least don't hesitate contacting us e.g. via the Annif user group.'",SWIB,,,,,,,,,,,
,2020,Automated tools for propagating a common hierarchy from a set of vocabularies,Joeli Takala,SWIB/SWIB2020/Automated tools for propagating a common hierarchy from a set of vocabularies.md,"#swib/2020,#Finto,#controlled-vocabularies,#indexing,#subject indexing,#SKOS,#YSO,#vocabulary,#KOKO","Through finto.fi the National Library of Finland publishes controlled vocabularies for subject indexing and linking data. Linked Open Data formats such as SKOS, also enable us to combine several vocabularies into a common repository of concepts from various sources. The purpose is to expand one general-purpose vocabulary with others of more specific fields of knowledge in a way that enables us to cover a wider context with one vocabulary. The problem is in assessing and ensuring the interoperability of each vocabulary when used in this manner. The combined data set consists of the Finnish General Upper Ontology (YSO) and fifteen domain-specific controlled vocabularies published in SKOS format. Each vocabulary broadly follows the same data model, with each sharing the upper hierarchy of YSO. In total this amounts to 2.1M triples which are combined into 57k unique concepts and 246k labels in a single common vocabulary, KOKO. The tool used for creating the combined vocabulary is a twelve-step algorithm which is combined with the tools for change-tracking and automated quality assessment in order to publish a common vocabulary of consistent quality. The difficulties arise from recognising the common error types in the data structure of a single vocabulary which would not look alarming on their own, but would create complex dynamics if the vocabularies were combined and different error types cascaded on top of each other. The end result may seem confusing for a user and should be avoided whenever possible. Apart from assessing whether such mistakes exist in the data, we also need to address data synchronisation problems when concepts from one vocabulary are shifted in the hierarchy, removed altogether or split into several new concepts of similar meaning. To achieve this, the update process of each of the vocabularies is synchronised with the updates of the YSO’s hierarchy.",SWIB,,,,,,,,,,,
,2020,Automatic indexing of institutional repository content using SKOS,Ricardo Eito-Brun,SWIB/SWIB2020/Automatic indexing of institutional repository content using SKOS.md,"#swib/2020,#indexing,#SKOS,#UNESCO,#thesaurus,#PoolParty,#descriptors,#thesaurus/descriptors","The lack of well-defined indexing practices is a common problem in most institutional repositories. Researchers typically assign keywords to their submissions, these terms, however are not extracted from a controlled vocabulary or thesaurus. This leads to ambiguity and lack of specificity in the terms which are used to describe the content of their contributions. This presentation describes an approach used to complete the automatic assignment of descriptors and keywords taken from a thesaurus (the UNESCO thesaurus) to contributions which had already been published. The process is run with the help of an existing commercial tool, PoolParty. The experiment runs a process to automatically identify the thesaurus descriptors that describe the content of the documents published in the institutional repository of a Spanish university. Once the thesaurus descriptors are assigned to the existing documents, it is feasible to use the theasurus to expand queries and to assist end-users in the selection of search terms. This brings about improved search capabilities, as users are in a position to identify both, additional related terms and more general and specific terms in order to improve their search queries.''",SWIB,,,,,,,,,,,
,2020,BIBFRAME instance mining: Toward authoritative publisher entities using association rules,Jim Hahn,SWIB/SWIB2020/BIBFRAME instance mining Toward authoritative publisher entities using association rules.md,"#swib/2020,#Share-VDE,#project,#enrichment,#BIBFRAME,#data/mining,#machine-learning,#ISBN,#MARC","The catalyst for this talk stems from work within the Share-VDE initiative, a shared discovery environment based on linked data. The project encompasses enrichment with linked open data and subsequent conversion from MARC to BIBFRAME/RDF and creation of a cluster knowledge base made up of over 400 million triples. The resulting BIBFRAME network is comprised of the BIBFRAME entities Work and Instance, among other Share-VDE specific entities. With the transition of a shared catalog to BIBFRAME linked data, there is now a pressing need for identifying the canonical Instance for clustering in BIBFRAME. A fundamental component of Instance identification is by way of authoritative publisher entities. Previous work in this area by OCLC research (Connaway & Dickey, 2011) proposed a data mining approach for developing an experimental Publisher Name Authority File (PNAF). The OCLC research was able to create profiles for ""high-incidence"" publishers after data mining and clustering of publishers. As a component of PNAF, Connaway & Dickney were able to provide detailed subject analysis of publishers. This presentation will detail a case study of machine learning methods over a corpus of subjects, main entries, and added entries, as antecedents into association rules to derive consequent publisher entities. The departure point for the present research into identification of authoritative publisher entities is to focus on clustering, reconciliation and re-use of ISBN and subfield b of MARC 260 along with the subjects (650 - Subject Added Entry), main entries (1XX - Main Entries) and added entries (710 - Added Entry-Corporate Name) as signals to inform a training corpus into association rule mining, among other machine learning algorithms, libraries, and methods.",SWIB,,,,,,,,,,,
,2020,Cataloging rare books as linked data: a use case,"Paloma Graciani-Picardo,Brittney Washington",SWIB/SWIB2020/Cataloging rare books as linked data a use case.md,"#swib/2020,#LD4P2,#ontologies,#BIBFRAME,#workflows,#self-training,#controlled/vocabularies","Linked Data for Production phase 2 (LD4P2), a two-year pilot supported by the Andrew W. Mellon Foundation wrapped up in May 2020. As a member of the LD4P2 cohort, the Harry Ransom Center is eager to share with the community some of our activities within the project and lessons learnt. Application profiles have been at the core of LD4P2 activities and the Ransom Center has supported this effort with the evaluation of ontologies, models, vocabularies and best practices for item-level description of rare and special collection materials in a linked data collaborative environment. In this presentation, we will discuss our work analyzing MARC to BIBFRAME conversion, defining local workflows for linked data cataloguing and self-training strategies, and developing an application profile for special collections materials. We will do a quick review of the existing ontologies and controlled vocabularies relevant to the project, and present data modeling approaches and challenges. Finally, but no less important, we will emphasize the value of special collections community engagement in these types of projects and the need for continued collaboration beyond the grant.",SWIB,,,,,,,,,,,
,2020,Changing the tires while driving the car: A pragmatic approach to implementing linked data,"David Seubert,Shawn Averkamp,Michael Lashutka",SWIB/SWIB2020/Changing the tires while driving the car A pragmatic approach to implementing linked data.md,"#swib/2020,#DAHR,#78rpm,#enriching,#data/mining","The Discography of American Historical Recordings (DAHR) is an online database of sound recordings made by American record companies during the 78rpm era. Based at the University of California, Santa Barbara, DAHR now includes authoritative information on over 300,000 master recordings by over 60,000 artists and has 40,000 streaming audio files online. To provide even more context for researchers using the database, DAHR editors chose to use linked data to enrich the database with information from other open data sources. With funding from the Library of Congress National Recording Preservation Board, UCSB engaged consultants at AVP in the development of a strategy for enriching DAHR by mining public data. After the harvesting and integration of data for over 18,000 names from Library of Congress Name Authority File, Wikidata, and MusicBrainz, users can now find Wikipedia biographies, photographs, and links to additional content at many other databases, such as LP reissues in Discogs, record reviews on Allmusic, or streaming audio on Spotify as well as links to names in other authority files like VIAF. In this presentation, we will share our process of harvesting data, retrofitting DAHR’s underlying FileMaker Pro data model and workflows to accommodate the addition of this new data and the minting of URIs, and leveraging the unique circumstances of the COVID-19 outbreak to redirect staff time towards quality control of this new data. We will also discuss current efforts to populate Wikidata and MusicBrainz with our newly minted URIs to provide broader entry and visibility to the DAHR database.",SWIB,,,,,,,,,,,
,2020,Developing BIBFRAME application profiles for a cataloging community,"Paloma Graciani-Picardo,Nancy Lorimer,Christine DeZelar-Tiedman,Nancy Fallgren,Steven Folsom,Jodi Williamschen",SWIB/SWIB2020/Developing BIBFRAME application profiles for a cataloging community.md,"#swib/2020,#BIBFRAME,#ontology,#RDA,#PCC,#Sinopia,#modeling,#challenges,#reconciling","As libraries experiment with integrating BIBFRAME (BF) data into library workflows and applications, it is increasingly clear that there is little to no formal agreement on what a baseline BF description might be, and even how specific properties are modeled in what is a very flexible ontology. This basic agreement is imperative, at least in these early days, for data producers and developers in building out and implementing practical workflows and viable interactions among disparate data sources; the more flavors that need to be dealt with, the more difficult initial implementation will be. Additionally, there is little consensus on how to integrate BF and RDA, our primary cataloging standard, and it is difficult to move ahead without a basic mapping. The Program for Cooperative Cataloging (PCC), with its close connection to the Library of Congress and the Linked Data for Production grants, and its focus on standards-building in the MARC cataloging community, is well set up to develop standards and become a steward of well-formed BF. To that end, the PCC Sinopia Application Profiles Task Group is working on developing BF application profiles through creating PCC templates in Sinopia, the linked data editor developed in Linked Data for Production 2 grant, to serve as the basis for metadata creation by the PCC community. In this talk, we discuss the community process and challenges encountered in creating applications profiles through template development, including modeling questions, technical challenges, and reconciling BF with RDA and PCC standards.",SWIB,,,,,,,,,,,
,2020,Generating metadata subject labels with Doc2Vec and DBPedia,Charlie Harper,SWIB/SWIB2020/Generating metadata subject labels with Doc2Vec and DBPedia.md,"#swib/2020,#metadata,#unsupervised learning,#clusters,#labeling,#Doc2Vec,#DBPedia,#corpora,#unlabeled/clustering,#metadata-generation","Previous approaches to metadata creation using unsupervised learning have often centered on generating document clusters, which then require manual labeling. Common approaches, such as topic modelling with Latent Dirichlet Allocation, are also limited by the need to determine the number of clusters prior to training. While this is useful for finding underlying relationships in corpora, unlabeled clustering does not provide an ideal way to generate metadata. In this presentation, I examine one way that unsupervised machine learning and linked data can be employed to generate rich metadata labels for textual resources and thereby improve resource discovery. To generate document-specific metadata, I build high-dimensional vectors using the doc2vec algorithm on DBPedia entries. DBPedia regularly collects millions of pages from wikipedia, including a unique label, an abstract, and extensive information on the semantic links between pages. While abstracts and labels can be extremely specific and are unlikely to provide broadly usable metadata tags, the linked nature of this dataset provides a valuable way to improve this. Using predicates like dct:subject and skos:broader, page vectors can be averaged together to encode higher conceptual levels of information that are labelled by a unique subject or idea. By then vectorizing an unseen document’s abstract, a k-d search tree can quickly locate the nearest subjects in vector space and suggest what labels should be assigned to a document. To explore its efficacy, this tagging approach is applied to a corpus of dissertations and theses published at Ohio universities and colleges. Methods for visualizing the tagged corpus are finally explored to determine if the linked nature of the subject tags may allow users to visually discover related texts in a more natural, less constrained way.",SWIB,,,,,,,,,,,
,2020,Integration and organization of knowledge in a Current Research Information System (CRIS) based on semantic technologies,"Ana Maria Fermoso García,Maria Isabel Manzano García,Julian Porras Reyes,Juan Blanco Castro",SWIB/SWIB2020/Integration and organization of knowledge in a Current Research Information System (CRIS) based on semantic technologies.md,"#swib/2020,#CRIS,#OpenUPSA,#ontology,#LOD","We present OpenUPSA, a system based, inter alia, on semantic technologies. It is a project developed in collaboration with the university library, and whose goal is to share with society information about research at the university, about its agents and its scientific production. The result is a software system that can be regarded as a Current Research Information System (CRIS). This system, however, can be considered as an advanced CRIS. It does not only visualize information about university research including its researchers, its groups of research and scientific production (projects, publications, academic works or patents), but also provides other progressive features. The first feature allows to integrate information from a variety of sources where nowadays information about research is provided, and to even enrich information from external sources by linking for instance a publication with its information in a database like Scopus. This is mainly achieved by implementing analyzers for the different formats that have to be integrated in a common relational database format. Besides, after integration information can be also managed. The second feature is the possibility to perform system-specific queries and to view and download the results obtained in different formats. This is possible thanks to the use of semantic technologies, particularly due to the use of an ontology as semantic data model and a SPARQLPoint service. The ontology allows research authorities and knowledge to be organized and shared with the community, and even to enrich this knowledge from external sources as a LOD system. The ontology is new, but based on the CERIF (Common European Research Information Format) ontology, the European standard for CRIS systems. Thus it facilitates the internationalization of our proposal and sharing our data with other systems.",SWIB,,,,,,,,,,,
,2020,Linked data for opening up discovery avenues in library catalogs,Huda Khan,SWIB/SWIB2020/Linked data for opening up discovery avenues in library catalogs.md,"#swib/2020,#LD4P2,#Blacklight,#LD4P3,#dbpedia,#JSON-LD","Exploring the integration of linked data sources into library discovery interfaces was an important goal for the recently concluded Linked Data For Production: Pathway to Implementation ( LD4P2) grant. We conducted a series of focused experiments involving user studies and the implementation of Blacklight-based prototypes. In this presentation, we will provide an overview of lessons learned through these experiments as well as subsequent discovery research as part of the ongoing Linked Data for Production: Closing the Loop (LD4P3) LD4P3 grant. Examples of areas we investigated for the integration of linked data include: knowledge panels bringing in contextual information and relationships from knowledge graphs like Wikidata to describe people and subjects related to library resources in the catalog; suggested searches based on user-entered queries using results from Wikidata and DbPedia; browsing experiences for subjects and authors bringing in relationships and data from Wikidata and library authorities; and autosuggest for entities represented in the catalog using supplementary information from FAST, the Library of Congress authorities, and Wikidata. Grant work also supported the development of Blacklight functionality for embedding Schema.org JSON-LD representation of some catalog metadata. We will also review opportunities for the larger community to engage in discussions around use cases and implementation techniques for using linked data in discovery systems.",SWIB,,,,,,,,,,,
,2020,Making use of the coli-conc infrastructure for controlled vocabularies,"Jakob Voss,Stefan Peters",SWIB/SWIB2020/Making use of the coli-conc infrastructure for controlled vocabularies.md,"#swib/2020,#coli-conc,#Cocoda,#JSKOS,#API,#cataloging,#controlled-vocabularies","'Project coli-conc has created an infrastructure to facilitate management and exchange of concordances between library knowledge organization systems. The most visible outcome of the project is Cocoda, a web application that simplifies the creation and evaluation of mappings between concepts from different classifications, thesauri, and other controlled vocabularies. This tutorial will give an introduction to the infrastructure that allows to work with controlled vocabularies from diverse sources. After a brief introduction to the architecture, the data format JSKOS, its API, and utility node packages, we will live code a small cataloging application for semantic tagging of resources with concepts from controlled vocabularies. Active participation requires basic knowledge of JavaScript and HTML. The Workshop Making use of the coli-conc infrastructure for controlled vocabularies is not recorded. Workshop material and slides for self-learning are available. The coli-conc homepage also provides pointers to screencasts, documents, and source code. Last but not least don't hesitate contacting us!''",SWIB,,,,,,,,,,,
,2020,Managing and Preserving Linked Data with Fedora,David Wilcox,SWIB/SWIB2020/Managing and Preserving Linked Data with Fedora.md,"#swib/2020,#Fedora,#digital/preservation,#workshop","'Fedora is a flexible, extensible, open source repository platform for managing, preserving, and providing access to digital content. Fedora is used in a wide variety of institutions including libraries, museums, archives, and government organizations. For the past several years the Fedora community has prioritized alignment with linked data best practices and modern web standards. We are now shifting our attention back to Fedora's digital preservation roots with a focus on durability and the Oxford Common File Layout (OCFL). This workshop will provide an introduction to the latest version of Fedora with a focus on both the linked data and digital preservation capabilities. Both new and existing Fedora users will be interested in learning about and experiencing Fedora features first-hand. Attendees will be given access to individual cloud-hosted Fedora instances to use during the workshop. These instances will be used to participate in hands-on exercises that will give attendees a chance to experience Fedora by following step-by-step instructions.'",SWIB,,,,,,,,,,,
,2020,ORCID for Wikidata: A workflow for matching author and publication items in Wikidata,Eva Seidlmayer,SWIB/SWIB2020/ORCID for Wikidata A workflow for matching author and publication items in Wikidata.md,"#swib/2020,#ORCID,#Wikidata,#bibliometric,#Wikidata","In the context of a bibliometric project, we retrieved social context information on authors of scientific publications from Wikidata in order to import it into the metadata of our dataset. While we were able to capture about 95% of the requested scholarly publications in Wikidata, only 3% of the authors could be assigned and used for retrieval of social context information. One reason probably was that authors in general are rarely curated in Wikidata. Whilst research papers account for 31.5% of the items, it is only 8.9% which represent humans in general, not even researchers in particular (according to Wikidata statistics, January 2020). Another reason we observed is the frequent absence of relations between the Wikidata item of a publication and the Wikidata item of the author(s), although the author is already listed. To fill the gap and in order to improve the foundation for bibliographic analysis in general, we established a workflow for matching authors and scholarly articles by making use of the ORCID (Open Researcher and Contributor ID) database. The presentation will demonstrate how we harvest information on publications and their authors from ORCID, how we query Wikidata for existing items that are also listed in ORCID, and how we perform the matching. Finally, it will be illustrated how the initial research project benefits from the presented enrichment of bibliometric details in Wikidata.",SWIB,,,,,,,,,,,
,2020,Open Data & Social Innovation: Experiences from Taiwan,Audrey Tang,SWIB/SWIB2020/Open Data & Social Innovation - Experiences from Taiwan.md,"#swib/2020,#taiwan,#iot","When we see “internet of things”, let’s make it an internet of beings. When we see “virtual reality”, let’s make it a shared reality. When we see “machine learning”, let’s make it collaborative learning. When we see “user experience”, let’s make it about human experience. When we hear “the singularity is near”, let us remember: the Plurality is here.",SWIB,,,,,,,,,,,
,2020,Sinopia Linked Data Editor,Jeremy Nelson,SWIB/SWIB2020/Sinopia Linked Data Editor.md,"#swib/2020,#Sinopia,#BIBFRAME,#Share-VDE,#machine-learning","'The Sinopia Linked Data environment is a Mellon foundation funded project that provides catalogers a native Linked-Data editor with a focus on the BIBFRAME ontology for describing resources. Following an iterative Agle development process, Sinopia is currently in it's third version with improved user interface and third-party integrations based on continuous feedback from an international cohort of users. This presentation will start off with a high-level introduction of Sinopia, followed by cataloger data workflows using external authority sources like Library of Congress and Share-VDE and supporting technologies, and finishing with the new and planned features, including machine learning RDF classification, in the upcoming year'.",SWIB,,,,,,,,,,,
,2020,Using IIIF and Wikibase to syndicate and share cultural heritage material on the Web,Jeff Keith Mixter,SWIB/SWIB2020/Using IIIF and Wikibase to syndicate and share cultural heritage material on the Web.md,"#swib/2020,#IIIF,#Wikibase,#CONTENTdm,#metadata","Digitized cultural heritage material is ubiquitous across the library, archive, and museum landscape but the material descriptions can vary based on domain, institutional best practices, and the amount of effort dedicated to digitization programs. OCLC Research has spent the past few years exploring two primary functions of digital material management: syndication of the material for research use and best metadata management practices for discoverability. This work is closely tied to OCLC’s participation in the IIIF Community. A key benefit of IIIF is aggregation. In 2019, OCLC started a research project that created an index of all CONTENTdm metadata and a discovery interface, providing access to 11 million digitized materials represented by 30 million images, all with IIIF standard support. This project showed the benefits of building an aggregated index and the challenges of working with highly-heterogenous metadata. Based on the findings of that project, OCLC launched a linked data pilot project to explore how we could help CONTENTdm users create and manage linked data for cultural materials. We used the MediaWiki Wikibase platform and, working with the pilot participants, designed a new data model. This effort reinforced the value of applying decentralized domain expertise to converting record metadata into linked data and identified transformation workflows. This presentation will discuss the project findings and demonstrate the services and applications we developed.",SWIB,,,,,,,,,,,
,2020,Using SkoHub for web-based metadata management & content syndication,"Adrian Pohl,Steffen Rörtgen",SWIB/SWIB2020/Using SkoHub for web-based metadata management & content syndication.md,"#swib/2020,#controlled/vocabulary,#SkoHub,#SKOS,#ActivityPub,#workshop","Authority files, thesauri and other controlled vocabularies systems have long been a key element for knowledge management. Frequently a controlled vocabulary is used in cataloguing by different institutions, thus indirectly connecting resources about one topic. In order to find all resources about one topic, one has to query different databases or create and maintain a discovery index. This approach is error-prone and requires high maintenance. SkoHub moves cataloguing of web-based material consequently to the web and builds new, powerful discovery tools on top. It is based on web standards like Simple Knowledge Organizations System (SKOS), ActivityPub and Linked Data Notifications. With the SkoHub infrastructure, knowledge management systems act as topic-based communication channels between content publishers and people looking for relevant resources. In effect, SkoHub allows to follow specific subjects (as in descriptors of a classification, thesaurus etc.) in order to be notified when someone has published new content about that subject on the web. After presenting SkoHub at SWIB19, we now offer a workshop so people can try it out. The workshop participants will learn about the different components by using them in different stages of metadata management'",SWIB,,,,,,,,,,,
,2020,"id.loc.gov and Wikidata, one year later",Matt Miller,"SWIB/SWIB2020/id.loc.gov and Wikidata, one year later..md","#swib/2020,#Wikidata,#LCSH,#LCCN,#bibliographic/systems","The id.loc.gov linked data platform at the Library of Congress has been ingesting Wikidata identifiers since mid-2019. This process has enabled the connection of over 1.2 million links between the two systems. These links are powered by Wikimedians adding a NACO or LCSH LCCN identifier to a Wikidata entity which then flows into the ID platform. Due to the scale and nature of Wikidata there is a high velocity of change to this data. New connections are made and broken everyday in addition to ancillary data changes like Wikidata labels. This talk will present the analysis of this ingest process from 2019 and 2020. We will take a detailed look at trends that emerged from this analysis as well as a holistic look at linked records in both systems. Topics around vandalism, comparing record completeness in the two systems, and change frequency will be explored. As more data from the Wikimedia ecosystem is leveraged in our bibliographic systems it is important to understand the dynamics and differences between the two worlds.",SWIB,,,,,,,,,,,
,2021,An Introduction to SKOS and SkoHub Vocabs,"Adrian Pohl,Steffen Rörtgen",SWIB/SWIB2021/An Introduction to SKOS and SkoHub Vocabs.md,"#swib/2021,#SKOS,#RDF,#controlled-vocabularies,#SkoHub","With Simple Knowledge Organization Systems (SKOS), the World Wide Web Consortium (W3C) more than 15 years ago published a clear and simple RDF-based data model for publishing controlled vocabularies on the web following Linked Data principles. Although a large part of controlled vocabularies – from simple value lists, to thesaurus and classifications – is created and maintained in libraries, SKOS has not been widely adopted yet in the library world. This workshop gives an introduction to SKOS with hands-on exercises. Participants will create and publish their own SKOS vocabulary using GitHub/GitLab and SkoHub Vocabs, a static site generator for SKOS concept schemes.",SWIB,,,,,,,,,,,
,2021,Annif corpus hackathon,Osma Suominen,SWIB/SWIB2021/Annif corpus hackathon.md,"#swib/2021,#Annif,#booth","Description: This booth is for those who’ve already completed the Annif tutorial (or are otherwise familiar with the Annif automatic indexing tool) and now want to get started using their own data sets with Annif. Bring your own data set (e.g. bibliographic records in any format, and/or abstracts or full text documents, indexed/classified with some controlled vocabulary) and we will help you convert those into the required corpus formats for Annif. Some programming skills are going to be necessary for the conversion; e.g. Jupyter Notebooks work great for this but we can help with any programming environment that you are familiar with.",SWIB,,,,,,,,,,,
,2021,Automatic subject indexing with Annif at the German National Library,"Sandro Uhlmann,Claudia Grote",SWIB/SWIB2021/Automatic subject indexing with Annif at the German National Library.md,"#swib/2021,#Annif,#DNB,#GND,#Docker,#bibliographic/database","The German National Library (Deutsche Nationalbibliothek, DNB) is currently setting up a new, modular system for fully automatic subject cataloguing, which allows for service-based flexible workflows. For the automatic-subject-indexing task, Annif, an open-source software for automatic indexing developed at the National Library of Finland, has been evaluated successfully for the DNB data. In the first part of the talk, results of the automatic subject indexing of German-language publications with descriptors of the Gemeinsame Normdatei (Integrated Authority File, GND) are presented. In this use case, 1.3 million GND descriptors are available for subject cataloguing and are applied to digital publications or tables of contents. Aspects of quality as well as the labeling of the metadata provenance of automatically assigned descriptors will be addressed. The second part illustrates the technical integration of Annif based on Docker in the new, modular environment in order to become part of the automatic cataloguing workflows from data collection, identification of the language of the text, selection of the appropriate Annif model, communication with Annif, to updating the bibliographic record with the retrieved Annif results in the bibliographic database.",SWIB,,,,,,,,,,,
,2021,BIBFRAME as a data model for aggregating heterogeneous data in a search portal,Thorsten Fritze,SWIB/SWIB2021/BIBFRAME as a data model for aggregating heterogeneous data in a search portal.md,"#swib/2021,#BIBFRAME,#Lin|gu|is|tik,#thesaurus,#ontology","The Lin|gu|is|tik portal is a research tool for the field of linguistics that has been developed at the University Library Frankfurt am Main since 2012. It provides an integrated search for discipline-specific scientific resources: printed as well as electronic publications, websites, and research data. In order to facilitate the inclusion of language resources that are part of the Linguistic Linked Open Data cloud, linked data technologies have increasingly been incorporated into the portal since 2015. As a major part of this effort the established thesaurus of the Bibliography of Linguistic Literature (BLL) comprising more than 9,000 subject terms has been re-modeled as an ontology (BLL Ontology) and made freely available online. Since our goal is to make full use of the opportunities linked data technologies offer, we decided to replace the underlying proprietary data scheme of the portal by a standardized data model that has been expressly designed for linked data applications. BIBFRAME has been selected to fulfill this role. In this presentation we will give an overview of the ongoing work in the current project phase (2020-2022) and discuss why we chose BIBFRAME for this use case, how we adapted it in terms of an application profile, how it fits into the overall LOD-centric architecture of the portal, and in what way it interacts with the BLL Ontology that is used throughout the portal as authority data. We will also show specific features that the chosen data model makes possible as well as give a brief technological overview.",SWIB,,,,,,,,,,,
,2021,BIOfid: Accessing legacy literature the semantic (search) way,Adrian Pachzelt,SWIB/SWIB2021/BIOfid Accessing legacy literature the semantic (search) way.md,"#swib/2021,#BIOfid,#semantic-search,#nlp","In BIOfid, we make full texts of legacy biodiversity literature available through a semantic search. The semantic search is capable of processing simple single queries for a species (e.g. ""beeches""), but also handles restriction for traits like ""Plants with red flowers"" by applying Natural Language Processing combined with a rule-based generation of database queries. For this purpose, the semantic search is aware of biological systematics (e.g. beeches are plants). Subsequently, the semantic search returns all documents that contain these species. Future expansions will include the geolocated search for a species in a specific area (e.g. ""Beeches in the alps""). To enable these search capabilities, the semantic search draws from a pool of both ontologies and semantically annotated full texts. Within BIOfid, these two kinds of data are intertwined to support the machine understanding of the full texts. In this talk, I will give an introduction into how literature and data are automatically harvested, processed, and prepared for being queried and presented in the BIOfid portal. Furthermore, I will give insights on how user query analysis is done in the portal, discuss its pros, cons, and alternative approaches (e.g. machine learning). Finally, I will give a insight on the current work in BIOfid that involves the extraction of facts from the full texts (Information Retrieval and Extraction).",SWIB,,,,,,,,,,,
,2021,CubicWeb and SparqlExplorer,SWIB/SWIB2021/CubicWeb and SparqlExplorer.md,"#swib/2021,#CubicWeb,#booth,#RDF,#SPARQL,#LogiLab","CubicWeb is a semantic web application framework, licensed under the LGPL, that empowers developers to efficiently build web applications and manage data. It can create an application based on an OWL ontology and import RDF described with this ontology (see also the CubicWeb workshop on Tuesday, 14:00 UTC). With SparqlExplorer, we can visually explore the content of a SPARQL endpoint without writing queries (see also the SparqlExplorer talk on Wednesday, 15:30 UTC). CubicWeb & SparqlExplorer are both maintained by LogiLab.",SWIB,,,,,,,,,,,,
,2021,"CubicWeb, the Semantic Content Management System","Fabien Amarger,Elodie Thieblin,Nicolas Chauvat","SWIB/SWIB2021/CubicWeb, the Semantic Content Management System.md","#swib/2021,#CubicWeb,#Linked-Open-Data,#OWL","Linked Open Data is often published via SPARQL endpoint or data ""dumps"" which do not provide content negociation on the URIs of the data they contain. Moreover, there is no easy to use user interface to manage linked data (including CRUD operations, but also user permissions, rendering, etc.). CubicWeb is a SCMS (Semantic Content Management System) for Linked Open Data.This python-based framework can be used to import OWL schema and RDF data automatically to generate a new CubicWeb instance. This instance can be used out-of-the-box as a single application to serve RDF data through a conventional web interface for browsing and through content negociation for downloading. No need to configure anything, just import and launch the app. The CubicWeb framework implements an administration interface to manage data easily even for non technical people. All the common features of a web application framework are available. We propose to discover together CubicWeb to deploy and serve Linked Open Data: - How to make a CubicWeb instance from an OWL ontology; - How to import RDF data that conforms to the above ontology; - How to browse and query the imported data; - How to add/edit/delete data from the interface; - How to have other applications get RDF data through content negociation. Last but not least, CubicWeb is free/libre and open-source software. Do not hesitate to reuse and contribute!",SWIB,,,,,,,,,,,
,2021,From string to thing: Wikidata based query expansion,Bernd Uttenweiler,SWIB/SWIB2021/From string to thing Wikidata based query expansion.md,"#swib/2021,#Wikidata,#Primo/VE,#SPARQL/queries","'Our discovery system Primo VE (Ex Libris) returns a result list after entering a search string. Retrieval of resources by instances of the categories ""Person"" and ""Place"" is not possible. But these would have 2 advantages: We can better link (in both directions) to other sites that are built on persons or places. And: We can offer additional research support to our users, especially from the Digital Humanities field. We need to find a way from the entered search term (string) to the object (thing) ""person"" or ""place"". For this purpose, the user's ""normal"" search query is dynamically preprocessed by SPARQL queries against Wikidata in such a way that the user is provided with offers for matching places and people. When the user makes a selection from the list of persons or places, we have completed the step from string to thing. Besides, this allows names to be disambiguated and name variants that do not exist in the catalog to be taken into account. If the user makes a selection, then a person page or place page is rendered. On the person page are biographical information, links to other sites, researcher profiles, and archive holdings (from Wikidata, Entityfacts, beacon.findbuch, Metagrid). Among other things, teacher student relationships are also presented and the user can go to the teacher's or student's person page. Based on Wikidata graphs, the resources of the catalog are linked together in new ways. The presentation will provide insights into the objectives, decisions and implementation under Primo. The person and place pages are are implemented in our productive system. Selection options for the user within the search process are implemented in a test system and planned to go live soon. Link to Test system: Suggestions for persons'",SWIB,,,,,,,,,,,
,2021,Handling IIIF and Linked Data in the Handschriftenportal,"Leander Seige,Annika Schröer",SWIB/SWIB2021/Handling IIIF and Linked Data in the Handschriftenportal.md,"#swib/2021,#IIIF,#Handschriftenportal","The new manuscript portal for Germany - Handschriftenportal - will become the central information platform for medieval and early modern manuscripts in German collections, providing descriptive information on manuscripts as well as digital facsimiles. In a decentralized approach, digital images from various institutions will be integrated leveraging the International Image Interoperability Framework (IIIF). Both images and texts will later be the target of user generated annotations. This does and will provide various challenges for the Handschriftenportal, such as: publishing reliable URIs for digitized manuscripts with truly persistent content in the institutions’ backends; linking portal data with external authority data; linking annotations with author information; handling born-digital texts in the context of the IIIF image viewer Mirador; enhancing annotations to citable and persistent micro-publications. Also, there should be ways to feed information on user-generated content back to the institutions holding the original manuscripts. The talk will report on the project's current implementations and future plans, while - at the time SWIB 2021 will take place - the Handschriftenportal will just recently have gone live. The Handschriftenportal is a DFG-funded joint project of the State Libraries in Berlin and Munich, the Herzog August Library Wolfenbüttel and Leipzig University Library.",SWIB,,,,,,,,,,,
,2021,IIIF in the wild,"Leander Seige,Karsten Heck",SWIB/SWIB2021/IIIF in the wild.md,"#swib/2021,#IIIF,#booth,#workshop","The International Image Interoperability Framework (IIIF) allows cross-institutional use of digital images and other media. Not only does this technology allow scholarly work with unique artifacts of human history to be taken to new levels and provide researchers with unprecedented tools. The flexibility of the standard also allows artifacts often made available under free licenses to be used to create new works of art, culture and education. The workshop will start with an introduction on how to find and use IIIF resources. From there, we will show which implementation details of IIIF services enable or prevent the reusability of the digital resources. For this we will use freely available IIIF applications and real IIIF repositories of well-known institutions around the globe. This may include digital workspaces, storytelling tools and fun apps. Participants should bring their own laptops. Having an own pre-created account on Github might be helpful. The workshop is intended for both: those who provide IIIF services and those who, as scholars or otherwise creative workers, would like to use IIIF in their daily work.",SWIB,,,,,,,,,,,
,2021,Introduction to Fedora 6.0,"Arran Griffith,Daniel Bernstein",SWIB/SWIB2021/Introduction to Fedora 6.0.md,"#swib/2021,#Fedora,#workshop","Fedora is a flexible, extensible, open source repository platform for managing, preserving, and providing access to digital content. For the past several years the Fedora community has prioritized alignment with linked data best practices and modern web standards, including the Linked Data Platform (LDP), Web Access Controls (WebAC), Memento, Activity Streams, and more. With the recent release of Fedora 6.0, we have shifted our attention back to Fedora's digital preservation roots with a focus on durability and the Oxford Common File Layout (OCFL). This workshop will provide an introduction to Fedora 6.0 with a focus on both the linked data and digital preservation capabilities. Both new and existing Fedora users will be interested in learning about and experiencing Fedora features first-hand. Attendees will be given access to individual cloud-hosted Fedora instances to use during the workshop.'",SWIB,,,,,,,,,,,
,2021,Introduction to OpenRefine,Sandra Fauconnier,SWIB/SWIB2021/Introduction to OpenRefine.md,"#swib/2021,#OpenRefine,#Wikidata","OpenRefine is a powerful free and open source tool for working with messy data: cleaning it, transforming it from one format into another, and connecting it with knowledge bases, including Wikidata. OpenRefine is used by quite diverse communities interested in data manipulation and cleaning: librarians, researchers, data scientists, data journalists, and in the Wikimedia community. This workshop demonstrates the basic functionalities of OpenRefine, including how to reconcile data with Wikidata.",SWIB,,,,,,,,,,,
,2021,Introduction to Solid,Hochsten Bach,SWIB/SWIB2021/Introduction to Solid.md,"#swib/2021,#Solid,#booth,#pods,#protocol","Description: Solid lets people store their data securely in decentralized data stores called Pods. Pods are like secure personal web servers for data. All data in a pod is accessible via the Solid Protocol. When data is stored in someone’s pod, they control who and what can access it. Any kind of data can be stored in a Solid pod, including regular files that you might store in a Google Drive or Dropbox folder, but it is the ability to store Linked Data that makes Solid special.",SWIB,,,,,,,,,,,
,2021,Introduction to the Annif automated indexing tool,"Osma Suominen,Mona Lehtinen,Juho Inkinen,Moritz Fürneisen,Anna Kasprzik",SWIB/SWIB2021/Introduction to the Annif automated indexing tool.md,"#swib/2021,#Annif,#tutorial,#metadata,REST,API,#flipped-classroom,#hackathon","Many libraries and related institutions are looking at ways of automating their metadata production processes for example through the adoption of AI technology. In this hands-on tutorial, participants will be introduced to the multilingual automated subject indexing tool Annif (annif.org) as a potential component in a library’s metadata generation system. By completing exercises, participants will get practical experience on setting up Annif, training algorithms using example data, and using Annif to produce subject suggestions for new documents using the command line interface, the web user interface and REST API provided by the tool. The tutorial will also introduce the corpus formats supported by Annif so that participants will be able to apply the tool to their own vocabularies and documents. The tutorial will be organized using the flipped classroom approach: participants are provided with a set of instructional videos and written exercises, and are expected to attempt to complete them on their own time before the tutorial event, starting at least a week in advance. The actual event will be dedicated to solving problems, asking questions and getting a feeling of the community around Annif. Participants are instructed to use a computer with at least 8GB of RAM and at least 20 GB free disk space to complete the exercises. The organizers will provide the software as a preconfigured VirtualBox virtual machine. Alternatively, Docker images and a native Linux install option are provided for users familiar with those environments. No prior experience with the Annif tool is required, but participants are expected to be familiar with subject vocabularies (e.g. thesauri, subject headings or classification systems) and subject metadata that reference those vocabularies. Exercises and introductory videos can be found in the Annif-tutorial GitHub repository. We also plan to hold a hackathon-style event on Open Day at SWIB 21. This hackathon would be targeted to those who can already work fluently with Annif but would like deeper knowledge on how to apply Annif to their own data.",SWIB,,,,,,,,,,,
,2021,Metafacture,SWIB/SWIB2021/Metafacture.md,"#swib/2021,#Metafacture,#metadata",Metafacture is a toolkit for processing semi-structured data with a focus on library metadata.,SWIB,,,,,,,,,,,,
,2021,OpenRefine,SWIB/SWIB2021/OpenRefine.md,"#swib/2021,#OpenRefine","Description: OpenRefine is a powerful free and open source tool for working with messy data: cleaning it, transforming it from one format into another, and connecting it with knowledge bases, including Wikidata. [OpenRefine] is used by quite diverse communities interested in data manipulation and cleaning: librarians, researchers, data scientists, data journalists, and in the Wikimedia community.",SWIB,,,,,,,,,,,,
,2021,"Publication, dissemination and network collaboration in the documentation of digital collections of memory institutions: interoperability between the information environments Wikidata, Wikimedia Commons, Wikipedia and the free software Tainacan",Dalton Lopes Martins,"SWIB/SWIB2021/Publication, dissemination and network collaboration in the documentation of digital collections of memory institutions interoperability between the information environments Wikidata, Wikimedia Commons, Wikipedia and the free software Tainacan.md","#swib/2021,#Wikidata,#information-architecture,#Tainacan,#Brazil","The presentation proposes the development of a service that facilitates the publication and monitoring of editions and collaborations carried out by users of digital collections from memory institutions in the Wikidata, Wikimedia Commons and Wikipedia environments. The project is in the initial phase of modeling and information architecture development. Therefore, it proposes the technical modeling of a feedback service, called roundtripping, between the information networks of the wiki ecosystem and the free software Tainacan. Tainacan has become an important free software for the management and dissemination of digital collections in a network of memory institutions in Brazil. There are important technical and operational challenges for the implementation of a service that not only allows the publication of collections from a digital repository of a memory institution, but also monitors the reuse, editions and collaborations of users in other information networks. By allowing the connection between the Wikidata, Wikimedia Commons and Wikipedia information networks for the publication and encouragement of the reuse of the digital collections of institutions already published in Tainacan, the project aims to expand the circulation of heritage collections, make the knowledge generated about them relevant and, thus, value Brazilian material culture on the network society. All functionalities imagined for the project will be implemented integrated to the Tainacan plugin. The research project involves a cooperation between the Wikimedia Brasil Association and the São Paulo Museum of the University of São Paulo.",SWIB,,,,,,,,,,,
,2021,QA catalogue,SWIB/SWIB2021/QA catalogue.md,"#swib/2021,#FRBR,#MARC21","Description: QA catalogue is an open source toolset for detecting particular aspects of either a selection of MARC21 records or a full library catalogue. The aspects that the tool analyses are: validity of the structure and the data element values against MARC21 bibliographic standard (and definitions of locally defined data elements); completeness and three variations of weighted completeness (these are implementation of suggestions found in research papers); the support of FRBR functions; analysis of subject indexing and authorities, the analysis of the positional control fields (Leader, 00[6-8]); the weight of the most frequently used data elements; and the cataloguing history.",SWIB,,,,,,,,,,,,
,2021,RDA/RDF at the University of Washington Libraries,"Benjamin Moore Riesenberg,Theodore Gerontakos,Jian Lee,Melissa Morgan,Crystal Clements",SWIB/SWIB2021/RDARDF at the University of Washington Libraries.md,"#swib/2021,#RDA,#RDF,#Sinopia","The Linked Data Team at the University of Washington Libraries describes their work using RDA/RDF. This work includes ongoing creation and use of RDA/RDF machine-readable application profiles compatible with the Sinopia Linked Data Editor, a mapping and conversion project between the RDA/RDF and BIBFRAME data models, and a recently-launched project to create a mapping from MARC21 to RDA/RDF with future large-scale metadata conversion in mind. The Linked Data Team at the University of Washington Libraries’ goal in doing this work is to demonstrate that RDA/RDF is a better tool for representing RDA bibliographic descriptions than BIBFRAME, and to lay the groundwork for expanding the use of the RDA/RDF ontology in the international GLAM community.",SWIB,,,,,,,,,,,
,2021,Representing the Luxembourg Shared Authority File based on CIDOC-CRM in Wikibase,"Jose Emilio Labra Gayo,Michelle Pfeiffer,Andra Waagmeester,Maarten Zeinstra,Maarten Brinkerink,Joël Thill,Christel Kayser",SWIB/SWIB2021/Representing the Luxembourg Shared Authority File based on CIDOC-CRM in Wikibase.md,"#swib/2021,#CIDOC,#Shared-Authority-File,#Wikibase","'The Luxembourg Competency Network on Digital Cultural Heritage is developing a Shared Authority File, to combine the knowledge from national heritage institutions and to increase the impact of the member institutions' digitised collections. The data of the authority file uses a custom developed CIDOC-CRM RDF/XML data model, starting with a model for authority records for person entities. The Shared Authority File uses the open source product Wikibase as its main data store and internal user interface. Referring to CIDOC-CRM as an ontological standard in the context of Wikibase is challenged in a particular manner due to the event-centric approach of the domain ontology. This contribution describes how the project solved the challenge of hosting data conforming to CIDOC-CRM on a Wikibase instance, while leveraging the qualifier/reference model of Wikibase. This solution takes into account some quality attributes like intuitiveness, expressiveness and consistency of the hosted data. We have created a prototype of a populated Wikibase from existing database dumps, which demonstrates the feasibility of the proposed approach. We consider that the mapping between the developed CIDOC-CRM model and the Wikibase model that we present in this paper can be valuable for other cultural heritage institutions working with Wikibase.'",SWIB,,,,,,,,,,,
,2021,Semi-automated methods for BIBFRAME work entity description,Jim Hahn,SWIB/SWIB2021/Semi-automated methods for BIBFRAME work entity description.md,"#swib/2021,#BIBFRAME,#LCSH,#Annif,#API","Describing library resources with the BIBFRAME vocabulary and its core entities of Work, Instance, and Item is a resource intensive process. Cataloging in linked data RDF editors with BIBFRAME involves careful selection of, and referencing to, external authority entities. Creating external authoritative links is essential to produce an accurate context while describing the BIBFRAME Work entity in an RDF editor. This presentation will report an investigation of machine learning methods for the semi-automated creation of a BIBFRAME Work entity description within the RDF linked data editor Sinopia. The automated subject indexing software Annif was configured with the Library of Congress Subject Headings (LCSH) vocabulary from the Linked Data Service. A dataset comprising 9.3 million titles and LCSH linked data references from the IvyPlus POD project was used as the training corpus. POD is a data aggregation project involving member institutions of the IvyPlus Library Confederation and contains over seventy million MARC21 records, nearly 40 million of which are unique to the corpus. The machine learning outputs, accessed by Annif web API, enable a feature for dynamically auto suggesting subject attributes based on a cataloger supplied title. This method of research and development is foregrounded with considerations for ethical use of semi-automated subject description. Semi-automation as a potential integration target is in contrast to completely automated cataloging and is a very specific use of machine learning. In this work automation was used as a way to support, not replace, professional expertise.",SWIB,,,,,,,,,,,
,2021,SkoHub Vocabs,SWIB/SWIB2021/SkoHub Vocabs.md,"#swib/2021,#SkoHub,#Vocabs,#SKOS",SkoHub Vocabs is a static site generator for publishing SKOS concepts schemes on the web.,SWIB,,,,,,,,,,,,
,2021,"SparqlExplorer, exploring Linked Open Data","Fabien Amarger,Elodie Thieblin,Chauvat Nicolas","SWIB/SWIB2021/SparqlExplorer, exploring Linked Open Data.md","#swib/2021,#SPARQL,#SparqlExplorer","Since developing and maintaining web applications is costly, a lot of the data published in SPARQL endpoints is difficult to explore by users that are not experts of LOD tools and languages. In order to make that data more accessible we developed SparqlExplorer, a web application that can be plugged into any SPARQL endpoint to make its content browsable without writing a single query by applying views to render web pages. For example, to explore a dataset about books, SparqlExplorer would use a book-specific views that executes a SPARQL query to retrieve that data related to a book and generate a web page that displays the title, the ISBN and maybe a thumbnail of the cover. Views are javascript functions that take data and output HTML. Views depend on the structure of the input data, but are independent of the SPARQL endpoint. A ""Book"" view can be reused on all libraries and bookstore SPARQL endpoints that published data using the vocabulary that the view expects as input. Views are simple functions that do not require the developers to have a deep understanding of Semantic Web technologies: usual front-end knowledge is enough. Last but not least, SparqlExplorer is free/libre and open-source software. Do not hesistate to reuse and contribute!",SWIB,,,,,,,,,,,
,2021,String matching algorithms in OpenRefine clustering and reconciliation functions - a case study of person name matching,Christiane Klaes,SWIB/SWIB2021/String matching algorithms in OpenRefine clustering and reconciliation functions - a case study of person name matching.md,"#swib/2021,#OpenRefine,#clustering,#reconciliation,#ElexiFinder,#LexBib","Person entities are important linking nodes both within and between Linked Open Data resources across different domains and use cases. Therefore, efficient identity management is a crucial part of resource development and maintenance. This case study is concerned with the task of semi-automatic population of a newly developed domain knowledge graph, LexBib Wikibase with high-quality person data. We aim to transform person name literals taken from publication metadata into Semantic Web entities, to enable improved retrieval and entity enrichment for the domain-specific discovery portal ElexiFinder. In a prototype workflow, the open source tool OpenRefine is used as a one-tool solution to perform deduplication, disambiguation and reconciliation of person names with reference datasets, using a sample of 3.104 name literals taken from LexBib bibliography. We closely examine OpenRefine’s clustering functions with its underlying string matching algorithms, focusing on their ability to account for different error types that frequently occur in person name matching, such as spelling errors, phonetic variations, initials, or double names. Following the same approach, string matching processes implemented in two widely used reconciliation services for Wikidata and VIAF are examined. OpenRefine offers various features. We also analyse the usefulness of OpenRefine features to support further processing of algorithmic output. The results of this case study may contribute to a better understanding and subsequent further development of interlinking features in OpenRefine and adjoining reconciliation services. By offering empiric data on OpenRefine’s underlying string matching algorithms, the study’s results supplement existing guides and tutorials on clustering and reconciliation, especially for person name matching projects.",SWIB,,,,,,,,,,,
,2021,Surveillance capitalism in our libraries,Sarah Lamdan,SWIB/SWIB2021/Surveillance capitalism in our libraries.md,"#swib/2021,#RELX,#ProQuest","'In the transition from industrial to informational capitalism, much of our lived experience has gone from physical to digital, including library services. As publishers, library vendors, and other informational service providers have become internet-based companies, their business models have transitioned from analog services to data-based services. In short, our traditional library service providers are becoming data analytics companies, dabbling in, or diving into personal data brokering. From RELX to ProQuest, major library vendors are finding new ways to extract and monetize people's personal data. Researchers are finding surveillance software like ThreatMetrix in their research databases, and data analytics companies like Clarivate are trying to acquire ProQuest, a major library service platform provider to exploit library patrons' data to create more academic metrics to sell grant funders and research institutions. All of these corporate decisions are part of a trend of our vendors collecting library patrons' personal data. The increasing surveillance capitalism in our library spaces makes open access more important than ever.'",SWIB,,,,,,,,,,,
,2021,Using Linked Data relationships to enhance discovery and mitigate bias,Juliet L. Hardesty,SWIB/SWIB2021/Using Linked Data relationships to enhance discovery and mitigate bias.md,"#swib/2021,#metadataBias","This presentation will share an open-source JavaScript-based Linked Data project that explores techniques to improve terminology used for discovering resources from systemically marginalized communities (metadataBias). As a research project, it is also investigating a practical application of Linked Data to enhance usability of library systems. Controlled vocabularies used in cultural heritage organizations (galleries, libraries, archives, and museums) are a helpful way to standardize terminology but can also result in misrepresentation or exclusion of systemically marginalized communities. Library of Congress Subject Headings (LCSH) is one example of a widely used yet problematic controlled vocabulary for subject headings. Linked Data vocabularies can connect terms between larger, less representative vocabularies (like LCSH) and terms from a community’s vocabulary to aid and instruct end users conducting research online. This project uses The Homosaurus, an LGBTQ+ Linked Data controlled vocabulary, to provide an augmented and updated search experience to mitigate bias within a system that only uses LCSH for subject headings. The presentation will provide a demonstration, share progress to date on research findings, usability feedback, and implementation, instructions for how to use or contribute to this research, as well as plans for further development.",SWIB,,,,,,,,,,,
,2021,Using linked data notifications to assemble the scholarly record on the decentralised web,"Patrick Hochstenbach,Ruben Dedecker,Miel Vander Sande,Jeroen Werbrouck,Herbert van de Sompel,Ruben Verborgh",SWIB/SWIB2021/Using linked data notifications to assemble the scholarly record on the decentralised web.md,"#swib/2021,#decentralised-architecture,#Decentralised-Web","In this presentation we present the results of the Mellon funded Scholarly Communication project that proposes a decentralised architecture for generating, propagating and notification of artefact lifecycle information in scholarly networks. Scholarly artefacts go through many stages from the creation of artefacts, through the registration of these artefacts in repositories, requesting certification at a publisher websites where they will be peer-reviewed and eventually published, to the archivation in a (web) archive. The results of each of these events are typically stored in different environments that are rarely interconnected. This makes assembling the complete lifecycle of artefacts an expensive post-factum endeavour involving mining many information sources and applying heuristics to combine the information into a meaningful result. The Mellon Scholarly Communication project proposes a researcher-centric, institution-enabled scholarly communication system aligned with Decentralised Web concepts and technologies. In this vision researchers use a personal domain and associated storage space (researcher pod) as their long-term scholarly hub. Using Linked Data Notifications these scholarly hubs communicate with service hubs (such as peer review systems, discovery systems, archives) for the fulfilment of the functions of scholarly communication. The research pod stores all the information pertaining to the artefacts that the researcher contributes to the scholarly record where it can be shared and consulted.",SWIB,,,,,,,,,,,
,2021,Web editor for JSON/Bibframe data in libraries,Nicolas Prongué,SWIB/SWIB2021/Web editor for JSONBibframe data in libraries.md,"#swib/2021,#Bibframe,#JSON,#JSON-LD,#RERO+,#Swiss,#ILS","This presentation shows a cataloguing form created to edit data stored in a Bibframe/JSON format. It points out the specificities, obstacles, advantages and difficulties of this implementation in comparison to a raw MARC editor. On the one hand, the interface must be really user-friendly, as it is targeted at public, school and special libraries. On the other hand, it must enable to receive granular bibliographic records from MARC and to edit JSON data that is hierarchically structured and deeply nested, composed of a multitude of fields with different conditionalities and validation rules. The system does not enable to create directly RDF data but it represents, in the specific library context, a relevant step towards the edition of linked and interoperable data. The next step regarding semantic web is to define a transformation and context for exposing this data in JSON-LD. This happens within the transition of RERO+, a Swiss competence and service centre for libraries, to an open source and in house library system called RERO ILS. About 60 libraries in Switzerland are using it since July 2021. In this context, it was decided to abandon MARC21 in favour of JSON, using the Bibframe model as far as possible and maintaining an interoperability for data import/export. RERO ILS is based on the Invenio 3 framework proposed by CERN, which heavily relies on JSON and JSON schema for resource management. The editor is also implemented within SONAR, another RERO+ software made to manage institutional repositories. RERO ILS source code: https://github.com/rero/rero-ils/",SWIB,,,,,,,,,,,
,2021,"coli-ana – Automatic analysis of the Dewey Decimal Classification, a service of the Verbundzentrale des GBV","Uma Balakrishnan,Stefan Peters,Jakob Voss","SWIB/SWIB2021/coli-ana – Automatic analysis of the Dewey Decimal Classification, a service of the Verbundzentrale des GBV.md","#swib/2021,#DDC,#dewey-decimal-classification,#VZG-colibri,#project,#coli-ana,#enrich,#K10plus","Dewey Decimal Classification (DDC) is the most widely used classification system internationally. It was developed by Melvil Dewey in 1873 based on the Decimal Classification conceived by Gottfried Wilhelm Leibniz in the 17ᵗʰ century. At the dawn of the 21ᵗʰ century, DDC attracted great interest amongst academic libraries in Europe, and was translated into German and several other European languages. The contemporary Dewey Decimal System with over 48.000 classes and six tables allows a huge flexibility and fineness in building new numbers such as 700.90440747471. However, this process is complex and is based on several intricate rules and instructions. The lack of captions in such built numbers makes it difficult to understand and re-use by the non-Dewey catalogers. Thus, in 2003 under the project VZG-colibri, a sub-project coli-ana was initiated to develop a tool that would automatically decompose and analyze any given DDC notation to its tiniest component, enrich them with their caption, and provide the semantic relationship between each element in JSKOS (a JSON-LD format based on SKOS). The results of coli-ana improve information retrieval, facilitate re-use, and aid further study to enhance knowledge organization systems in general. coli-ana delivers DDC captions only in German, but it is planned to integrate the English version of DDC as well. The results of the tool are incorporated in the mapping tool Cocoda as well to assist mappings to and from DDC built numbers. This talk will give a brief background of the project coli-ana, elucidate with examples the decomposition of notations with their adherent rules and instructions, present the coli-ana web service as well as demonstrate its use cases in the project coli-conc and in the K10plus union catalogue.",SWIB,,,,,,,,,,,
,2021,"coli-conc infrastructure (Cocoda mapping tool, BARTOC, coli-ana…)",Nich Tich,"SWIB/SWIB2021/coli-conc infrastructure (Cocoda mapping tool, BARTOC, coli-ana…).md","#swib/2021,#coli-conc,#Cocoda,#booth,#BARTOC,#coli-ana,#JSKOS,#API","Description: Project coli-conc subsumes services and standards to facilitate working with controlled vocabularies and mappings between their concepts. We welcome questions and discussion around these topics, including the mapping tool Cocoda, BARTOC vocabulary registry, coli-ana DDC decomposition, JSKOS data format and API etc.",SWIB,,,,,,,,,,,
,2022,A LITL more quality: improving the correctness and completeness of library catalogs with a librarian-in-the-loop linked data workflow,"Sven Lieber,Ann Van Camp,Hannes Lowagie",SWIB/SWIB2022/A LITL more quality improving the correctness and completeness of library catalogs with a librarian-in-the-loop linked data workflow.md,"#swib/2022,#KBR,#BELTRANS,#project,#SPARQL","Traditionally, a library maintains both bibliographic data about publications and authority data about publication contributors such as authors or publishers. At the Royal Library of Belgium (KBR), we currently maintain more than 900,000 authority records about persons and metadata records of over 3.5 million publications. As an authoritative data source we are expected to have correct and complete data. In contrast to syntactical data quality issues – which can be detected automatically – issues regarding the correctness of data or completeness of the catalogue often require semantic checks, e.g., if the correct ISBN is used and if the nationality of the contributor is correct. However, the amount of Belgian cultural heritage data makes manual quality checks very time-consuming. Within the BELTRANS project which studies Intra-Belgian book translation flows of publications based on metadata (who?, when?, where?), we developed an approach to support librarians in the detection and correction of semantic data issues. This approach consists in the integration of data from different heterogeneous sources via RDF and identifying contradicting data automatically with SPARQL. Librarians can inspect the contradicting data fields, choose the correct data, and thus semi-automatically correct the contradiction. This contribution describes the approach, which is implemented as a 5-step workflow using linked data and Python. We discuss how we use the workflow to improve the correctness of the data used in the BELTRANS project and the wider applicability of the workflow within the National Library to improve the completeness of our catalog with respect to legal deposit. Since the expertise of librarians is needed to interpret identified data contradictions, the next step is to improve the usability of our prototypical solution.",SWIB,,,,,,,,,,,
,2022,A crosswalk in the park? Converting from MARC 21 to Linked Art,"Martin Lovell,Timothy A. Thompson",SWIB/SWIB2022/A crosswalk in the park Converting from MARC 21 to Linked Art.md,"#swib/2022,#LUX,#CIDOC,#ontology,#Yale,#MARC/21,#discovery/environment,#JSON-LD","Yale University is currently undertaking a multiyear effort to create a cross-collections discovery environment called LUX, or “light,” from the university’s Latin motto. LUX aggregates metadata from the catalogs of Yale’s four main collecting units, including the university library. The LUX platform has been designed using the Linked Art (LA) profile of the CIDOC CRM ontology as its common data model. Each collecting unit has developed a crosswalk from its local domain standard to the LA model. In the library, metadata librarians and software engineers have collaborated to develop and document a crosswalk from MARC 21 to LA JSON-LD - JavaScript Object Notation Linked Data. Implementing the crosswalk has meant designing a system that is more complex than a typical transformation. A single MARC record may be expanded into multiple top-level LA entities. In total, Yale’s 12.5 million catalog records translate into 47.5 million entities in LA. To implement the semantics of the LA model and manage data dependencies within and across MARC records, a new system was developed using Java and Spring Boot with Postgres. The system was designed to serve up LA entities, exposed through an activity stream, and to provide a framework for a modular set of transformation components. Generating linked data at scale was a new challenge for the library. The initial design used a database schema based on triples, with resolvable identifiers for all objects; this approach provided an intuitive way to create and query relationships without using a native triple store or graph database, given the learning curve that an unfamiliar system would have entailed. However, the triples-based approach proved too resource-intensive at scale, and the system was modified to store some data in JSON and resolve only top-level entities, while still storing a subset of the relationship data for querying.",SWIB,,,,,,,,,,,
,2022,An Introduction to SKOS and SkoHub Vocabs,"Adrian Pohl,Steffen Rörtgen",SWIB/SWIB2022/An Introduction to SKOS and SkoHub Vocabs.md,"#swib/2022,#SKOS,#SkoHub","With Simple Knowledge Organization Systems (SKOS), the World Wide Web Consortium (W3C) more than 15 years ago published a clear and simple RDF-based data model for publishing controlled vocabularies on the web following Linked Data principles. Although a large part of controlled vocabularies – from simple value lists, to thesaurus and classifications – is created and maintained in libraries, SKOS has not been widely adopted yet in the library world.",SWIB,,,,,,,,,,,
,2022,BIBFRAME for academic publishing in psychology,Tina Trillitzsch,SWIB/SWIB2022/BIBFRAME for academic publishing in psychology.md,"#swib/2022,#PSYNDEX,#ZPID,#BIBFRAME,#PsychPorta,#ontologies","PSYNDEX of ZPID is a publically funded reference database for psychological research literature from German-speaking countries. We are currently rewriting our cataloging and indexing software and exposing all PSYNDEX publication data as linked open data, the resulting BIBFRAME-based knowledge graph serving as the basis for our new search portal PsychPorta. This case study discusses our rationales, modeling difficulties, and tools and strategies for migration and transformation.
Our cataloging software had accumulated many errors and problems over the years; we needed a modern, maintainable application. PubPsych, our search portal for researchers, teaching staff, practicing psychologists, and laypeople is also showing its age. To compete with commercial search engines, its successor PsychPorta needs to offer an improved, modern search experience while also providing persistent, open, interoperable, and reusable data.
PSYNDEX has high indexing standards: scientific publications are described with many details about the actual studies they document, using various keyword thesauri, classification systems, controlled vocabularies for research methods and study samples, metadata about preregistration, funding etc. BIBFRAME has little to offer for such details. To represent all this data and make it usable by humans and machines, other ontologies must be integrated, and new properties and classes have to be created. A new requirement was grouping “versions” of the same content in different publication forms – thus the adoption of BIBFRAME Works and Instances. Our experiences may be useful to other domains with indexing needs not covered by BIBFRAME, and also offer guidance on where BIBFRAME seems vague or unfinished. Our approach to handling e.g. dependent parts of things (journal articles, chapters) or aggregates and serials may be reused, thus improving interoperability.",SWIB,,,,,,,,,,,
,2022,Digital Scriptorium 2.0: toward a community-driven LOD knowledge base and national union catalog for premodern manuscripts,"L.P. Coladangelo,Lynn Ransom,Doug Emery",SWIB/SWIB2022/Digital Scriptorium 2.0 toward a community-driven LOD knowledge base and national union catalog for premodern manuscripts.md,"#swib/2022,#Scriptorium,#manuscript,#OpenRefine","Digital Scriptorium 2.0 (DS 2.0) is a project to redevelop the technical platform of a national union catalog of premodern manuscripts representing the full range of global manuscript cultures in US collections. This paper documents the development and implementation of a workflow for aggregating and transforming metadata records for premodern manuscripts in order to structure and publish them as linked open data (LOD). Strategies and practices for extracting, enhancing, and combining metadata from heterogeneous sources were explored. Data sources included library catalogs and other institutional data repositories, ranging from metadata in CSV files to XML-structured schemas. Automated scripts extracted and combined metadata by entity types for such entities as agents, places, materials, subjects, genres, languages, and date information recorded in catalog records as string values. Using OpenRefine, string values were reconciled to linked open databases and vocabularies such as Wikidata, FAST, and the Getty’s AAT and TGN. This resulted in the creation of open-source metadata repositories to be used in the transformation of DS member metadata records into LOD for uploading into a shared knowledge base (Wikibase), thus semantically enriching existing cultural heritage metadata and enabling end user function like faceted search and SPARQL querying. This approach facilitated low institutional barriers to contribution as participating institutions did not have to alter their metadata records to contribute data and they retained control over their own catalog record formats and cataloging practices. The resulting database of heterogeneous, cross-institutional published LOD can be leveraged by the community of scholars, librarians, curators, and citizen scholars who participate in Digital Scriptorium to enhance and build upon existing knowledge about their collections.",SWIB,,,,,,,,,,,
,2022,Evaluation and evolution of the Share-VDE 2.0 linked data catalog,"Jim Hahn,Beth Camden,Kayt Ahnberg,Filip Jakobsen",SWIB/SWIB2022/Evaluation and evolution of the Share-VDE 2.0 linked data catalog.md,"#swib/2022,#Share-VDE,#IFLA-LRM,#SVDE,#library-reference-model","Share-VDE (SVDE) is a library-driven initiative which brings together the bibliographic catalogs and authority files of a community of libraries in an innovative discovery environment based on linked data. The beta release of the SVDE 2.0 (https://www.svde.org) catalog was collaboratively shaped among multiple perspectives and stakeholder groups. A team at the University of Pennsylvania Libraries gathered feedback through structured interviews and observations from library catalogers working in linked data, university faculty, and new undergraduate students in order to understand how linked data supports user tasks promulgated in the IFLA Library Reference Model (IFLA-LRM). Specific user tasks evaluated over remote testing sessions include ascertaining how library catalogers make use of advanced search functionality provided in the linked data interface. Context finding tasks included evaluating how Penn catalogers might find a linked data search useful for providing context to their searching or for helping to understand a research area. Specific LRM mapping focused on the LRM Identify user task; particularly disambiguation of similar name results. For comparative results similar questions are posed to students and faculty. Several targeted questions of faculty included understanding the relationships in linked data that are useful for future research planning using linked data search. In compiling results of the study we describe the linked data functionality and scenarios which the Share-VDE 2.0 discovery system addresses and the ways in which user feedback is supporting the evolution of linked data discovery. We will show how we have evolved the system to align with user needs based on evaluations across multiple stakeholder groups.",SWIB,,,,,,,,,,,
,2022,Getting Started with SPARQL and Wikidata,"Katherine Thornton,Joachim Neubert",SWIB/SWIB2022/Getting Started with SPARQL and Wikidata.md,"#swib/2022,#SPARQL,#Wikidata,#workshop",Would you like to learn how to write SPARQL queries for the Wikidata Query Service? In this workshop we will demonstrate a variety of SPARQL queries that highlight useful features of the Wikidata Query Service. We will also practice writing SPARQL queries in a hands-on session. After participating in this workshop you will have familiarity with designing and writing your own SPARQL queries to ask questions of the Wikidata Query Service.,SWIB,,,,,,,,,,,
,2022,Getting started with Wikidata and SPARQL: build a dynamic web page,Daniel Scott,SWIB/SWIB2022/Getting started with Wikidata and SPARQL build a dynamic web page.md,"#swib/2022,#Wikidata,#SPARQL,#workshop","In this workshop, you will learn how to explore and retrieve data from linked data stores such as Wikidata using the SPARQL Protocol and RDF Query Language (SPARQL). Starting from a simple selection of random data, you will create progressively complex SPARQL queries. As you practice each element of SPARQL, you will also learn the structure of Wikidata and the SPARQL extensions offered by the Wikidata Query Service. Finally, you will create a simple web page that, using your new knowledge of SPARQL, dynamically queries Wikidata to display data.",SWIB,,,,,,,,,,,
,2022,How are data collections and vocabularies teaching AI systems human stereotypes?,Artem Reshetnikov,SWIB/SWIB2022/How are data collections and vocabularies teaching AI systems human stereotypes.md,"#swib/2022,#bias,#machine-learning,#fairness,#ai-models,#AI,#cultural-heritage,#digitalization,#GLAM,#Iconclass","Bias is a concept found in machine learning which means that the data which was used for training is in some way not representative of the real world and therefore the patterns or models that are generated are systematically skewed. While bias is a technical concept, fairness is a more social concept that can have direct implications for users. Hannes Hapke et al. define the concept of fairness as “the ability to identify when some groups of people get a different experience than others in a problematic way”. The authors illustrate the problem of fairness with an example of people who tend not to pay back their loans. If an AI model is trying to predict who should have their credit extended, this group of people should have a different experience than others, i.e. not have their credit extended. An example of a situation to avoid is when people whose application for a loan is unfairly turned down are predominantly of a certain race. Digitalization of CH objects for machine ingestion may have started for preservation reasons, but it allowed applying AI technology in order to extract knowledge that can improve user experience and be a valuable resource for GLAM. Generating digitalized data for AI use implies that it is annotated according to what is envisioned to be meaningful and relevant for future ML tasks; Iconclass is a great example. These annotations are the basis on which the AI models are built, thus whatever ideas and concepts were included in these structures can be recognized in the final AI model, which will reflect the possible original bias. Our presentation will show data collections of GLAM and vocabularies such as Iconclass containing prejudices against LGBT, gender inequality, or colonization stereotypes, and we will illustrate how these systems can embed these human stereotypes in AI generated data and why fairness is important for AI in GLAM.",SWIB,,,,,,,,,,,
,2022,Improving language tags in cultural heritage data: a study of the metadata in Europeana,"Nuno Freire,Paolo Scalia,Antoine Isaac,Eirini Kaldeli,Arne Stabenau",SWIB/SWIB2022/Improving language tags in cultural heritage data a study of the metadata in Europeana.md,"#swib/2022,#Europeana,#quality,#metadata,#Europeana-Translate,#project","Enhancing the multilingual accessibility of its rich cultural heritage material is a constant objective of Europeana and key to improving the user experience it offers. Technological advances are opening new ways for multilingual access but for their successful application the language of the existing data must be identified. In RDF data, language is indicated by the language tag in a dedicated attribute (xml:lang in RDF/XML). Previous studies conducted on Europeana datasets show that language tags are not used as often as they should, but we do not have precise statistics on this. Moreover, language tags found in Europeana have data quality issues – they do not always follow established standards even though Europeana already performs some (automatic) normalisation of tags. We conducted a study on the language tags included in the metadata of Europeana with two objectives in mind: First, to inform decision-making about possible improvements in the current language tag normalisation process, and second, to enhance the quality and quantity of training data for specialising automatic translation systems in the cultural heritage domain (a crucial objective for the Europeana Translate project, which aims to translate 25 million records of Europeana). Our study analysed the totality of the data in Europeana, which contains over 1,700 million RDF literals, and identified that only 15.9% of the literals are language-tagged. We also determined that 3.3% of the language tags are not valid according to the IETF BCP 47 standard. In our presentation, we recount the results of this study along with the improvements in the normalisation process we applied to collect training data for machine translation.",SWIB,,,,,,,,,,,
,2022,Insight into the machine-based subject cataloguing at the German National Library,Christoph Poley,SWIB/SWIB2022/Insight into the machine-based subject cataloguing at the German National Library.md,"#swib/2022,#Germany,#GND,#DDC,#machine-learning,#Erschließungsmaschine,#indexing,#subjct-cataloguing,#Annif,#machine-learning,#architecture,#practical-insights","The German National Library has the mandate to collect online publications published in Germany or in the German language, translated from German or relating to Germany. Our approach is to provide metadata including classification codes and subject headings to make them searchable and retrievable. To handle the amount of data, which increases by about 1–2 million records every year, we introduced machine-learning-based tools into productive operations in order to be able to assign GND index terms, DDC subject categories and DDC short numbers automatically. The first steps in that direction were taken more than ten years ago. In the last three years we have set up a project called “Erschließungsmaschine” (“indexing machine”; EMa) to replace the legacy system with a new modular and flexible architecture for automated subject cataloguing. The core component is the toolkit Annif developed by the National Library of Finland, which provides us with lexical, statistical and neural-network-based machine learning algorithms behind a unified interface. In this presentation we want to give practical insights into the architecture, main workflows, and quality measurements of EMa within our productive environment. We also talk about our experiences and challenges concerning both human and IT resources. Furthermore, we give a brief overview over the DNB AI project. The goal of the project is to look into state-of-the-art algorithms, text data, and vocabularies in order to improve the quality of subject cataloguing using AI methods.",SWIB,,,,,,,,,,,
,2022,Introduction into the Solid Project and its implementations,Patrick Hochstenbach,SWIB/SWIB2022/Introduction into the Solid Project and its implementations.md,"#swib/2022,#workshop,#solid,#protocol","This workshop will introduce the Solid project, the protocols and one of its implementations: the IMEC/Inrupt Community Solid Server CSS.",SWIB,,,,,,,,,,,
,2022,Introduction to the Annif automated indexing tool,"Osma Suominen,Mona Lehtinen,Juho Inkinen,Moritz Fürneisen,Anna Kasprzik",SWIB/SWIB2022/Introduction to the Annif automated indexing tool.md,"#swib/2022,#Annif,#workshop","Many libraries and related institutions are looking at ways of automating their metadata production processes. In this hands-on tutorial, participants will be introduced to the multilingual automated subject indexing tool Annif as a potential component in a library’s metadata generation system. By completing exercises, participants will get practical experience on setting up Annif, training algorithms using example data, and using Annif to produce subject suggestions for new documents.",SWIB,,,,,,,,,,,
,2022,"Libraries, linked data, and decolonization (keynote)",Stacy Allison-Cassin,"SWIB/SWIB2022/Libraries, linked data, and decolonization (keynote).md","#swib/2022,#libraries,#decolonization,#linked-data,#cultural-heritage","Global decolonization movements have focused attention on the colonial nature of cultural heritage. Events such as the toppling and removal of statues of colonial figures, the fight for the return of stolen artefacts and ancestors, and questions regarding the ownership and usage of historical photographs in cultural heritage collections are not new; such activities have gained momentum and greater prominence in public discourse in recent years. While less visible than tangible objects, tendrils and structures of colonialism run through data practices and standards. Such practices continue to harm peoples and lands that have been and continue to be, subject to colonialism. It is vital that linked data practice address this continued legacy and work toward anticolonial within contemporary library and cultural data work. What does decolonial (data) practice mean for linked data in the library and broader cultural data environment? Why does this matter for all individuals involved in linked data work? This talk will delve into issues facing the linked data community, including the limitations of current standards and systems, the vital need for change in practice, the importance and implications of recognizing Indigenous rights and rights of nations and states, and opportunities on the horizon with emerging standards and principles such as CARE and projects related to respectful naming. The talk will also focus on concrete actions the library linked data community can take to support decolonization and a more just and ethical future for linked data.",SWIB,,,,,,,,,,,
,2022,Library data on Wikidata: a case study of the National Library of Latvia,Eduards Skvireckis,SWIB/SWIB2022/Library data on Wikidata a case study of the National Library of Latvia.md,"#swib/2022,#bibliographic/data,#authority/data,#Wikidata,#modeling","Libraries have changed but the core purpose of the library is very much the same – to give access to knowledge and learning. Knowledge organisation in libraries has moved from clay tablets to extensive catalogues, from the backs of playing cards to card indices. Then in the 1970s and early 1980s integrated library systems (ILS) appeared and changed the game. Now national libraries are increasingly stepping leaving locally used ILS behind and steering towards new technologies and possibilities that were not feasible before – bridging silos, multilingual support, alternatives to traditional authority control, etc. Although there is no right or prevailing answer, the direction is set. And that direction is toward the semantic web and linked data which are influencing how bibliographic and authority data are created, shared, and made available to potential users. In this presentation I will focus on modeling, mapping, and creating workflows for moving the bibliographic and authority data of the National Library of Latvia to Wikidata as a major hub of the semantic web. The main focus will be on modeling and integrating library authority files (persons, organizations, works, locations, etc.) to a full extent into Wikidata, reducing any possible data loss to a minimum. This way, Wikidata will not be used as just a hub for institutional identifiers but as a universal and collaborative knowledge base where libraries can truly share and disseminate their knowledge, connect it and use it to describe and query their resources.",SWIB,,,,,,,,,,,
,2022,Mapping and transforming MARC21 bibliographic metadata to LRM/RDA/RDF,"Theodore Gerontakos,Crystal Yragui,Zhuo Pan",SWIB/SWIB2022/Mapping and transforming MARC21 bibliographic metadata to LRMRDARDF.md,"#swib/2022,#MARC21,#mapping,#entities,#BIBFRAME,#XSLT","The MARC21-to-RDA/LRM/RDF Mapping Project is an international, cross-organizational project that aims to broaden the adoption of the RDA/LRM/RDF ontology (that is, the Resource Description and Access implementation of the Library Reference Model using the Resource Description Framework). One premise of the project is that RDA-based values situated in the rich RDA/LRM/RDF context using RDA entities and relationships are more consistent and potentially more useful than when situated in less-granular formats such as BIBFRAME or non-graph-based metadata formats such as MARC21. The starting point of the project is MARC21 data accumulated over decades, and the potential to create a lossless conversion while at the same time creating high quality RDA/LRM/RDF has been a central concern. Once the mapping is complete, there is the expectation that abundant RDA/LRM/RDF datasets will be created and used to test the efficacy of RDA entities and relationships for specific uses. To that end, the project is producing an open-source, XSLT-based conversion tool that can process most MARC bibliographic data to output a close approximation of the intent of the mapping. Presenters describe the theoretical foundation of the project, review the mapping structure and format in detail, demonstrate the conversion tool with a focus on its RDA/LRM/RDF output, and share a vision for the future of RDA/LRM/RDF metadata in the greater library linked data ecosystem. Mappings, code, and project information can be found at the project GitHub repository.",SWIB,,,,,,,,,,,
,2022,Multilingual BERT for library classification in Romance languages using Basisklassifikation,"José Calvo Tello,Enrique Manjavacas,Susanne Al-Eryani",SWIB/SWIB2022/Multilingual BERT for library classification in Romance languages using Basisklassifikation.md,"#swib/2022,#BERT,#bibliographic/data,#K10plus","We apply the multilingual version of the language model BERT (mBERT) to predict classes from the library classification system Basisklassifikation (BK). We frame the present task as a multi-label classification problem, where each input instance (a library record) must be assigned at least one class from the BK. As input, we only use data from the catalogue, we do not use the full text of the publications. Three feature sets are considered: title only, bibliographic data only, and extended bibliographic data. We apply two algorithms: mBERT, which we fine-tune using raw text from different metadata fields as input. We also train a multi-label Support Vector Machine classifier with a vector based on the tokenizer of mBERT. We decided to work with records from Romance Studies because it challenges the perspective of considering only one or only a few languages, such as in national libraries. The dataset contains 189,134 library records associated with Romance Studies from the catalogue K10plus. The general results for the different approaches yield micro F1-scores between 0.6 and 0.8 (macro F1-scores between 0.2 and 0.4), with better performance for mBERT as classifier. In this presentation we will explore how these results are influenced by factors such as the language, specific classes, or the number of records per class. This is a promising approach for the generation of suggestions which should be considered by subject specialists.",SWIB,,,,,,,,,,,
,2022,"New, newer, newest: incrementally integrating linked data into library catalog discovery","Huda Khan,Steven Folsom,Astrid Usong","SWIB/SWIB2022/New, newer, newest incrementally integrating linked data into library catalog discovery.md","#swib/2022,#LD4P3,#Cornell,#ShareVDE,#Share-VDE,#Sinopia","We are exploring how the use of linked data can enhance discovery in library catalogs as we design, evaluate, prototype, and integrate linked data solutions as part of the Linked Data for Production: Closing the Loop (LD4P3) grant.
In one phase of this grant, we linked data to supplement information about authors and subjects in the Cornell production library catalog. This integration used data from Wikidata, DBpedia, and the Library of Congress (LOC) linked data service. We used feedback from user studies and from our library catalog user representatives team to implement and refine these features.
In a separate phase, we focused on how the linked data representation of creative works and related information could be integrated into a discovery layer. We examined how works are aggregated in multiple BIBFRAME representations of library catalog data, such as the Library of Congress Hubs and ShareVDE BIBFRAME conversion of library data, and how these aggregations may help us identify relationships between library catalog records. In addition to this data analysis, we also implemented a prototype of the Cornell library catalog which uses BIBFRAME relationships to display related items to the user. We used Sinopia, the linked data cataloging editor being developed as part of LD4P3, to define these relationships. One of the outcomes of this phase is a better understanding of how these linked data sources can demonstrate linkages between library catalog items representing the same or related works. This presentation will provide an overview of the production integration from the first phase and the data analysis and experimental prototype from the second phase. This work is further described [here](https://wiki.lyrasis.org/display/LD4P3/WP3%3A+Discovery). Related work is also captured in [this article](https://kula.uvic.ca/index.php/kula/article/view/229) .",SWIB,,,,,,,,,,,
,2022,On leveraging artificial intelligence and natural language processing to create an open source workflow for the rapid creation of archival linked data for digital collections,Jennifer Erin Proctor,SWIB/SWIB2022/On leveraging artifical intelligence and natural language processing to create an open source workflow for the rapid creation of archival linked data for digital collections.md,"#swib/2022,#AI,#HIVE2,#OpenRefine","This paper proposes, tests, and evaluates an Artificial-Intelligence-supported workflow to enhance the ability of librarians and archivists to convert standardized metadata to better-than-item-level archival linked data. The protocol combines elements of computer vision with natural language processing, entity extraction, and metadata linking techniques to provide new approaches for findability and usability of cultural resources in digital spaces. Existing metadata and, optionally, images are taken as input. Metadata text is processed with natural language processing including sentenizing, tokenizing, part-of-speech tagging, chunking of phrases and clauses, and finally named entity recognition, extraction, and linking. Entities are used to query HIVE2, a search tool that matches ontology terms to linked data tags, which are then parsed into triples through semantic processing. For images, each image is processed to identify people who are pictured in it. Cropped sub-images are created for each person where each image is given a unique identifier to act as its primary linked data entity, and a first triple is created showing that that entity is depicted in the image being processed. Once the spreadsheet of triples is output, it can be imported into OpenRefine in order to convert it into the Wikidata format which is useful for linked digital collections, crowdsourcing, and cooperative collections-as-data programs.",SWIB,,,,,,,,,,,
,2022,Performance comparison of select and construct queries of triplestores on the example of the JVMG project,Tobias Malmsheimer,SWIB/SWIB2022/Performance comparison of select and construct queries of triplestores on the example of the JVMG project.md,"#swib/2022,#Japanese-Visual-Media-Graph,#project,#jvmg,#knowledge-graph,#triplestore,#SPARQL,#Blazegraph,#Virtuoso,#GraphDB,#co-occurrence,#discrepancies","In the Japanese Visual Media Graph (JVMG) project ([Project blog](https://jvmg.iuk.hdm-stuttgart.de/), [JVMG web frontend (Beta)](https://mediagraph.link/)) we use the Resource Description Framework (RDF) to create a knowledge graph for researchers working with contemporary popular Japanese media. The project is funded by the German Research Foundation and the main project partners are Stuttgart Media University and Leipzig University Library.
In order to easily access our RDF data we use a triplestore and SPARQL. We initially chose the Apache Fuseki triple store because of its open licensing terms and ease of installation and management.
Once the database was completed to a certain degree, we implemented and tested our knowledge graph using different triplestore software solutions (Apache Fuseki, Blazegraph, Virtuoso and GraphDB) and compared their performance on several tasks that we consider representative operations on our data. These tasks include simple queries such as aggregating all data for a given entity and more complex analyses such as finding co-occurrence. We found major performance discrepancies, both between the different software solutions and between SELECT and CONSTRUCT SPARQL queries. Query times differ by factors of up to 100 across the software solutions, and CONSTRUCT queries consistently perform much worse, even when using the exact same WHERE patterns.
In summary, the Apache Fuseki triple store performed quite well across all tasks. While some other software solutions were faster for some tasks, the gains were not significant enough to consider migrating our infrastructure to a new solution.",SWIB,,,,,,,,,,,
,2022,"Shapes, forms and footprints: web generation of RDF data without coding",Patrick Hochstenbach,"SWIB/SWIB2022/Shapes, forms and footprints web generation of RDF data without coding.md","#swib/2022,#knowledge-graph,#ontology,#FormViewer,#Solid,#protocol","While browsing the Web a user is often confronted with a form requiring them to enter their personal data. We share book reviews on Goodreads; share our names, addresses and affiliations information with conference tools; submit our bibliography to institutional websites and centralized services such as ORCID. Filling out this information is a repetitive task for many users, using data that in principle should already be available in a knowledge graph somewhere. For the creation of (ad hoc) forms from scratch users do not have many options other than using platforms such as Google Forms which provide a limited set of input fields and a Google sheet as the end result, or asking their IT department to build a form which can require some time to implement and publish. During the COVID-19 pandemic many IT departments were asked to provide such ad hoc forms for all kinds of crowd sourcing where metadata was entered by library staff working from home. All these forms have hard-coded locations where the produced data needs to be stored, so a user has no choice in that matter. I will present an abstract way how RDF data can be read, updated, and stored in a decentralized way using RDF forms. The RDF data is defined by shapes, the forms are defined using a Form ontology, and the footprint (where to store the result) is also coded in RDF. All these inputs are web resources that declare to FormViewer apps how to read, update, and store data. An entire app can be written just by manipulating RDF resources, using the Solid protocol as persistence layer.",SWIB,,,,,,,,,,,
,2022,Structured Data in Wikimedia Commons,Christian Erlinger,SWIB/SWIB2022/Structured Data in Wikimedia Commons.md,"#swib/2022,#SDC,#Wikidata,#Wikimedia,#GLAM,#SPARQL,#workshop","Since 2020, Structured Data on Wikimedia Commons (SDC) is on the way to completely renew the way of describing media files on the open and central media repository Wikimedia Commons. Instead of plain (unstructured) Wikitext media files can be described and categorized with structured data, in detail with both with properties and items out of Wikidata, the free knowledge graph.",SWIB,,,,,,,,,,,
,2022,The application of IIIF and LOD in digital humanities: a case study of the dictionary of wooden slips,"Shu-Jiun Chen,Lu-Yen Lu",SWIB/SWIB2022/The application of IIIF and LOD in digital humanities a case study of the dictionary of wooden slips.md,"#swib/2022,#IIIF,#study,#digital-humanities,#annotation,#APIs,#API,#framework,#LOD,#lifecycle","This study explores the use of the International Image Interoperability Framework (IIIF) and linked open data (LOD) in digital humanities with regard to different layers of interoperability. Focusing on imaged texts, it takes the text interpretation and slip restoration of the “Juyan Han Wooden Slips (202-220 CE)” as use case, integrates the demands of scholars in wooden slip research in the humanities, and establishes a digital research environment. By analysing and deconstructing historians’ work on character interpretation and manuscript restoration into discrete tasks and categories, the study proposes that the digital humanities research platform needs to be able to provide a complete, structured annotation function that allows input of annotated information from different knowledge fields. In addition, it needs to provide functionalities for reading, comparing, and referring to images, for example for the annotation of interpretations in wooden slip images, zooming in or out of image areas, or side-by-side image comparison for multiple wooden slip regions, which allow arguments about the context surrounding a character, to be displayed together in the image interface. The study has adopted the image, presentation and content search APIs of IIIF, and developed a LOD lifecycle framework in order to transform the legacy data into LOD, enable access and cross-referencing for these resources, and satisfy different needs in research. “The Wooden Slips Character Dictionary–Database of Juyan Han Wooden Slips from the Institute of History and Philology Collections” (WCD)” and the “Multi-database Search System for Historical Chinese Characters” were taken as practical examples to explain how interoperability is utilised in systems for the digital humanities.",SWIB,,,,,,,,,,,
,2022,What linked data can tell about geographical trends in Finnish fiction literature – using the BookSampo knowledge graph in digital humanities,"Telma Peura,Petri Leskinen,Eero Hyvönen",SWIB/SWIB2022/What linked data can tell about geographical trends in Finnish fiction literature – using the BookSampo knowledge graph in digital humanities.md,"#swib/2022,#BookSampo,#Finland,#knowledge-graph,#metadata,#digital-humanities","BookSampo – a linked data (LD) service and portal for Finnish fiction literature – was launched in 2011 by the public libraries of Finland. Since then, the original knowledge graph (KG) has grown from 400,000 subjects to over 8,740,000, including literary works, authors, book covers, reviews, literary awards, along with other detailed semantic metadata. The portal, maintained by a team of Finnish librarians, has nearly 2 million annual users and its KG resides in a SPARQL endpoint. However, the potential of the KG has not been explored from a Digital Humanities (DH) research perspective where digital tools could help with knowledge discovery. This study presents novel data analyses on the BookSampo KG to demonstrate how LD can be used in DH studies, building on the concepts of distant reading and literary geography, i.e., the application of computational methods to study literature and its geographical dimension. Our work reveals and illustrates trends and biases in the development of Finnish literature based on semantic metadata. Focusing on novels and their geographical settings, the analyses show interesting annotation patterns related to genres and themes. Although the geographical diversity in fictional settings has increased over time, our results suggest that Finnish fiction literature still focuses on national settings and historical topics. Moreover, our digital perspective shows that the Finnish literary geography is biased by gender. Finally, we discuss to which extent the findings depend on the KG annotation practices, and how this potential meta-level bias should be addressed in the future.",SWIB,,,,,,,,,,,
,2022,Working with linked open data in the context of annotation and semantic enrichment of 3D media: A new FOSS toolchain,"Lozana Rossenova,Zoe Schubert,Paul Duchesne,Lucia Sohmen,Lukas Günther,Ina Blümel",SWIB/SWIB2022/Working with linked open data in the context of annotation and semantic enrichment of 3D media A new FOSS toolchain.md,"#swib/2022,#digital-curators,#data/manager,#workshop,#OpenRefine,#Wikibase,#Kompakkt,#NFDI4Culture,#consortium","This workshop aims to help researchers, digital curators and data managers learn how to make datasets including 3D models and other media files available as linked open data within a collaborative annotation and presentation-ready environment. Participants will take part in practical demonstrations using an integrated toolchain that connects three existing open source software tools: 1) OpenRefine – for data reconciliation and batch upload; 2) Wikibase – for linked open data storage; and 3) Kompakkt – for rendering and annotating 3D models, and other 2D and AV media files. This toolchain and associated workflow was developed in the context of NFDI4Culture, a German consortium of research and cultural institutions working towards a shared infrastructure for research data that meets the needs of 21st-century data creators, maintainers and end users across the broad spectrum of the digital libraries and archives field, and the digital humanities. All components of the toolchain feature graphical user interfaces aiming to lower the barrier of participation in the semantic web for a wide range of practitioners and researchers. Furthermore, the toolchain development involves the specification of a common data model that aims to increase interoperability across datasets of digitised objects from different domains of culture. The workshop will be of interest to researchers, digital curators and information science professionals who work with datasets containing 3D media, and want to learn more about the possibilities of linked open data, open source software and collaborative annotation workflows.",SWIB,,,,,,,,,,,
